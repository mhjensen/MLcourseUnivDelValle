%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-vancouver]{sn-jnl}% ME

\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
%%\documentclass[pdflatex,sn-mathphys]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[sn-standardnature]{sn-jnl}% Standard Nature Portfolio Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads



%% ADDED BY ME
\usepackage{bm}
\usepackage{lscape}
\usepackage{xcolor}

\usepackage[section]{placeins}

%code related
\usepackage{listings}


% table related
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{afterpage}

%%
\DeclareMathOperator*{\argmin}{arg\,min} % Jan Hlavacek
%%
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
%%
\newcommand{\myref}[2]{\href{#1}{\emph{\underline{#2}}}}
\newcommand{\gitlink}[2]{\href{https://github.com/#1/#2}{\emph{\underline{#2}}}}
\newcommand{\arxivlink}[1]{\href{https://arxiv.org/abs/#1}{\emph{\underline{#1}}}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

\newcounter{paranum}[subsubsection]
\newcommand{\subsubsubsection}{\vspace{10pt}\noindent\textbf{\thesubsubsection.\refstepcounter{paranum}\theparanum\;}\textbf}

% \newcounter{paranum}[paragraph]
% \newcommand{\subPara}{\vspace{10pt}\noindent\textbf{\thesection.\refstepcounter{paranum}\theparanum}\textbf}

\usepackage{caption}
\usepackage{subcaption}




%\graphicspath{ {./figs/} }



\begin{document}

\title[Scientific Machine Learning through PINNs]{Scientific Machine Learning through Physics-Informed Neural Networks: Where we are and What's next}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Salvatore} \sur{Cuomo}}\email{salvatore.cuomo@unina.it}

\author*[2]{\fnm{Vincenzo} \sur{Schiano Di Cola}}\email{vincenzo.schianodicola@unina.it}
%\equalcont{These authors contributed equally to this work.}

\author[1]{\fnm{Fabio} \sur{Giampaolo}}\email{fabio.giampaolo@unina.it}
%\equalcont{These authors contributed equally to this work.}

\author[2]{\fnm{Gianluigi} \sur{Rozza}}\email{grozza@sissa.it}

\author[3]{\fnm{Maziar} \sur{Raissi}}\email{mara4513@colorado.edu}

\author*[1]{\fnm{Francesco} \sur{Piccialli}}\email{francesco.piccialli@unina.it}

\affil*[1]{\orgdiv{Department of Mathematics and Applications "R. Caccioppoli"}, \orgname{University of Naples Federico II}, \orgaddress{\city{Napoli}, \postcode{80126}, \country{Italy} } }

\affil*[2]{\orgdiv{Department of Electrical Engineering and Information Technology}, \orgname{University of Naples Federico II}, \orgaddress{\street{Via Claudio}, \city{Napoli}, \postcode{80125}, \country{Italy} } }


\affil[2]{\orgname{SISSA – International School for Advanced Studies}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \country{Italy}}}

\affil[3]{\orgdiv{Department of Applied Mathematics}, \orgname{University of Colorado Boulder}, \orgaddress{\street{Street}, \city{Boulder}, \postcode{610101}, \country{United States}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{
Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself.
PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs.
This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. 
This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). 
The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures.
Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Physics-Informed Neural Networks, Scientific Machine Learning, Deep Neural Networks, Nonlinear equations, Numerical methods, Partial Differential Equations, Uncertainty}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec1}


%%%%%%%%%%%%%%%%%%%%%%%%
%%\paragraph{Intro on DL} 
%%%%%%%%%%%%%%%%%%%%%%%%
Deep neural networks have succeeded in tasks such as computer vision, natural language processing, and game theory.
Deep Learning (DL) has transformed how categorization, pattern recognition, and regression tasks are performed across various application domains.


%%%%%%%%%%%%%%%%%%%%%%%%
%%\paragraph{DL over numeric} 
%%%%%%%%%%%%%%%%%%%%%%%%
Deep neural networks are increasingly being used to tackle classical applied mathematics problems such as partial differential equations (PDEs) utilizing machine learning and artificial intelligence approaches.
% Solving problems numerically
Due to, for example, significant nonlinearities, convection dominance, or shocks, some PDEs are notoriously difficult to solve using standard numerical approaches. Deep learning has recently emerged as a new paradigm of scientific computing 
%machine learning 
thanks to neural networks' universal approximation and great expressivity.
%
Recent studies have shown deep learning to be a promising method for building meta-models for fast predictions of dynamic systems. In particular, NNs have proven to represent the underlying nonlinear input-output relationship in complex systems. %
%
Unfortunately, dealing with such high dimensional-complex systems are not exempt from the curse of dimensionality, which Bellman first described in the context of optimal control problems \citep{bellman1966dynamic}.
%TODO cita (citeBellman1957) trova paper e differenze 1966 e 1957
%
However, machine learning-based algorithms are promising for solving PDEs \citep{Ble2021_ThreeWaysSolve_ErnBE}.


%%%%%%%%%%%%%%%%%%%%%%%%
%%\paragraph{New trends and problems} 
%%%%%%%%%%%%%%%%%%%%%%%%
Indeed, \cite{Ble2021_ThreeWaysSolve_ErnBE} consider machine learning-based PDE solution approaches will continue to be an important study subject in the next years as deep learning develops in methodological, theoretical, and algorithmic developments. 
%
Simple neural network models, such as MLPs with few hidden layers, were used in early work for solving differential equations \citep{Lag1998_ArtificialNeuralNetworks_LikLLF}.
Modern methods, based on NN techniques, take advantage of optimization frameworks and auto-differentiation, like \cite{Ber2018_UnifiedDeepArtificial_NysBN} that suggested a unified deep neural network technique for estimating PDE solutions.
%
Furthermore, it is envisioned that DNN will be able to create an interpretable hybrid Earth system model based on neural networks for Earth and climate sciences \citep{Irr2021_TowardsNeuralEarth_BoeIBS}.




%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Taxonomy} %Naming
%%%%%%%%%%%%%%%%%%%%%%%%

Nowadays, the literature does not have a clear nomenclature for integrating previous knowledge of a physical phenomenon with deep learning.
`Physics-informed,' `physics-based,' `physics-guided,' and `theory-guided' are often some used terms. \cite{Kim2021_KnowledgeIntegrationDeep_KimKKLL}  developed the overall taxonomy of what they called informed deep learning, followed by a literature review in the field of dynamical systems.
Their taxonomy is organized into three conceptual stages: (i) what kind of deep neural network is used, (ii) how physical knowledge is represented, and (iii) how physical information is integrated. 
Inspired by their work, we will investigate PINNs, a 2017 framework, and demonstrate how neural network features are used, how physical information is supplied, and what physical problems have been solved in the literature. %A taxonomy review has been analyzed in a paper \cite{Kim2021_KnowledgeIntegrationDeep_KimKKLL},




%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{What the PINNs are} 
%%%%%%%%%%%%%%%%%%%%%%%%
%Physics-Informed Neural Networks (PINNs) are a new class of DL networks that have emerged as a very promising DL solution for solving scientific computing challenges.
%
Physics-Informed Neural Networks (PINNs) are a scientific machine learning technique used to solve problems involving Partial Differential Equations (PDEs).
PINNs approximate PDE solutions by training a neural network to minimize a loss function; it includes terms reflecting the initial and boundary conditions along the space-time domain's boundary and the PDE residual at selected points in the domain (called collocation point). 
PINNs are deep-learning networks that, given an input point in the integration domain, produce an estimated solution in that point of a differential equation after training.
%
Incorporating a residual network that encodes the governing physics equations is a significant novelty with PINNs.
The basic concept behind PINN training is that it can be thought of as an unsupervised strategy that does not require labelled data, such as results from prior simulations or experiments.
%
The PINN algorithm is essentially a mesh-free technique that finds PDE solutions by converting the problem of directly solving the governing equations into a loss function optimization problem.
It works by integrating the mathematical model into the network and reinforcing the loss function with a residual term from the governing equation, which acts as a penalizing term to restrict the space of acceptable solutions.



%\subsection{PINN's historical timeline }\label{secH}

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Story of PINN} 
%%%%%%%%%%%%%%%%%%%%%%%%
PINNs take into account the underlying PDE, i.e. the physics of the problem, rather than attempting to deduce the solution based solely on data, i.e. by fitting a neural network to a set of state-value pairs.
%
The idea of creating physics-informed learning machines that employ systematically structured prior knowledge about the solution can be traced back to earlier research by \cite{Owh2015_BayesianNumericalHomogenization_Owh}, which revealed the promising technique of leveraging such prior knowledge.
%
\cite{Rai2017_InferringSolutionsDifferential_PerRPK, Rai2017_MachineLearningLinear_PerRPK} used Gaussian process regression to construct representations of linear operator functionals, accurately inferring the solution and providing uncertainty estimates for a variety of physical problems; this was then extended in \citep{Rai2018_HiddenPhysicsModels_KarRK, Rai2018_NumericalGaussianProcesses_PerRPK}.
%They further extended this method to nonlinear problems, solution inference and system identification [7,8]
%
PINNs were introduced in 2017 as a new class of data-driven solvers in a two-part article \citep{Rai2017_PhysicsInformedDeep1_PerRPK,Rai2017_PhysicsInformedDeep2_PerRPK} published in a merged version afterwards in 2019 \citep{Rai2019_PhysicsInformedNeural_PerRPK}.
%
\cite{Rai2019_PhysicsInformedNeural_PerRPK}
 introduce and illustrate the PINN approach for solving nonlinear PDEs, like Schrödinger, Burgers, and Allen-Cahn equations. %\cite{Rai2018_HiddenPhysicsModels_KarRK}
%
They created physics-informed neural networks (PINNs) 
%in \cite{Rai2019_PhysicsInformedNeural_PerRPK}, 
which can handle both forward problems of estimating the solutions of governing mathematical models and inverse problems, where the model parameters are learnt from observable data.


The concept of incorporating prior knowledge into a machine learning algorithm is not entirely novel.
%Raissi et al. \cite{Rai2019_PhysicsInformedNeural_PerRPK} %research was influenced by articles from
In fact \cite{Dis1994_NeuralNetworkBased_PhaDP} can be considered one of the first PINNs. 
This paper followed the results of the universal approximation achievements of the late 1980s, \citep{Hor1989_MultilayerFeedforwardNetworks_StiHSW}; then in the early 90s several methodologies were proposed to use neural networks to approximate PDEs, like the work on constrained neural networks
\citep{Psi1992_HybridNeuralNetwork_UngPU, Lag1998_ArtificialNeuralNetworks_LikLLF}
%Psichogios and Ungar [PU92],  %https://doi.org/10.1002/aic.690381003
%Lagaris et al. [LLF98], %https://doi.org/10.1109/72.712178
or 
\citep{Lee1990_NeuralAlgorithmSolving_KanLK}.
%Following the universal approximation achievements of the late 1980s, several methodologies were proposed to use neural networks to approximate PDEs.
%In the early 90s, a machine learning framework similar to PINN was proposed for approximating differential equation solutions. 
In particular \cite{Dis1994_NeuralNetworkBased_PhaDP}
employed a simple neural networks to approximate a PDE, where the neural network's output was a single value that was designed to approximate the solution value at the specified input position. The network had two hidden layers, with 3, 5 or 10 nodes for each layer.
The network loss function approximated the $L^2$ error of the approximation on the interior and boundary of the domain using point-collocation.
While, the loss is evaluated using a quasi-Newtonian approach and the gradients are evaluated using finite difference.
\\
In \cite{Lag1998_ArtificialNeuralNetworks_LikLLF}, the solution of a differential equation is expressed as a constant term and an adjustable term with unknown parameters, the best parameter values are determined via a neural network. However, their method only works for problems with regular borders.
\cite{Lag2000_NeuralNetworkMethods_LikLLP} extends the method to problems with irregular borders.
%However, one of the first papers to approximate the unknown function in a PDE by a multilayer perceptron networks, can be considered \cite{Dis1994_NeuralNetworkBased_PhaDP}.


As computing power increased during the 2000s, increasingly sophisticated models with more parameters and numerous layers became the standard \cite{Oez2021_PoissonCnnConvolutional_HamOeHL}. 
Different deep model using MLP, were introduced, also using Radial Basis Function \cite{Kum2011_MultilayerPerceptronsRadial_YadKY}.

Research into the use of NNs to solve PDEs continued to gain traction in the late 2010s, thanks to advancements in the hardware used to run NN training, the discovery of better practices for training NNs, and the availability of open-source packages, such as Tensorflow \citep{Hag2021_SciannKerastensorflowWrapper_JuaHJ}, and the availability of Automatic differentiation in such packages \citep{Pas2017_AutomaticDifferentiationPytorch_GroPGC}.

Finally, more recent advancements by 
\cite{Kon2018_GeneralizationEquivarianceConvolution_TriKT}, %Kondor [Kon18], %http://arxiv.org/abs/1803.01588
%Hirn et al. [HMP17], %https://doi.org/10.1137/16M1075454
and 
\cite{Mal2016_UnderstandingDeepConvolutional_Mal}, %Mallat [Mal16].%https://doi.org/10.1098/rsta.2015.0203
brought to %Raissi et al. 
\cite{Rai2019_PhysicsInformedNeural_PerRPK} solution that extended previous notions while also introducing fundamentally new approaches, such as a discrete time-stepping scheme that efficiently leverages the predictive ability of neural networks  \citep{Kol2021_PhysicsInformedNeural_DAKDJH}.
The fact that the framework could be utilized directly by plugging it into any differential problem simplified the learning curve for beginning to use such, and it was the first step for many researchers who wanted to solve  their problems with a Neural network approach \citep{Mar2021_OldNewCan_Mar}. 
The success of the PINNs can be seen from the rate at which \cite{Rai2019_PhysicsInformedNeural_PerRPK} is cited, and the exponentially growing number of citations in the recent years (Figure~\ref{fig:time}).




%\section{Other approaches for SciML}\label{secH}
%https://www.youtube.com/watch?v=lp4vCo40J3I
However, PINN is not the only NN framework utilized to solve PDEs.
Various frameworks have been proposed in recent years,  and, while not exhaustive, we have attempted to highlight the most relevant ones in this paragraph. 

The Deep Ritz method (DRM) \citep{Wei2018_DeepRitzMethod_YuWY}, where the loss is defined as the energy of the problem's solution.

Then there approaches based on the Galerkin method, or Petrov–Galerkin method, where the loss is given by multiplying the residual by a test function, and when is the volumetric residual we have a Deep Galerkin Method (DGM) \citep{Sir2018_DgmDeepLearning_SpiSS}.
Whereas, when a Galerkin approach is used on collocation points the framework is a variant of PINNs, i.e. a hp-VPINN \cite{Kha2021_HpVpinnsVariational_ZhaKZK}.
%Essentially, the PINN approach is a collocation method for evaluating the loss function. 

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Different type of PINN} 
%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%% https://www.sciencedirect.com/science/article/pii/S0021999121000875?via%3Dihub#br0240
Within the a collocation based approach, i.e. PINN methodology  \citep{Rai2019_PhysicsInformedNeural_PerRPK, Yan2019_AdversarialUncertaintyQuantification_PerYP, Men2020_PpinnPararealPhysics_LiMLZK}, many other variants were proposed, as  the variational hp-VPINN, as well as conservative PINN (CPINN)\citep{Jag2020_ConservativePhysicsInformed_KhaJKK}.
Another approach is  physics-constrained neural networks (PCNNs) \citep{Zhu2019_PhysicsConstrainedDeep_ZabZZKP, Sun2020_SurrogateModelingFluid_GaoSGPW, Liu2021_DualDimerMethod_WanLW}.
while PINNs incorporate both the PDE and its initial/boundary conditions (soft BC) in the training loss function, PCNNs, are ``data-free'' NNs, i.e. they enforce the initial/boundary conditions (hard BC) via a custom NN architecture while embedding the PDE in the training loss. 
%https://epubs.siam.org/doi/pdf/10.1137/19M1274067
This soft form technique is described in  \cite{Rai2019_PhysicsInformedNeural_PerRPK}, where the term ``physics-informed neural networks'' was coined (PINNs).
%

Because there are more articles on PINN than any other specific variant, such as PCNN, hp-VPINN, CPINN, and so on, this review will primarily focus on PINN, with some discussion of the various variants that have emerged, that is, NN architectures that solve differential equations based on collocation points. 
%%%%%%%%%%%%%%%%%%% 

%Finally, because we are presenting PINN as a representative of a more general approach, we will use the singular version of the acronym PINN rather than the plural form in this review. 
Finally, the acronym PINN will be written in its singular form rather than its plural form in this review, as it is considered representative of a family of neural networks of the same type. 





%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Where can I read about PINN} 
%%%%%%%%%%%%%%%%%%%%%%%%
Various review papers involving PINN have been published.
About the potentials, limitations and applications for forward and inverse problems  \citep{Kar2021_PhysicsInformedMachine_KevKKL}
for three-dimensional flows \citep{Cai2021_PhysicsInformedNeural_MaoCMW},
or a comparison with other ML techniques \citep{Ble2021_ThreeWaysSolve_ErnBE}. %,
%a review on fluid mechanics ... TODO ritrova doi!!!
%
An introductory course on PINNs that covers the fundamentals of Machine Learning and Neural Networks can be found from \cite{Kol2021_PhysicsInformedNeural_DAKDJH}.
%
PINN is also compared against other methods that can be applied to solve PDEs, like the one based on the Feynman–Kac theorem \citep{Ble2021_ThreeWaysSolve_ErnBE}.
%
Finally, PINNs have also been extended to solve integrodifferential equations (IDEs)\citep{Pan2019_FpinnsFractionalPhysics_LuPLK, Yua2022_PinnAuxiliaryPhysics_NiYNDH} or stochastic differential equations (SDEs) \citep{Yan2020_PhysicsInformedGenerative_ZhaYZK}. %\citep{Kar2021_PhysicsInformedMachine_KevKKL}.

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{PINN vs others}  %PINN is good
%%%%%%%%%%%%%%%%%%%%%%%%
%PINNs have been shown to perform better than conventional numerical techniques in many areas. 
%
%Numerical techniques for approximating PDEs include finite difference, finite element (FEM), finite volume, and spectral methods.
%Unlike many classic computing approaches, PINNs’ computational cost does not grow with the number of grid points.
%Furthermore, the trained PINN network may be used to predict values on simulated grids of various resolutions without retraining.
%
%Since FEM is the most frequently used numerical method, it is compared to PINN in this paragraph, based on the results in \cite{lu2021deepxde}. %
%%\paragraph{FEM and PINN}
%In FEM, the solution is approximated by a piecewise polynomial with unknown point values, while in PINNs, the surrogate model is a neural network with weights and biases.
%
%Moreover, FEM usually requires a mesh generation, whereas PINN, being mesh-free, can employ a grid or random points.
%Finally, PINN approximates the function and its derivatives nonlinearly, whereas FEM does it linearly.% \citep{lu2021deepxde}.
%
%PDEs are converted to algebraic systems using the stiffness and mass matrices, while PINN embeds them within the loss function.
%In the last stage, a linear solver solves the algebraic system in FEM, but a gradient-based optimizer learns the network in PINN.



%%%%%%%%%%%%%%%%%%%%%%%%
%%\paragraph{Synthesis} 
%%%%%%%%%%%%%%%%%%%%%%%%
Being able to learn PDEs, PINNs have several advantages over conventional methods.
%
PINNs, in particular, are mesh-free methods that enable on-demand solution computation after a training stage, and they allow solutions to be made differentiable using analytical gradients.
Finally, they provide an easy way to solve forward jointly and inverse problems using the same optimization problem.
%%%%%%%%%%%%%%%%%%%%%%%%
%%\paragraph{Inverse problems} 
%%%%%%%%%%%%%%%%%%%%%%%%
In addition to solving differential equations (the forward problem), PINNs may be used to solve inverse problems such as %inferring unknown dynamics from observations and
characterizing fluid flows from sensor data. 
%
In fact, the same code used to solve forward problems can be used to solve inverse problems with minimal modification. 
%
Indeed, PINNs can address PDEs in domains with very complicated geometries or in very high dimensions that are all difficult to numerically simulate as well as inverse problems and constrained optimization problems.
%

% Why PINN is bad, and what we will see in the conclusion.
% Structure of the Paper




%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{What is this review about} 
%%%%%%%%%%%%%%%%%%%%%%%%
In this survey, we focus on how PINNs are used to address different scientific computing problems, the building blocks of PINNs, the aspects related to learning theory, what toolsets are already available, future directions and recent trends, and issues regarding accuracy and convergence.
According to different problems, we show how PINNs solvers have been customized in literature by configuring the depth, layer size, activation functions and using transfer learning.


%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{How the survey was done} 
%%%%%%%%%%%%%%%%%%%%%%%%
This article can be considered as an extensive literature review on PINNs.
%
It was mostly conducted by searching Scopus for the terms:
\\
\noindent
\texttt{((physic* OR physical) W/2 (informed OR constrained) W/2 ``neural network’’)
}
\\
\noindent
% %
% \begin{verbatim*}
% ((physic* OR physical) W/2 (informed OR constrained) W/2 ``neural network'')
% \end{verbatim*}
% %
% \begin{lstlisting}
% ((physic* OR physical) W/2 (informed OR constrained) W/2 ``neural network'')
% \end{lstlisting}
% %
% \code{
% ((physic* OR physical) W/2 (informed OR constrained) W/2 ``neural network’’)
% }
% %
%
%
The primary research question was to determine what PINNs are and their associated benefits and drawbacks.
The research also focused on the outputs from the CRUNCH research group in the Division of Applied Mathematics at Brown University and then on the (Physics-Informed Learning Machines for Multiscale and Multiphysics Problems) PhILMs Center, which is a collaboration with the Pacific Northwest National Laboratory.
In such a frame, the primary authors who have been involved in this literature research are Karniadakis G.E., Perdikaris P., and Raissi M.
Additionally, the review considered studies that addressed a broader topic than PINNs, namely physics-guided neural networks, as well as physics-informed machine learning and deep learning.
%%%%%%%%%%%%%%%%%%%%



%An overview of what is the impact of PINN in today's literature and applications, is syntetyzed in Figure~\ref{fig:time}.
Figure~\ref{fig:time} summarizes what the influence of PINN is in today's literature and applications. 


% Sampled PINN topic
%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\textwidth]{fig_1}
    \caption{%
% START UPDATE (Fabio)
A number of papers related to PINNs (on the right) addressed problems on which PINNs are applied (on the left) by year.
% END UPDATE (Fabio)
PINN is having a significant impact in the literature and in scientific applications.
The number of papers referencing Raissi or including PINN in their abstract title or keywords is increasing exponentially.
The number of papers about PINN more than quintupled between 2019 and 2020, and there were twice as many new papers published in 2021 as there were in 2020. 
Since Raissi's first papers on arXiv in 2019 \citep{Rai2019_PhysicsInformedNeural_PerRPK}, a boost in citations can be seen in late 2018 and 2019.
%
On the left, we display a sample problem solved in that year by one of the articles, specifically a type of time-dependent equation. 
%
Some of the addressed problems to be solved in the first vanilla PINN, for example, were the Allen–Cahn equation, the Korteweg–de Vries equation, or the 1D nonlinear Shr\"{o}dinger problem (SE). %todo  (NLS)
By the end of 2019 \cite{Mao2020_PhysicsInformedNeural_JagMJK} solved with the same PINN Euler equations (EE) that model high-speed aerodynamic flows.
By the end of 2020 \cite{Jin2021_NsfnetsNavierStokes_CaiJCLK} solved the incompressible Navier-Stokes equations (NSE).
Finally, in 2021 \cite{Cai2021_PhysicsInformedNeural_WanCWW} coupled the Navier–Stokes equations with the
corresponding temperature equation for analyzing heat flow convection (NSE+HE). %and estimate unknown parameters of the 1D two-phase Stefan problem.
}
    \label{fig:time}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\section{Physical informed neural networks}
\section{The building blocks of a PINN}\label{sec2}


%TODO gestire ponte da contiuno a discreto.



%\subsection{What are... what do they do}
%\subsection{What are PINN}
%%%%%%%%%%% FROM \cite{Kol2021_PhysicsInformedNeural_DAKDJH}
Physically-informed neural networks can address problems that are described by few data, or noisy experiment observations.
%
Because they can use known data while adhering to any given physical law specified by general nonlinear partial differential equations, PINNs can also be considered neural networks that deal with supervised learning problems
\citep{Gos2020_TransferLearningEnhanced_AniGACR}.
%
PINNs can solve differential equations expressed, in the most general form, like:
\begin{equation}\label{eq:general_form}
  \begin{aligned}
    \mathcal{F}(\bm{u}(\bm{z}); \gamma ) &= \bm{f}(\bm{z}) \quad &&  \bm{z} \text{ in } \Omega,\\
    \mathcal{B}(\bm{u}(\bm{z})) &= \bm{g}(\bm{z})  \quad &&  \bm{z} \text{ in } \partial\Omega
  \end{aligned}
\end{equation} %TODO chiarire problmea differenziale
%TODO where z in R n+1
defined on the domain $\Omega \subset \mathbb{R}^d$ with the boundary $\partial \Omega$.
Where 
%In our formulation 
$\bm{z} := [x_1,\ldots,x_{d-1};t]$ indicates the space-time coordinate vector, % $d=n+1$, %. %TODO gestire ponte da contiuno a discreto.
$\bm{u}$ represents the unknown solution, $\gamma$ are the parameters related to the physics, $\bm{f}$ is the function identifying the data of the problem and $\mathcal{F}$ is the non linear differential operator.
Finally, since the initial condition can actually be considered as a type of Dirichlet boundary condition on the spatio-temporal domain, it is possible to 
%and usually one adds the
denote 
$\mathcal{B}$ as the operator indicating arbitrary initial or boundary conditions related to the problem and $\bm{g}$ the boundary function. Indeed, the boundary conditions can be Dirichlet, Neumann, Robin, or periodic boundary conditions.



%  address %intendere indicate
Equation~\eqref{eq:general_form} 
can describe numerous physical systems including 
%, those with or without time dependency, as well as 
both forward and inverse problems.
%TODO SPIEGA FORWR INVERSE...
The goal of forward problems is to find the function $\bm{u}$ for every $\bm{z}$, where $\gamma$ are specified parameters.
In the case of the inverse problem, $\gamma$ must also be determined from the data.
%
The reader can find an operator based mathematical formulation
of equation~\eqref{eq:general_form}
in the work of  \cite{Mis2021_EstimatesGeneralizationError_MolMM}.
%%TODO chiarire problmea differenziale
%% FROM \cite{He2020_PhysicsInformedNeural_BarHBTT}
%The boundary conditions can be of the Dirichlet and Neumann types applied on $\partial_D \Omega$ and $\partial_N \Omega$, respectively, such that $\partial_D \Omega \cup \partial_N \Omega = \partial \Omega$ and $\partial_D \Omega \cap \partial_N \Omega = \emptyset$ \cite{He2020_PhysicsInformedNeural_BarHBTT}.
%%%%
% \\
% \noindent
% %TODO ristrutturare: 
% %TODO rapppresenta un surrogato dle problmea differenziale. Chiarire il problema differenziale
% For example, note that in the original formulation %Raissi et al.
% \cite{Rai2018_HiddenPhysicsModels_KarRK, Rai2019_PhysicsInformedNeural_PerRPK}, as well as in many papers, %TODO add 5 citazioni vari di paper che fanno u_t +u_x
% the problem
% % studied are in the broad spectrum of mathematical physics problems encompassing advection-diffusion-reaction systems or kinetic equations.
% %%a broad spectrum of mathematical physics problems such as conservation laws, diffusion processes, advection-diffusion-reaction systems, and kinetic equations 
% %studied how deep neural networks can be confined by partial differential equations (PDE)
% is in the form  
% $$
% \bm{u}_t + \mathcal{N}_{\bm{x}}\bm{u} = 0,
% $$
%  where $\bm{x}$  is a vector of space coordinates, $t$ is a time coordinate, and $\mathcal{N}_{\bm{x}}$ is a nonlinear differential operator.
%  This description covers a wide range of topics, from chemical reactions to continuum mechanics \citep{Kol2021_PhysicsInformedNeural_DAKDJH}.
% %In the PINN methodology $\bm{u}(\bm{x},t)$ is computationally predicted by a Deep NN, $f_{\theta}(\bm{x},t)$, parametrized  by a set of parameters $\theta$.
% %%%%%%%%%%% 


%TODO giestire ponte tra contiuno e approsimaizone.
In the PINN methodology, $\bm{u}(\bm{z})$ is computationally predicted by a NN, parametrized  by a set of parameters $\theta$, giving rise to an approximation $$\hat{\bm{u}}_\theta(\bm{z}) \approx \bm{u}(\bm{z});$$
where $\hat{(\cdot)}_\theta$ denotes a NN approximation realized with $\theta$.

In this context, where forward and inverse problems are analyzed in the same framework, and given that PINN can adequately solve both problems, we will use $\theta$ to represent both the vector of all unknown parameters in the neural network that represents the surrogate model and unknown parameters $\gamma$ in the case of an inverse problem. 


%TODO intropezione ed esmpeio di questo ponte... di appossimaione con rip

%todo CENNO A residui
%todo introdrre loss... cosi da dar senzo alla figura
In such a context, the NN must learn to approximate the differential equations through finding $\theta$ that define the NN by minimizing a loss function that depends on the differential equation $\mathcal{L}_\mathcal{F}$, the boundary conditions $ \mathcal{L}_\mathcal{B}$, and eventually some known data
$\mathcal{L}_{data}$, each of them adequately weighted:
\begin{equation}\label{eq:general_loss}
\theta^* =  \mathop{\argmin}_{\theta} \left(
\omega_\mathcal{F} \mathcal{L}_\mathcal{F}(\theta) +
\omega_\mathcal{B} \mathcal{L}_\mathcal{B}(\theta) +
\omega_{d} \mathcal{L}_{data} (\theta) 
\right).
\end{equation}
%%%%
To summarize, PINN can be viewed as an unsupervised learning approach when they are trained solely using physical equations and boundary conditions for forward problems; however, for inverse problems or when some physical properties are derived from data that may be noisy, PINN can be considered supervised learning methodologies.


In the following paragraph, we will discuss the types of NN used to approximate  $\bm{u}(\bm{z})$, how the information derived by $\mathcal{F}$ is incorporated in the model, and how the NN learns from the equations and additional given data.


Figure~\ref{fig:PINN} summarizes all the PINN's building blocks discussed in this section.
PINN are composed of three components: a neural network, a physics-informed network, and a feedback mechanism.
The first block is a NN, $\hat{\bm{u}}_\theta$,  that accepts vector variables $\bm{z}$  from the equation~\eqref{eq:general_form} and outputs the filed value $\bm{u}$.
%
The second block can be thought of PINN's functional component, as it computes the derivative to determine the losses of equation terms, as well as the terms of the initial and boundary conditions of equation~\eqref{eq:general_loss}.
Generally, the first two blocks are linked using algorithmic differentiation, which is used to inject physical equations into the NN during the training phase.
Thus, the feedback mechanism minimizes the loss according to some learning rate, in order to fix the NN parameters vector $\theta$ of the NN $\hat{\bm{u}}_\theta$. 
In the following, it will be clear from the context to what network we are referring to, whether the NN or the functional network that derives the physical information.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\textwidth]{info_pinn}
    \caption{%
Physics-informed neural networks building blocks. 
PINNs are made up of differential equation residual (loss) terms, as well as initial  and boundary conditions.
%PINNs are made up of differential equation residual (loss) terms (in yellow), as well as initial (red) and boundary conditions (blue).
The network's inputs (variables) are transformed into network outputs (the field $\bm{u}$). 
The network is defined by $\theta$.
The second area is the physics informed network, which takes the output field $\bm{u}$ and computes the derivative using the given equations. The boundary/initial condition is also evaluated (if it has not already been hard-encoded in the neural network), and also the labeled data observations are calculated (in case there is any data available).
The final step with all of these residuals is the feedback mechanism, that minimizes the loss, using an optimizer, according to some learning rate in order to obtain the NN parameters  $\theta$. %, and backpropagate the feedback to the neural network.
}
    \label{fig:PINN}
\end{figure}




%TODO giestire ponte tra contiuno e approsimaizone.
\subsection{Neural Network Architecture}
%TODO ristrutturare: [descrizione contestulizzazione ambito PINN e ossrvazioni.]
%descrizione contestulizzazione  e ossrvazioni
%1. descriver architettura da un punto di vista informatico
%2. esempi usati qui
%3. particolari, una nota specifica

The representational ability of neural networks is well established.
%
According to the universal approximation theorem, any continuous function can be arbitrarily closely approximated by a multi-layer perceptron with only one hidden layer and a finite number of neurons \citep{Hor1989_MultilayerFeedforwardNetworks_StiHSW, Cyb1989_ApproximationSuperpositionsSigmoidal_Cyb, Yar2017_ErrorBoundsApproximations_Yar, Ber2019_SurveyDeepLearning_BucBBCC}.
%
While neural networks can express very complex functions compactly, determining the precise parameters (weights and biases) required to solve a specific PDE can be difficult \citep{Wah2021_PinneikEikonalSolution_HagWHA}.
%
% It has been demonstrated that a neural network with a single hidden layer and a finite number of neurons may accurately represent any bounded continuous function \citep{Hor1989_MultilayerFeedforwardNetworks_StiHSW, Cyb1989_ApproximationSuperpositionsSigmoidal_Cyb}, i.e the so-called universal approximation property, which means that DNN can approximate any continuous function \citep{Bar1993_UniversalApproximationBounds_Bar, Yar2017_ErrorBoundsApproximations_Yar}. 
% While neural networks can express very complex functions compactly, determining the precise parameters (weights and biases) required to solve a specific PDE can be difficult \citep{Wah2021_PinneikEikonalSolution_HagWHA}.
% %%\cite{Ber2019_SurveyDeepLearning_BucBBCC}
% According to the universal approximation theorem, any continuous function that maps intervals of real numbers to some output interval of real numbers can be arbitrarily closely approximated by a multi-layer perceptron with only one hidden layer \citep{Cyb1989_ApproximationSuperpositionsSigmoidal_Cyb, Ber2019_SurveyDeepLearning_BucBBCC}.
%
% In fact, artificial neural networks (ANNs) with a single hidden layer can produce any non-linear continuous function.
% Shallow learning networks are those that have only one hidden layer, while Deep neural networks (DNN) are a type of ANNs that employs more than two layers of neural networks to model complicated  relationships
Furthermore, identifying the appropriate artificial neural network (ANN) architecture can be challenging.
%
There are two approaches: shallow learning networks, which have a single hidden layer and can still produce any non-linear continuous function, and deep neural networks (DNN), a type of ANN that uses more than two layers of neural networks to model complicated relationships
\citep{Ald2020_DeepLearningApproaches_DerADE}.
%There are numerous definitions of deep learning and deep neural networks (DNNs). \cite{Ber2019 SurveyDeepLearning BucBBCC}
Finally, the taxonomy of various Deep Learning architectures is still a matter of research  \citep{Muh2021_DeepLearningApplication_AseMAC, Ald2020_DeepLearningApproaches_DerADE, Sen2020_ReviewDeepLearning_BasSBS}


The main architectures covered in the Deep Learning  literature include fully connected feed-forward networks (called FFNN, or also FNN, FCNN, or FF-DNN), convolutional neural networks (CNNs), and recurrent neural networks (RNNs) \citep{LeC2015_DeepLearning_BenLBH, Che2018_RiseDeepLearning_EngCEW}.
Other more recent architectures encompass Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Bayesian Deep Learning (BDL) \citep{Alo2019_StateArtSurvey_TahATY, Ber2019_SurveyDeepLearning_BucBBCC}.%


Various PINN extensions have been investigated, based on some of these networks. An example is estimating the PINN solution's uncertainty using Bayesian neural networks. Alternatively, when training CNNs and RNNs, finite-difference filters can be used to construct the differential equation residual in a PINN-like loss function. 
This section will illustrate all of these examples and more; first, we will define the mathematical framework for characterizing the various networks. 


%%%%%%%
According to 
\cite{Cat2018_GenericRepresentationNeural_ChaCC}
a generic deep neural network with $L$ layers, can be represented  as the composition of $L$ functions $f_i(x_i,\theta_i)$  where $x_i$ are known as state variables, and
 $\theta_i$ denote the set of parameters for the $i$-th layer. So a function $\bm{u}(x)$ is approximated as
\begin{equation}
\label{eq:dnn}
\bm{u}_{\theta}(x) = f_L \circ f_{L-1}  \ldots   \circ f_1(x).
\end{equation} 
where
each $f_i$ is defined on two inner product spaces $E_i$ and $H_i$, being $f_i\in E_i \times H_i$
and 
the layer composition, represented  by $\circ$, 
is to be read as $f_2 \circ f_1 (x) = f_2(f_1(x))$.



%%%%%%%%%%%%%%


Since \cite{Rai2019_PhysicsInformedNeural_PerRPK} original vanilla PINN, the majority of solutions have used feed-forward neural networks. %, in various versions such as...
%TODO
However, some researchers have experimented with different types of neural networks to see how they affect overall PINN performance, in particular with CNN, RNN, and GAN, and there seems there has been not yet a full investigation of other networks within the PINN framework.
In this subsection, we attempt to synthesize the mainstream solutions, utilizing feed-forward networks, and all of the other versions. 
%
Table~\ref{tab:NeuralNetworks} contains a synthesis reporting the kind of neural network and the paper that implemented it. 


%Feed-Forward Fully-Connected Deep Neural Networks
\subsubsection{Feed-forward neural network}
A feed-forward neural network, also known as a multi-layer perceptron (MLP), is a collection of neurons organized in layers that perform calculations sequentially through the layers. %TODO  FF-DNNs
Feed-forward networks, also known as multilayer perceptrons (MLP), are DNNs with several hidden layers that only move forward (no loopback). 
%%%
In a fully connected neural network, neurons in adjacent layers are connected, whereas neurons inside a single layer are not linked. Each neuron is a mathematical operation that applies an activation function to the weighted sum of it's own inputs plus a bias factor \citep{Wah2021_PinneikEikonalSolution_HagWHA}.
%%%%%  \cite{Mis2021_EstimatesGeneralizationError_MolMM}
Given an input $x \in \Omega$ an MLP transforms it to an output, through a layer of units (neurons) which compose of affine-linear maps between units (in successive layers) and scalar non-linear activation functions within units, resulting in a composition of functions.
So for MLP, it is possible to specify~\eqref{eq:dnn}  as
$$
f_i (x_i ; W_i, b_i) = \alpha_i (W_i \cdot x_i + b_i)
$$
equipping each $E_i$ and $H_i$ with the standard Euclidean inner product, i.e. $E=H=\mathbb{R}$ \citep{Cat2018_SpecificNetworkDescriptions_ChaCC}, and $\alpha_i$ is a scalar (non-linear) activation function.  The machine learning literature has studied several different activation functions, which we shall discuss later in this section.
%
Equation~\eqref{eq:dnn} can also be rewritten in conformity with the notation used in \cite{Mis2021_EstimatesGeneralizationError_MolMM}:
\begin{equation}
\label{eq:ann}
\bm{u}_{\theta}(x) = C_K \circ\alpha \circ C_{K-1}  \ldots  \circ \alpha \circ C_1(x),
\end{equation} 
%
where for any $1 \leq k \leq K$, it it is defined
\begin{equation}
\label{eq:C}
C_k (x_k) = W_k x_k + b_k.
\end{equation}
%where $W_k \in \mathbb{R}^{d_{k+1} \times d_k}$, $z_k \in \mathbb{R}^{d_k}, b_k \in \mathbb{R}^{d_{k+1}}$
Thus a neural network consists of an input layer, an output layer, and $(K-2)$ hidden layers.
%%%%

\paragraph{FFNN architectures}
While the ideal DNN architecture is still an ongoing research; papers implementing PINN have attempted to empirically optimise the architecture's characteristics, such as the number of layers and neurons in each layer.
%
Smaller DNNs may be unable to effectively approximate unknown functions, whereas too large DNNs may be difficult to train, particularly with small datasets. 
%
%%%%
\cite{Rai2019_PhysicsInformedNeural_PerRPK} used different typologies of DNN, for each problem, like a 5-layer deep neural network with 100 neurons per layer, an DNN with 4 hidden layers and 200 neurons per layer or a  9 layers with 20 neuron each layer.
%In further paper an architecture composed of five the feed-forward neural network is proposed by \cite{Hag2021_PhysicsInformedDeep_RaiHRM}.
\cite{Tar2020_PhysicsInformedDeep_MarTMP} 
empirically determine the feedforward network size, in particular they use three hidden layers and 50 units per layer, all with an hyperbolic tangent activation function.
%
Another example of how the differential problem  affects network architecture can be found in \cite{Kha2021_HpVpinnsVariational_ZhaKZK} for their hp-VPINN. The architecture is implemented with four layers and twenty neurons per layer, but for an advection equation with a double discontinuity of the exact solution, they use an eight-layered deep network. 
%%%%
For a constrained approach,  by utilizing a specific portion of the NN to satisfy the required boundary conditions, \cite{Zhu2021_MachineLearningMetal_LiuZLY} use five hidden layers and 250 neurons per layer to constitute the fully connected neural network.
%%%
Bringing the number of layers higher, 
in PINNeik \citep{Wah2021_PinneikEikonalSolution_HagWHA}, a DNN with ten hidden layers containing twenty neurons each is utilized, with a locally adaptive inverse tangent function as the activation function for all hidden layers except the final layer, which has a linear activation function. 
%
%%%%
\cite{He2020_PhysicsInformedNeural_BarHBTT} examines the effect of neural network size on state estimation accuracy.
They begin by experimenting with various hidden layer sizes ranging from three to five, while maintaining a value of 32 neurons per layer.
Then they set the number of hidden layers to three, the activation function to  hyperbolic tangent, while varying the number of neurons in each hidden layer. 
%%%%
Other publications have attempted to understand how the number of layers, neurons, and activation functions effect the NN's approximation quality with respect to the problem to be solved, like \citep{Ble2021_ThreeWaysSolve_ErnBE}.  %TODO raddoppia citazioni.

%FROM \cite{Rai2018_DeepHiddenPhysics_Rai}
%So far, our empirical findings indicate that deeper and wider networks are usually more expressive, but they are often more expensive to train



% %TODO raddoppia citazioni con altri esempi...


\paragraph{Multiple FFNN}  %Patching FNN
Although many publications employ a single fully connected network, a rising number of research papers have been addressing PINN with multiple fully connected network blended together, e.g. to approximate 
%each sub-domain area or 
specific equation of a larger mathematical model.
%%%%
An architecture composed of five the feed-forward neural network is proposed by \cite{Hag2021_PhysicsInformedDeep_RaiHRM}.
%%%%
In a two-phase Stefan problem, discussed later in this review and in \cite{Cai2021_PhysicsInformedNeural_WanCWW},
a DNN is used to model the unknown interface between two different material phases, while another DNN describes the two temperature distributions of the phases.
%%
Instead of a single NN across the entire domain, \cite{Mos2021_FiniteBasisPhysics_MarMMN} suggests using multiple neural networks, one for each subdomain.
%
Finally, \cite{Lu2021_LearningNonlinearOperators_JinLJP} employed a pair of DNN, one for encoding the input space (branch net) and the other for encoding the domain of the output functions (trunk net).
% %TODO raddoppia citazioni con altri esempi...
%%%%%%%%%%%%%%%%%%
This architecture, known as DeepONet, is particularly generic because no requirements are made to the topology of the branch or trunk network, despite the fact that the two sub-networks have been implemented as FFNNs as in \cite{Lin2021_SeamlessMultiscaleOperator_MaxLMLK}. 


\paragraph{Shallow networks}
To overcome some difficulties,
various researchers have also tried to investigate shallower network solutions: these can be sparse neural networks, instead of fully connected architectures, or more likely single hidden layers as ELM (Extreme Learning Machine) \citep{Hua2011_ExtremeLearningMachines_WanHWL}.
%
When compared to the shallow architecture, more hidden layers aid in the modeling of complicated nonlinear relationships \citep{Sen2020_ReviewDeepLearning_BasSBS}, however, 
using PINNs for real problems can result in deep networks with many layers associated with high training costs and efficiency issues. For this reason, not only deep neural networks have been employed for PINNs but also shallow ANN are reported in the literature. 
%TODO raddoppia citazioni.
%
X-TFC, developed by \cite{Sch2021_ExtremeTheoryFunctional_FurSFL}, employs a single-layer NN trained using the ELM algorithm. 
%
While PIELM \citep{Dwi2020_PhysicsInformedExtreme_SriDS} is proposed as a faster alternative, using a hybrid neural network-based method that combines two ideas from PINN and ELM. 
ELM only updates the weights of the outer layer, leaving the weights of the inner layer unchanged.%, being ELMs subject to the universal approximation theorem.
%%
\\
\noindent
Finally, in \cite{Ram2021_SpinnSparsePhysics_RamRR}
a Sparse, Physics-based, and partially Interpretable Neural Networks (SPINN) is proposed.
The authors suggest a sparse architecture, using kernel networks, that yields interpretable results while requiring fewer parameters than fully connected solutions.
They consider various kernels such as Radial Basis Functions (RBF), softplus hat, or Gaussian kernels, and apply their proof of concept architecture to a variety of mathematical problems.



\paragraph{Activation function}
%%% Actaivations per FNN???
The activation function has a significant impact on DNN training performance.
ReLU, Sigmoid, Tanh are commonly used activations \citep{Sun2020_SurrogateModelingFluid_GaoSGPW}.
Recent research has recommended training an adjustable activation function like  Swish, which is defined as $x\cdot \textrm{Sigmoid}(\beta x)$ and $\beta$ is a trainable parameter, %\cite{Sun2020_SurrogateModelingFluid_GaoSGPW}.
and where $\textrm{Sigmoid}$ is supposed to be a general sigmoid curve, an S-shaped function, or in some cases a logistic function. 
%%
Among the most used activation functions there are logistic sigmoid, hyperbolic tangent, ReLu, and leaky ReLu.
Most authors tend to use the infinitely differentiable hyperbolic tangent activation function $\alpha(x) = \tanh(x)$ \citep{He2020_PhysicsInformedNeural_BarHBTT},
%%%%
whereas \cite{Che2021_DeepLearningMethod_ZhaCZ} use a Resnet block to improve the stability of the fully-connected neural network (FC-NN).
%.
They also prove that Swish activation function outperforms the others in terms of enhancing the neural network's convergence rate and accuracy. %\citep{Che2021_DeepLearningMethod_ZhaCZ}. 
%%%% %TODO add what is a ReLU
%Because ReLU's second-order derivative is $0$ (ignoring that is not defined in $x=0$), it is possible that it should not be employed for activation.
Nonetheless, because of the second order derivative evaluation, it is pivotal to choose the activation function in a PINN framework with caution. 
%
For example, while rescaling the PDE to dimensionless form, it is preferable to choose a range of $[0,1]$ rather than a wider domain, because most activation functions (such as Sigmoid, Tanh, Swish) are nonlinear near $0$. 
%
Moreover the regularity of PINNs can be ensured by using smooth activation functions like as the sigmoid and hyperbolic tangent, allowing estimations of PINN generalization error to hold true \citep{Mis2021_EstimatesGeneralizationError_MolMM}. 
%Working with a domain-wide range domain rather than rescaling is difficult because module weights are usually set to be quite small. 






\subsubsection{Convolutional neural networks}

Convolutional neural networks (ConvNet or CNN) are intended to process data in the form of several arrays, for example a color image made of three 2D arrays. %\citep{LeC2015_DeepLearning_BenLBH}.
CNNs usually have a number of convolution and pooling layers. %\citep{Che2018_RiseDeepLearning_EngCEW}.
The convolution layer is made up of a collection of filters (or kernels) that convolve across the full input  rather than general matrix multiplication. %\citep{Muh2021_DeepLearningApplication_AseMAC}.
The pooling layer instead performs subsampling, reducing the dimensionality. %\citep{Ald2020_DeepLearningApproaches_DerADE}.
%%%

%%%%
For CNNs, according to \cite{Cat2018_SpecificNetworkDescriptions_ChaCC}, the layerwise function $f$ can be written as
$$
f_i (x_i; W_i) = \Phi_i (\alpha_i(\mathcal{C}_i(W_i,x_i)))
$$
where $\alpha$ is an elementwise nonlinearity,  $\Phi$ is the max-pooling map, and $\mathcal{C}$  the convolution operator.
%%%%

It is worth noting that the convolution operation preserves translations 
and pooling is unaffected by minor data translations. 
This is applied to input image properties, such as corners, edges, and so on, that are translationally invariant, and will still be represented in the convolution layer's output.

%% what result?
As a result, CNNs perform well with multidimensional data such as images and speech signals, in fact is in the domain of images
%, and encoding and decoding with autoencoders, 
that these networks have been used in a physic informed network framework. 

%%%
For more on these topic the reader can look at 
\cite{LeC2015_DeepLearning_BenLBH, Che2018_RiseDeepLearning_EngCEW, Muh2021_DeepLearningApplication_AseMAC, Ald2020_DeepLearningApproaches_DerADE, Ber2019_SurveyDeepLearning_BucBBCC, Cal2020_ConvolutionalNetworks_Cal}




\paragraph{CNN architectures}  %CNN based architectures
%TODO un po stringato, aggiungere le proprieta di cui gode la CNN, simmetria etc, invarianza, traslazione.. etc..

Because CNNs were originally created for image recognition, they are better suited to handling image-like data and may not be directly applicable to scientific computing problems, as most geometries are irregular with non-uniform grids; for example, Euclidean-distance-based convolution filters lose their invariance on non-uniform meshes.

A physics-informed geometry-adaptive convolutional neural network (PhyGeoNet) was introduced in \cite{Gao2021_PhygeonetPhysicsInformed_SunGSW}.
%as a physics-informed CNN approach for solving parametric PDEs on irregular domains.
PhyGeoNet is a  physics-informed CNN that uses coordinate transformation to convert solution fields from an irregular physical domain to a rectangular reference domain.
%
Additionally, boundary conditions are strictly enforced making it a physics-constrained neural network.
%
\\
\noindent
%
\cite{Fan2021_HighEfficientHybrid_Fan}  observes that a Laplacian operator has been discretized in a convolution operation employing a finite-difference stencil kernel.
Indeed, a Laplacian operator can be discretized using the finite volume approach, and the discretization procedure is equivalent to convolution.
%
As a result, he enhances PINN by using a finite volume numerical approach with a CNN structure. He devised and implemented a CNN-inspired technique in which, rather than using a Cartesian grid, he computes convolution on a mesh or graph.
Furthermore, rather than zero padding, the padding data serve as the boundary condition.
Finally, \cite{Fan2021_HighEfficientHybrid_Fan}  does not use pooling because the data does not require compression. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\paragraph{Convolutional encoder-decoder network}
% convolutional Auto-Encoder network architecture
%
Autoencoders (AE) (or encoder–decoders) are commonly used to reduce dimensionality in a nonlinear way. 
It consists of two NN components: an encoder that translates the data from the input layer to a finite number of hidden units, and a decoder that has an output layer with the same number of nodes as the input layer
\citep{Che2018_RiseDeepLearning_EngCEW}. %\cite{Ber2019_SurveyDeepLearning_BucBBCC}
%%

For modeling stochastic fluid flows, \cite{Zhu2019_PhysicsConstrainedDeep_ZabZZKP}
developed a physics-constrained convolutional encoder-decoder network and a generative model. 
%
%The authors of 
\cite{Zhu2019_PhysicsConstrainedDeep_ZabZZKP}  propose a CNN-based technique for solving stochastic PDEs with high-dimensional spatially changing coefficients, demonstrating that it outperforms FC-NN methods in terms of processing efficiency.
\\
\noindent
AE architecture of the convolutional neural network (CNN) is also used in
\cite{Wan2021_TheoryGuidedAuto_ChaWCZ}. % convolutional Auto-Encoder network architecture
The authors propose a framework called a Theory-guided Auto-Encoder (TgAE) capable of incorporating physical constraints into the convolutional neural network.
\\
\noindent
\cite{Gen2020_ModelingDynamicsPde_ZabGZ} propose a deep auto-regressive dense encoder-decoder and the physics-constrained training algorithm for predicting transient PDEs. They extend this model to a Bayesian framework to quantify both epistemic and aleatoric uncertainty. % convolutional encoder-decoder neural network
%
Finally, \cite{Gru2021_DeepNeuralNetwork_HajGHL} also used an encoder–decoder fully convolutional neural network. 



%%%%%%%%%%%%%%%%
%TODO prima RNN e poi LSTM.  MAGARI INSIME?+parlare di GRU?
\subsubsection{Recurrent neural networks}
Recurrent neural network (RNN) is a further ANN type, where unlike feed-forward NNs, neurons in the same hidden layer are connected to form a directed cycle. RNNs may accept sequential data as input, making them ideal for time-dependent tasks \citep{Che2018_RiseDeepLearning_EngCEW}.
The RNN processes inputs one at a time, using the hidden unit output as extra input for the next element \citep{Ber2019_SurveyDeepLearning_BucBBCC}.
An RNN's hidden units can keep a state vector that holds a memory of previous occurrences in the sequence.

It is feasible to think of RNN in two ways: first, as a state system with the property that each state, except the first, yields an outcome; secondly, as a sequence of vanilla feedforward neural networks, each of which feeds information from one hidden layer to the next. %\citep{Cat2018_SpecificNetworkDescriptions_ChaCC}
%%%%
For RNNs, according to \cite{Cat2018_SpecificNetworkDescriptions_ChaCC}, the layerwise function $f$ can be written as
$$
f_i (h_{i-1}) = \alpha (W \cdot h_{i-1} + U \cdot x_i + b).
$$
where $\alpha$ is an elementwise nonlinearity (a typical choice for RNN is the tanh function), 
and where  the hidden vector state $h$  evolves according to
a hidden-to-hidden weight matrix $W$, which starts from an input-to-hidden weight matrix
$U$ and a bias vector $b$.
%%%%


%RNNs are typically more challenging to train since gradients can readily vanish or explode.
RNNs have also been enhanced with several memory unit types, such as long short time memory (LSTM) and gated recurrent unit (GRU) \citep{Ald2020_DeepLearningApproaches_DerADE}.
Long short-term memory (LSTM) units  have been created to allow RNNs to handle challenges requiring long-term memories, since LSTM units have a structure called a memory cell that stores information. % LSTM overcomes the vanishing gradient problem.
\\
\noindent
Each LSTM layer has a set of interacting units, or cells, similar to those found in a neural network. An LSTM is made up of four interacting units: an internal cell, an input gate, a forget gate, and an output gate. The cell state, controlled by the gates, can selectively propagate relevant information throughout the temporal sequence to capture the long short-term time dependency in a dynamical system \citep{Zha2020_PhysicsInformedMulti_LiuZLS}.
%%%%%%%%%%%%%%%%
\\
\noindent
The gated recurrent unit (GRU) is another RNN unit developed for extended memory;
GRUs are comparable to LSTM, however they contain fewer parameters and are hence easier to train \citep{Ber2019_SurveyDeepLearning_BucBBCC}.

%\cite{Ald2020_DeepLearningApproaches_DerADE, Ber2019_SurveyDeepLearning_BucBBCC, Muh2021_DeepLearningApplication_AseMAC, Che2018_RiseDeepLearning_EngCEW}

\paragraph{RNN architectures}
\cite{Via2021_EstimatingModelInadequacy_NasVNDY} introduce, in the form of a neural network, a model discrepancy term into a given ordinary differential equation.
%
Recurrent neural networks are seen as ideal for dynamical systems because they expand classic feedforward networks to incorporate time-dependent responses. %\citep{Via2021_EstimatingModelInadequacy_NasVNDY}.
%
Because a recurrent neural network applies transformations to given states in a sequence on a periodic basis, it is possible to design a recurrent neural network cell that does Euler integration; in fact, physics-informed recurrent neural networks can be used to perform numerical integration. 
%\citep{Via2021_EstimatingModelInadequacy_NasVNDY}.
\\
\noindent
\cite{Via2021_EstimatingModelInadequacy_NasVNDY} 
build recurrent neural network cells in such a way that specific numerical integration methods (e.g., Euler, Riemann, Runge-Kutta, etc.) are employed. The recurrent neural network is then represented as a directed graph, with nodes representing individual kernels of the physics-informed model.
The graph created for the physics-informed model can be used to add data-driven nodes (such as multilayer perceptrons) to adjust the outputs of certain nodes in the graph, minimizing model discrepancy.


\paragraph{LSTM architectures}
%TODO intro cosa e' un LSTM (sta scritto un po ala fine),.. che ha a che fare ocn sequenze
%TODO  [descrizione contestulizzazione ambito PINN e ossrvazioni.]
Physicists have typically employed distinct LSTM networks to depict the sequence-to-sequence input-output relationship; however, these networks are not homogeneous and cannot be directly connected to one another.
%
In \cite{Zha2020_PhysicsInformedMulti_LiuZLS} this relationship is expressed using a single network and a central finite difference filter-based numerical differentiator.
\cite{Zha2020_PhysicsInformedMulti_LiuZLS} show two architectures 
%($PhyLSTM^2$ and $PhyLSTM^3$) 
for representation learning of sequence-to-sequence features from limited data that is augmented by physics models.
The proposed networks 
%in \cite{Zha2020_PhysicsInformedMulti_LiuZLS}  
is made up of two ($PhyLSTM^2$) or three ($PhyLSTM^3$) deep LSTM networks that describe state space variables, nonlinear restoring force, and hysteretic parameter. Finally, a tensor differentiator, which determines the derivative of state space variables, connects the LSTM networks.
\\
\noindent
Another approach is \cite{Yuc2021_HybridPhysicsInformed_ViaYV}
for temporal integration, that implement an LSTM using a previously introduced Euler integration cell.





\subsubsection{Other architectures for PINN}
%Other NN approaches for PINN   -    Other approaches for PINN

Apart from fully connected feed forward neural networks, convolutional neural networks, and recurrent neural networks, this section discusses other approaches that have been investigated. While there have been numerous other networks proposed in the literature, we discovered that only Bayesian neural networks (BNNs) and generative adversarial networks (GANs) have been applied to PINNs. Finally, an interesting application is to combine multiple PINNs, each with its own neural network. 


%Neural networks designed for Ordianry Differential Equations (ODEs) are also used in literature, and Hamiltonian neural networks are compared against ODE-net and Taylor-net in \cite{Ton2021_SymplecticNeuralNetworks_XioTXH}. In the paper they use Symplectic Taylor neural networks (Taylor-nets) and ODE-net a kind of  deep neural network models.


\paragraph{Bayesian Neural Network}
In the Bayesian framework, \cite{Yan2021_BPinnsBayesian_MenYMK} propose to use Bayesian neural networks (BNNs), in their B-PINNs, that consists of a Bayesian neural network subject to the PDE constraint that acts as a prior. 
BNN are neural networks with weights that are distributions rather than deterministic values, and these distributions are learned using Bayesian inference.  %\cite{Yan2021_BPinnsBayesian_MenYMK}
For estimating the posterior distributions, the B-PINN authors use  the Hamiltonian Monte Carlo (HMC) method and the variational inference (VI). 
%
\cite{Yan2021_BPinnsBayesian_MenYMK} find that for the posterior estimate of B-PINNs, HMC is more appropriate than VI with mean field Gaussian approximation. 
\\
\noindent
They analyse also the possibility to use the  Karhunen-Loève expansion as a stochastic process representation, instead of BNN.
Although the KL is as accurate as BNN and considerably quicker, it cannot be easily applied to high-dimensional situations. 
%
Finally, they observe that to estimate the posterior of a Bayesian framework, KL-HMC or deep normalizing flow (DNF) models can be employed.
While DNF is more computationally expensive than HMC, it is more capable of extracting independent samples from the target distribution after training. 
This might be useful for data-driven PDE solutions, however it is only applicable to low-dimensional problems.


\paragraph{GAN architectures}
%%%%%%%%%%%%%%%%%%%%%%%  \cite{Ber2019_SurveyDeepLearning_BucBBCC}
In generative adversarial networks (GANs), two neural networks compete in a zero-sum game to deceive each other.
One network generates and the other discriminates.
The generator accepts input data and outputs data with realistic characteristics.
The discriminator compares the real input data to the output of the generator.
After training, the generator can generate new data that is indistinguishable from real data
\citep{Ber2019_SurveyDeepLearning_BucBBCC}.

\cite{Yan2020_PhysicsInformedGenerative_ZhaYZK} propose a new class of generative adversarial networks (PI-GANs) to address forward, inverse, and mixed stochastic problems in a unified manner. Unlike typical GANs, which rely purely on data for training, PI-GANs use automatic differentiation to embed the governing physical laws in the form of stochastic differential equations (SDEs) into the architecture of PINNs. 
%PI-GANs are constituted by a discriminator and a generator. 
The discriminator in PI-GAN is represented by a basic FFNN, while the generators are a combination of FFNNs and a NN induced by the SDE. 
%The discriminator is represented by a basic FFNN, and generators are a combination of FFNNs and a NN induced by the SDE.



\paragraph{Multiple PINNs} 
A final possibility is to combine several PINNs, each of which could be implemented using a different neural network. 
% %Jag et al. 
% \cite{Jag2020_ConservativePhysicsInformed_KhaJKK}
%  propose a conservative physics-informed neural network (cPINN) on discrete domains.
% In this framework, the complete solution is recreated by patching together all of the solutions in each sub-domain using the appropriate interface conditions.
% This method may be expanded to a more general situation, called by the authors as Mortar PINN, for connecting non-overlapping deconstructed domains.
% The suggested technique has the domain decomposition feature, with distinct neural networks in each sub-domain, and thus lends itself well to parallelized computing.
% \\
% \noindent
% cPINN \citep{Jag2020_ConservativePhysicsInformed_KhaJKK}
%  splits the domain into a number of tiny subdomains in which it may use totally distinct neural networks with various architectures to solve the same underlying PDE.
% This type of domain segmentation also allows for easy network parallelization, which is critical for obtaining computing efficiency.
\cite{Jag2020_ConservativePhysicsInformed_KhaJKK}
propose a conservative physics-informed neural network (cPINN) on discrete domains. In this framework, the complete solution is recreated by patching together all of the solutions in each sub-domain using the appropriate interface conditions. 
This type of domain segmentation also allows for easy network parallelization, which is critical for obtaining computing efficiency.
This method may be expanded to a more general situation, called by the authors as Mortar PINN, for connecting non-overlapping deconstructed domains.
Moreover, the suggested technique may use totally distinct neural networks, in each subdomain, with various architectures to solve the same underlying PDE.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\\
\noindent
\cite{Sti2020_LargeScaleNeural_BetSBB} proposes the GatedPINN architecture by incorporating conditional computation into PINN.
This architecture design is composed of a  gating network and set of PINNs, hereinafter referred to as ``experts''; each expert solves the problem for each space-time point, and their results are integrated via a gating network.
The gating network determines which expert should be used and how to combine them.
We will use one of the expert networks as an example in the following section of this review.





% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[hbt!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{} c  l  l  @{}}
\toprule
\textbf{NN family} &
  \textbf{NN type} &
  \textbf{Papers} \\ \midrule
\multirow{6}{*}{FF-NN} &
  1 layer / EML &
  \begin{tabular}[c]{@{}l@{}}\cite{Dwi2020_PhysicsInformedExtreme_SriDS},\\ \cite{Sch2021_ExtremeTheoryFunctional_FurSFL}\end{tabular} \\ \cline{2-3} 
 &
  2-4 layers &
  \begin{tabular}[c]{@{}l@{}}32 neurons per layer \cite{He2020_PhysicsInformedNeural_BarHBTT} \\ 50 neurons per layer \cite{Tar2020_PhysicsInformedDeep_MarTMP}\end{tabular} \\ \cline{2-3} 
 &
  5-8 layers &
  250 neurons per layer \cite{Zhu2021_MachineLearningMetal_LiuZLY} \\ \cline{2-3} 
 &
  9+ layers &
  \begin{tabular}[c]{@{}l@{}}\cite{Che2021_DeepLearningMethod_ZhaCZ}\\ \cite{Wah2021_PinneikEikonalSolution_HagWHA}\end{tabular} \\ \cline{2-3} 
 &
  Sparse &
  \cite{Ram2021_SpinnSparsePhysics_RamRR} \\ \cline{2-3} 
 &
  multi FC-DNN &
  \begin{tabular}[c]{@{}l@{}}\cite{Ami2021_PhysicsInformedNeural_HagANHC} \\ \cite{Isl2021_ExtractionMaterialProperties_ThaITMH} \end{tabular} \\ \midrule
\multirow{2}{*}{CNN} &
  plain CNN &
  \begin{tabular}[c]{@{}l@{}}\cite{Gao2021_PhygeonetPhysicsInformed_SunGSW}\\ \cite{Fan2021_HighEfficientHybrid_Fan} %\\ %\cite{Gru2021_DeepNeuralNetwork_HajGHL}
  \end{tabular} \\ \cline{2-3} 
 &
  AE CNN &
  \begin{tabular}[c]{@{}l@{}}\cite{Zhu2019_PhysicsConstrainedDeep_ZabZZKP}, \\ \cite{Gen2020_ModelingDynamicsPde_ZabGZ} \\ \cite{Wan2021_TheoryGuidedAuto_ChaWCZ}\end{tabular} \\ \midrule
\multirow{2}{*}{RNN} &
  RNN &
  \cite{Via2021_EstimatingModelInadequacy_NasVNDY} \\ \cline{2-3} 
 &
  LSTM &
  \begin{tabular}[c]{@{}l@{}}\cite{Zha2020_PhysicsInformedMulti_LiuZLS}\\ \cite{Yuc2021_HybridPhysicsInformed_ViaYV}\end{tabular} \\ \midrule
\multirow{2}{*}{Other} &
  BNN &
  \cite{Yan2021_BPinnsBayesian_MenYMK} \\ \cline{2-3} 
 &
  GAN &
  \cite{Yan2020_PhysicsInformedGenerative_ZhaYZK} \\ \bottomrule
\end{tabular}%
}
\caption{The main neural network utilized in PINN implementations is synthesized in this table.
We summarize Section 2 by showcasing some of the papers that represent each of the many Neural Network implementations of PINN.
Feedforward neural networks (FFNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNN) are the three major families of Neural Networks reported here.
A publication is reported for each type that either used this type of network first or best describes its implementation.
In the literature, PINNs have mostly been implemented with FFNNs with 5-10 layers.
CNN appears to have been applied in a PCNN manner for the first time, by incorporating the boundary condition into the neural network structure rather than the loss. 
}
\label{tab:NeuralNetworks}
\end{table}




%TODO 2.2 un po confuso, amalgamare tutto il paragrafo
\subsection{Injection of Physical laws}
%Last layer - derivation, ie. law enformment
%Model information
%Physical behaviour   - Physical informations injection  application

To solve a PDE with PINNs, derivatives of the network's output with respect to the inputs are needed.
Since the function $\bm{u}$ is approximated by a NN with smooth activation function,
$\hat{\bm{u}}_\theta$, it
%$u_\theta$, 
can be differentiated. 
%
There are four methods for calculating derivatives: hand-coded, symbolic, numerical, and automatic.
Manually calculating derivatives may be correct, but it is not automated and thus impractical \citep{Wah2021_PinneikEikonalSolution_HagWHA}.

Symbolic and numerical methods like finite differentiation perform very badly when applied to complex functions; automatic differentiation (AD), on the other hand, overcomes numerous restrictions as floating-point precision errors, for numerical differentiation, or memory intensive symbolic approach.
AD use exact expressions with floating-point values rather than symbolic strings, there is no approximation error \citep{Wah2021_PinneikEikonalSolution_HagWHA}.

Automatic differentiation (AD) is also known as autodiff, or algorithmic differentiation, although it would be better to call it algorithmic differentiation since AD does not totally automate differentiation: instead of symbolically evaluating derivatives operations, AD performs an analytical evaluation of them.

Considering  a function
$f:\mathbb{R}^n\to\mathbb{R}^m$ of which we want to calculate the Jacobian $J$,  after calculating the graph of all the operations composing the mathematical expression, AD can then work in either forward or reverse mode for calculating the numerical derivative.

%Forward mode AD evaluates a numerical derivative by conducting the elementary derivative operations. 
%While reverse AD uses the computational graph to compute a gradient by traversing the graph backward, and requiring a number of sweeps as much as the size of the number of variables $m$.

%Reverse mode calculates the gradient more effectively than the forward mode for a function whose number of variables is greater than the number of components $m>>n$, 
%Furthermore, AD must be carefully executed in order to completely express its potential and be effective in a number of settings. 

%Apart form this limitation,
AD results being the main  technique used in literature and used by all PINN implementations, in particular only 
\cite{Fan2021_HighEfficientHybrid_Fan}
use local fitting method
approximation of the differential operator to solve the PDEs instead of automatic differentiation (AD).
Moreover, by using local fitting method rather than employing automated differentiation, Fang is able to verify that a his PINN implementation has a convergence.
%TODO , a result quite unique in literature.



%TODO? conter example of one authror? fang

Essentially, AD incorporates a PDE into the neural network's loss equation~\eqref{eq:general_loss}, where
the differential equation residual is 
$$
r_\mathcal{F}[ \hat{\bm{u}}_{\theta}](\bm{z}) = r_{\theta}(\bm{z}):= \mathcal{F}( \hat{\bm{u}}_{\theta} (\bm{z}); \gamma  ) -  \bm{f},
$$
and similarly the residual NN corresponding to boundary conditions (BC) or initial conditions (IC) is obtained by substituting $\hat{\bm{u}}_{\theta}$ in the second equation of~\eqref{eq:general_form}, i.e.
$$
r_\mathcal{B}[ \hat{\bm{u}}_{\theta}](\bm{z}) :=
\mathcal{B}(\hat{\bm{u}}_{\theta}(\bm{z})) -  \bm{g}(\bm{z}).
$$
%
Using these residuals, it is possible to assess how well an approximation $u_{\theta}$ satisfies~\eqref{eq:general_form}.
It is worth noting that for the exact solution, $u$, the residuals are $r_\mathcal{F}[u]=r_\mathcal{B}[u]=0$ \citep{De2022_ErrorEstimatesPhysics_JagDRJM}.


In %Raissi et al. 
\cite{Rai2018_HiddenPhysicsModels_KarRK, Rai2019_PhysicsInformedNeural_PerRPK}, the original formulation of the aforementioned differential equation residual, leads to the form of
$$
r_\mathcal{F}[ \hat{u}_{\theta}](\bm{z}) =  r_{\theta}(\bm{x},t)= \frac{\partial}{\partial t} \hat{u}_{\theta}(\bm{x},t)  + \mathcal{F}_{\bm{x}}\hat{u}_{\theta}(\bm{x},t).
$$


In the deep learning framework, the principle of imposing physical constraints is represented by differentiating neural networks with respect to input spatiotemporal coordinates using the chain rule. 
%
In \cite{Mat2021_UncoveringTurbulentPlasma_FraMFH} the model loss functions are embedded and then further normalized into dimensionless form.
%
The repeated differentiation, with AD, and composition of networks used to create each individual term in the partial differential equations results in a much larger resultant computational graph.
As a result, the cumulative computation graph is effectively an approximation of the PDE equations \citep{Mat2021_UncoveringTurbulentPlasma_FraMFH}.



%%%%%%%%%%%%%%%
The chain rule is used in automatic differentiation for several layers to compute derivatives hierarchically from the output layer to the input layer.
Nonetheless, there are some situations in which the basic chain rule does not apply.
\cite{Pan2019_FpinnsFractionalPhysics_LuPLK}  substitute fractional differential operators with their discrete versions, which are subsequently incorporated into the PINNs' loss function. 
%%%%%%%%%%%%%%%%%%%%









\subsection{Model estimation by learning approaches}\label{sec:ModelEstimation}
% Learning procedure
%Training procedure - Loss function design
% Model identification by learning approaches

The PINN methodology determines the parameters $\theta$ of the NN, $\hat{\bm{u}}_\theta$, by minimizing a loss function, i.e.
\begin{equation*}
\theta =  \mathop{\argmin}_{\theta}  \mathcal{L}(\theta) 
\end{equation*}
where
\begin{equation}\label{eq:loss_pinn}
\mathcal{L}(\theta) = 
\omega_\mathcal{F} \mathcal{L}_\mathcal{F}(\theta) +
\omega_\mathcal{B} \mathcal{L}_\mathcal{B}(\theta) +
\omega_{d} \mathcal{L}_{data} (\theta) 
.
\end{equation}
%%%%
The three terms of $\mathcal{L}$ refer to the errors in describing the initial $\mathcal{L}_{i}$ or boundary condition $\mathcal{L}_{b}$, both indicated as $\mathcal{L}_\mathcal{B}$, the loss respect the partial differential equation $\mathcal{L}_{\mathcal{F}}$, and the validation of known data points $\mathcal{L}_{data}$. 
Losses are usually defined in the literature as a sum, similar to the previous equations, however they can be considered as integrals
\begin{equation*}
\mathcal{L}_\mathcal{F}(\theta) 
= 
\int_{\bar{\Omega}} \left( \mathcal{F} (\hat{\bm{u}}_\theta(\bm{z}))   - \bm{f}(\bm{z}_i)) \right)^2 \, d\bm{z}
\end{equation*}
This formulation is not only useful for a theoretical study, as we will see in~\ref{sec:Theory}, but it is also implemented in a PINN package, NVIDIA Modulus \citep{Modulus2021}, allowing for more effective integration strategies, such as sampling with higher frequency in specific areas of the domain to more efficiently approximate the integral losses. 


Note that, if the PINN framework is employed as a supervised methodology, the neural network parameters are chosen by minimizing the difference between the observed outputs and the model's predictions; otherwise, just the PDE residuals are taken into account. 

As in equation~\eqref{eq:loss_pinn} the first term, $\mathcal{L}_\mathcal{F}$, represents the loss produced by a mismatch with the governing differential equations $\mathcal{F}$ \citep{He2020_PhysicsInformedNeural_BarHBTT, Sti2020_LargeScaleNeural_BetSBB}.
It enforces the differential equation $\mathcal{F}$ at the \emph{collocation points}, which can be chosen uniformly or unevenly over the domain $\Omega$ of equation~\eqref{eq:general_form}.
%; during the training of the NN this loss essentially attempts to satisfy the differential equation at the collocations points.
%this element essentially trains the NN by attempting to satisfy the differential equation at the collocations points .
\\
\noindent
The remaining two losses attempt to fit the known data over the NN.
The loss caused by a mismatch with the data (i.e., the measurements of $\bm{u}$) is denoted by $\mathcal{L}_{data} (\theta)$.
%
The second term typically forces $\hat{\bm{u}}_\theta$ to mach the measurements of $\bm{u}$ over provided points $(\bm{z}, \bm{u}^*)$, which can be given as synthetic data or actual measurements, and the weight $\omega_d$ can account for the quality of such measurements.
\\
\noindent
The other term is the loss due to mismatch with the boundary or initial conditions, $\mathcal{B} (\hat{\bm{u}}_\theta) = \bm{g}$ from equation~\eqref{eq:general_form}.
%
%%%\cite{He2020_PhysicsInformedNeural_BarHBTT}.
%In \eqref{eq:loss_pinn}, $\omega_\mathcal{F}$, and $\omega_\mathcal{B}$ are weights that affect how strongly mismatch with the governing PDEs and boundary conditions are penalized relative to data mismatch \cite{He2020_PhysicsInformedNeural_BarHBTT}.
%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%% From \cite{Yan2019_AdversarialUncertaintyQuantification_PerYP}. 
Essentially, the training approach recovers the shared network parameters $\theta$ from:
\begin{itemize}
    \item few scattered observations of $\bm{u}(\bm{z})$, specifically $\{\bm{z}_{i}, \bm{u}_i^*\}$, $i = 1,\dots,N_d$
    \item as well as a greater number of collocation points $\{\bm{z}_{i}, \bm{r}_i = 0\}$, $i = 1,\dots,N_r$ for the residual,
\end{itemize}

The resulting optimization problem can be handled using normal stochastic gradient descent without the need for constrained optimization approaches by minimizing the combined loss function.
A typical implementation of the loss uses a mean square error formulation \citep{Kol2021_PhysicsInformedNeural_DAKDJH}, where:
\begin{equation*}
\mathcal{L}_\mathcal{F}(\theta) =  MSE_\mathcal{F} 
= \frac{1}{N_c} \sum_{i=1}^{N_c} \|
\mathcal{F} (\hat{\bm{u}}_\theta(\bm{z}_i))   - \bm{f}(\bm{z}_i))
\|^2    
= \frac{1}{N_c} \sum_{i=1}^{N_c} \|r_{\theta}(\bm{u}_{i}) - \bm{r}_i\|^2
\end{equation*}
enforces the PDE on a wide set of randomly selected collocation locations inside the domain, i.e. penalizes the difference between the estimated left-hand side of a PDE and the known right-hand side of a PDE  \citep{Kol2021_PhysicsInformedNeural_DAKDJH}; %
other approaches may employ an integral definition of the loss \citep{Hen2021_NvidiaSimnetAi_NarHNN}.
As for the boundary and initial conditions, instead 
\begin{equation*}
\mathcal{L}_\mathcal{B}(\theta)  = MSE_\mathcal{B} 
= \frac{1}{N_B} \sum_{i=1}^{N_B} 
\|
\mathcal{B} (\hat{\bm{u}}_\theta(\bm{z}))   - \bm{g}(\bm{z}_i))
\|^2    
%= \frac{1}{N_B} \sum_{i=1}^{N_B} \|r_{\theta}(\bm{u}^{i}_B) - \bm{r}^i_B\|^2.
\end{equation*}
%
whereas for the data points,
\begin{equation*}
\mathcal{L}_{data}(\theta)  = MSE_{data}  = \frac{1}{N_d}\sum\limits_{i=1}^{N_d}\|\hat{\bm{u}}_{\theta}(\bm{z}_{i}) - \bm{u}_i^*\|^2 .
\end{equation*}
computes the error of the approximation $u(t, x)$ at known data points. In the case of a forward problem, the data loss might also indicate the boundary and initial conditions, while in an inverse problem it refers to the solution at various places inside the domain  \citep{Kol2021_PhysicsInformedNeural_DAKDJH}.

%%%%%%%%%%%%%%%%%

%TODO verifica meglio!!!!
In %Raissi et al. 
\cite{Rai2018_HiddenPhysicsModels_KarRK, Rai2019_PhysicsInformedNeural_PerRPK}, original approach the overall loss~\eqref{eq:loss_pinn} was formulated as
$$
\mathcal{L}(\theta) =   \frac{1}{N_c} \sum_{i=1}^{N_c} \| \frac{\partial}{\partial t} \hat{u}_{\theta}(\bm{x},t)  + \mathcal{F}_{\bm{x}}\hat{u}_{\theta}(\bm{x},t) - \bm{r}_i\|^2
 +
 \frac{1}{N_d}\sum\limits_{i=1}^{N_d}\|\hat{\bm{u}}_{\theta}(x_{i},t_i) - \bm{u}_i^*\|^2 .
$$
 
%%%%%%%%%%%%%%%%%
The gradients in $ \mathcal{F}$ are derived using automated differentiation.
The resulting predictions are thus driven to inherit any physical attributes imposed by the PDE constraint \citep{Yan2019_AdversarialUncertaintyQuantification_PerYP}.  
The physics constraints are included in the loss function to enforce model training, which can accurately reflect latent system nonlinearity even when training data points are scarce \citep{Zha2020_PhysicsInformedMulti_LiuZLS}.
%%%%%%%%%%%%%%% 



\paragraph{Observations about the loss}
%\subsection{Loss observations}

%%%%%%%%%%%%%%%%%%  \cite{He2020_PhysicsInformedNeural_BarHBTT}.
The loss $\mathcal{L}_\mathcal{F}(\theta)$ is calculated by utilizing automated differentiation (AD) to compute the derivatives of $\hat{\bm{u}}_\theta(\bm{z})$ \citep{He2020_PhysicsInformedNeural_BarHBTT}.
Most ML libraries, including TensorFlow and Pytorch, provide AD, which is mostly used to compute derivatives with respect to DNN weights (i.e. $\theta$).
AD permits the PINN approach to implement any PDE and boundary condition requirements without numerically discretizing and solving the PDE \citep{He2020_PhysicsInformedNeural_BarHBTT}.

Additionally, by applying PDE constraints via the penalty term $\mathcal{L}_\mathcal{F}(\theta)$, it is possible to use the related weight $\omega_\mathcal{F}$ to account for the PDE model's fidelity. %\citep{He2020_PhysicsInformedNeural_BarHBTT}.
To a low-fidelity PDE model, for example, can be given a lower weight.
In general, the number of unknown parameters in $\theta$ is substantially greater than the number of measurements, therefore regularization is required for DNN training \citep{He2020_PhysicsInformedNeural_BarHBTT}.
%The losses $\mathcal{L}_\mathcal{F}$ and $\mathcal{L}_\mathcal{B}$ in the minimization problem can be viewed as physics-informed regularization terms \cite{He2020_PhysicsInformedNeural_BarHBTT}.
%%%%%%%%%%%%%%%%%%%%%%%%



By removing loss for equations from the optimization process (i.e., setting $\omega_\mathcal{F}=0$), neural networks could be trained without any knowledge of the underlying governing equations.
Alternatively, supplying initial and boundary conditions for all dynamical variables would correspond to solving the equations directly with neural networks on a regular basis \citep{Mat2021_UncoveringTurbulentPlasma_FraMFH}.
%The architecture is noise-robust, even when subjected to strong Gaussian noise.
%The deep learning architecture can learn the right turbulence density and infer the unmeasured electric field from extremely noisy data.


%%%%%%%%%%%%%%%%%%%%%%%%
While it is preferable to enforce the physics model across the entire domain, the computational cost of estimating and reducing the loss function~\eqref{eq:loss_pinn}, while training,  grows with the number of residual points \citep{He2020_PhysicsInformedNeural_BarHBTT}.  
Apart the number of residual points, also the position (distribution) of residual points are crucial parameters in PINNs because they can change the design of the loss function \citep{Mao2020_PhysicsInformedNeural_JagMJK}.

%TODO imporve
A deep neural network can reduce approximation error by increasing network expressivity, but it can also produce a large generalization error.
%
Other hyperparameters, such as learning rate, number of iterations, and so on, can be adjusted to further control and improve this issue. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%

The addition of extra parameters layer by layer  in a NN modifies the slope of the activation function in each hidden-layer, improving the training speed.
Through the slope recovery term, these activation slopes can also contribute to the loss function \citep{Jag2020_ConservativePhysicsInformed_KhaJKK}. 





\paragraph{Soft and hard constraint}

%%%%%%%%%%%
BC constraints can be regarded as penalty terms (soft BC enforcement) \citep{Zhu2019_PhysicsConstrainedDeep_ZabZZKP},
 or they can be encoded into the network design (hard BC enforcement) \citep{Sun2020_SurrogateModelingFluid_GaoSGPW}.
%%%%%%%%%% \cite{Gao2021_PhygeonetPhysicsInformed_SunGSW}
%%OSS. nota SUN nel 2020 ha introdotto per prima l'hard BC
%
%%%%%%%%%%%%%%%%%%%%
Many existing PINN frameworks use a \emph{soft} approach to constrain the BCs by creating extra loss components defined on the collocation points of borders.
The disadvantages of this technique are twofold:
\begin{enumerate}
    \item  satisfying the BCs accurately is not guaranteed;
    \item the assigned weight of BC loss might effect learning efficiency, and no theory exists to guide determining the weights at this time.
\end{enumerate}
%(1) (2)
%
\cite{Zhu2021_MachineLearningMetal_LiuZLY} address the Dirichlet BC in a \emph{hard} approach by employing a specific component of the neural network to purely meet the specified Dirichlet BC. 
Therefore, the initial boundary conditions are regarded as part of the labeled data constraint.% in \citep{Zhu2021_MachineLearningMetal_LiuZLY}. 
%%%%%%%%%%%%%%%%%%%%%%%



% The PDE boundary residual can be introduced as a soft penalty constraint penalty in the function of the model using a regularization approach \citep{Yan2019_AdversarialUncertaintyQuantification_PerYP}.  
% %
% The ``soft'' way on the physical parameters is originally formulated as %Raissi \cite{Rai2019_PhysicsInformedNeural_PerRPK}
% \begin{multline*}
%     \mathcal{L}(\mathbf{W}, \mathbf{b}, \omega_i, \omega_b; \boldsymbol{\theta}) = \underbrace{\mathcal{L}_{\mathcal{F}}(\mathbf{W}, \mathbf{b})}_{\mathrm{Equation \ loss}} + \underbrace{\omega_i \norm{\mathcal{I}(\mathbf{x}, \mathbf{u}, \boldsymbol{\theta}; \boldsymbol{\gamma})}_{\Omega}}_{\mathrm{Initial \ loss}} + \\
%     + \underbrace{\omega_b\norm{\mathcal{B}(t, \mathbf{x}, \mathbf{u}, \boldsymbol{\theta}; \boldsymbol{\gamma} )}_{\partial\Omega}}_{\mathrm{Boundary \ loss}},
% \end{multline*}
% where the penalty coefficients are  $\omega_i$ and $\omega_b$ over initial an boundary conditions.



When compared to the residual-based loss functions typically found in the literature, the variational energy-based loss function is simpler to minimize and so performs better
\citep{Gos2020_TransferLearningEnhanced_AniGACR}.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
Loss function can be constructed using
collocation points,
weighted residuals derived by the Galerkin-Method \citep{Kha2019_VariationalPhysicsInformed_ZhaKZK},
or energy based.
%
%%%%%%%%%%%%%%%%%%%
Alternative loss functions approaches are compared in \cite{Li2021_PhysicsGuidedNeural_BazLBZ}, by using either only data-driven (with no physics model), a PDE-based loss, and an energy-based loss. 
They observe that there are advantages and disadvantages for both PDE-based and  energy-based approaches. PDE-based loss function has more hyperparameters than the energy-based loss function. The energy-based strategy is more sensitive to the size and resolution of the training samples than the PDE-based strategy, but it is more computationally efficient. 


\paragraph{Optimization methods}
%Optimization - Learning procedure

The minimization process of the loss function is called \emph{training};
%
in most of the PINN literature, loss functions are optimized using minibatch sampling using 
%stochastic gradient descent (SGD) via 
Adam and the limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, a quasi-Newton optimization algorithm. When monitoring noisy data, \cite{Mat2021_UncoveringTurbulentPlasma_FraMFH} found that increasing the sample size and training only with L-BFGS achieved the optimum for learning.

%%%%%%%%%%%%%%%%%%%%%%%%%
For a moderately sized NN, such as one with four hidden layers (depth of the NN is 5) and twenty neurons in each layer (width of the NN is 20), we have over 1000 parameters to optimize.
There are several local minima for the loss function, and the gradient-based optimizer will almost certainly become caught in one of them; finding global minima is an NP-hard problem \citep{Pan2019_FpinnsFractionalPhysics_LuPLK}.
%%%%%%%%%%%%%%%%%%%%%%

The Adam approach, which combines adaptive learning rate and momentum methods, is employed in \cite{Zhu2021_MachineLearningMetal_LiuZLY} to increase convergence speed, because stochastic gradient descent (SGD) hardly manages random collocation points, especially in 3D setup.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cite{Yan2020_PhysicsInformedGenerative_ZhaYZK} use Wasserstein GANs with gradient penalty (WGAN-GP) and prove that they are more stable than vanilla GANs, in particular for approximating stochastic processes with deterministic boundary conditions.

cPINN \citep{Jag2020_ConservativePhysicsInformed_KhaJKK} allow to flexibly select network hyper-parameters such as optimization technique, activation function, network depth, or network width based on intuitive knowledge of solution regularity in each sub-domain.
E.g. for smooth zones, a shallow network may be used, and a deep neural network can be used in an area where a complex nature is assumed.


%%%%%%%%%%%%%%%%%%%%%%
\cite{He2020_PhysicsInformedNeural_BarHBTT}  propose a two-step training approach in which the loss function is minimized first by the Adam algorithm with a predefined stop condition, then by the L-BFGS-B optimizer.
%
According to the aforementioned paper, for cases with a little amount of training data and/or residual points, L-BFGS-B, performs better with a faster rate of convergence and reduced computing cost. 
%L-BFGS-B, a quasi-Newtown method, shows superior performance with a better rate of convergence, lower gradient vanishing, and a lower computational cost for problems with a relatively small amount of training data and/or residual points. 
%%%%%%%%%%%

%\subsection{hyperparameters}
%TODO RIVEDERE... PER RECUPERARE CONCETTO!!!! 
%%%%%%%%%%%%%%%%%%%%%%
%In \cite{Via2021_EstimatingModelInadequacy_NasVNDY} backpropagation is used to optimize hyperparameters.
%Every iteration of the optimization algorithm consists of essentially two phases.
%To begin, training data is fed forward, resulting in the associated outputs and prediction error.
%The gradient with respect to the parameters to be optimized is then obtained by propagating the loss function backward (through the chain rule).
%%%%%%%%%%%%%%%%%%%%%%

%TODO RIVEDERE... PER RECUPERARE CONCETTO!!!! 
%Incorporated physics can constrain network outputs and reduce overfitting problems 
%\citep{Zha2020_PhysicsInformedMulti_LiuZLS}.




%\subsubsection{weights}



%\subsection{training}
Finally, let's look at a practical examples for optimizing the training process; dimensionless and normalized data are used in DeepONet training to improve stability \citep{Lin2021_SeamlessMultiscaleOperator_MaxLMLK}. 
Moreover the governing equations in dimensionless form, including the Stokes equations, electric potential, and ion transport equations, are presented in DeepM\&Mnets  \citep{Cai2021_DeepmmnetInferringElectroconvection_WanCWL}.
%
% DeepM\&Mnets can be expanded to deal with the time-dependent scenario, in which physical quantities are functions of space and time. DeepONets are built with DeepXDE, a simple Python toolkit for scientific machine learning.
% \cite{Cai2021_DeepmmnetInferringElectroconvection_WanCWL} uses the ``unstacked'' architecture for DeepONet, which is made up of a branch and trunk network.


%\subsection{convergence - initializations}
In terms of training procedure initialization, slow and fast convergence behaviors are produced by bad and good initialization, respectively, but \cite{Pan2019_FpinnsFractionalPhysics_LuPLK}  reports a technique for selecting the most suitable one.
By using a limited number of iterations, one can first solve the inverse problem.
This preliminary solution has low accuracy due to discretization, sampling, and optimization errors. The optimized parameters from the low-fidelity problem can then be used as a suitable initialization.










%\cite{Tar2020_PhysicsInformedDeep_MarTMP} % hyperbolic tangent activation function
%Throughout this work, we use feedforward networks with three hidden layers and 50 units per layer. This DNN size is determined empirically to yield accurate results. Smaller DNNs might not be able to accurately approximate the unknown functions. Excessively large DNNs are difficult to train with the (relatively small) amount of data at hand. The optimal choice of DNN size is an active area of research (Claesen & De Moor, 2015) and is outside the scope of this work.

%\cite{He2020_PhysicsInformedNeural_BarHBTT} % fully connected feed-forward networks, 3X32



%\cite{Zhu2021_MachineLearningMetal_LiuZLY} %fully connected neural network with 5 hidden layers and 250 neurons per layer.  PCNN HARD

%%%%%%%%%%
%\cite{Kha2019_VariationalPhysicsInformed_ZhaKZK
% solves a one-dimensional steady-state Burger’s equation with shallow neural networks with one hidden layer, and 5 neurons in each hidden layer, using sine as an activation function.
%VPINN is used to solve a two-Dimensional Poisson’s Equation: with a DNN using 4 hidden layers, using 20 neurons in each layer, and sine as activation function.
%%%%%%%%%%%%%%


%\cite{Kha2021_HpVpinnsVariational_ZhaKZK}
% The hp-VPINN is implemented using 4 layers and 20 nuerons per layer, but in case of an advection equation with a double discontinuity of the exact solution, they implement a deep network with 8 hidden layers.



% \cite{Abr2021_StudyFeedforwardNeural_FloAF}  %9 stacked layers with   tanh  activation 
%NO number of neuron per layer!

%\cite{Wah2021_PinneikEikonalSolution_HagWHA} %10 hidden layers containing 20 neurons in each layer,  locally adaptive inverse tangent function for all hidden layers except the final layer, which has a linear activation function


%\cite{Che2021_DeepLearningMethod_ZhaCZ}
% fully-connected neural network is adopted to build up the Res-PINN and the Swish activation function is selected to determine the nonlinear capacity.
% the different number of training samples are also selected to validate that the Res-PINN has an accurate performance in solving fluid flows problems based on finite data samples.
% the best structure of the Res-PINN includes 10 hidden layers with 20 neurons in each layer for solving  Burger’s Equation
% neural network is 9 hidden layers with 20 neurons each layer, for Navier Stokes.



% \cite{Ami2021_PhysicsInformedNeural_HagANHC}, % feed forward multi-layer neural network
%A schematic of a discontinuous network architecture utilized for the bi-material problem. Two separate networks  and  are combined with Heaviside step function  to construct the solution space for .
% a fully connected neural network to have 7 hidden layers and 30 nodes per layer (5701 parameters in total) 
%AND  7 layers with 20 nodes per layer for each network (5202 parameters in total).


%\cite{Isl2021_ExtractionMaterialProperties_ThaITMH} %three fully connected neural networks% 4X20, 1X1, 2X20



%\cite{Ram2021_SpinnSparsePhysics_RamRR}


%%%%%%%% Caption      
%TABLE 1
%Synthesis of main neural network used in PINN/PCNN implementations.
%Extract of papers representing each Neural Network implementation of PINN/PCNN. There are reported three main families of Neural Networks, i.e. feedforward neural networks (FFNNs), convolutional neural networks (CNN), and recurrent neural networks (RNN). For each type, a paper is reported that either first has used this kind of network or best describes its implementation. Mainly PINNs have been implemented with FFNN with 5-10 layers in the literature. CNN seems to have been used uniquely in a PCNN approach, by integrating the boundary condition into the neural network structure rather than in the loss.
%%%%%%%% 

\subsection{Learning theory of PINN }\label{sec:Theory}

This final subsection provides most recent theoretical studies on PINN to better understand how they work and their potential limits.
These investigations are still in their early stages, and much work remains to be done.


Let us start by looking at how PINN can approximate the true solution of a differential equation, similar to how error analysis is done a computational framework. 
In traditional numerical analysis, we approximate the true solution $\bm{u}(\bm{z})$  of a problem with an approximation scheme that computes $\hat{\bm{u}}_\theta(\bm{z})$. The main theoretical issue is to estimate the global error 
$$\mathcal{E} = \hat{\bm{u}}_\theta(\bm{z}) - \bm{u}(\bm{z}).$$ 
Ideally, we want to find a set of parameters, $\theta$, such that $\mathcal{E}=0$.

%\subsubsection{What we want a new Lax Theorem}
When solving differential equations using a numerical discretization technique, we are interested in the numerical method's stability, consistency, and convergence \citep{Rya2006_TheoreticalIntroductionNumerical_TsyRT, Arn2015_StabilityConsistencyConvergence_Arn, Tho1992_NumericalMethods101convergence_Tho}.
%The discretization of differential equations allows to solve the discrete problem in a finite sequence of algebraic operations that can be efficiently implemented on a computer.
%The error in discretization is the difference between the original solution and the discrete solution, which must be defined and quantified. The stability of a discretization measures how well-posed a discrete problem is.
%
%When a series of model approximations with increasingly refined domains approaches a fixed value, a numerical model is said to be convergent.
%A numerical model is consistent if the sequence converges to the solution of the governing equations.
%Even if a model is consistent, applying it to a problem without testing for the sensitivity to the size of the discrete approximation of the solution domain's time and space is insufficient.
%In other words, convergence testing is a crucial component of modeling study.
%
\\
In such setting, discretization's error can be bound in terms of consistency and stability, a basic result in numerical analysis.
The Lax-Richtmyer Equivalence Theorem is often referred to as a fundamental result of numerical analysis %, despite its limited application to well-posed linear partial differential equations.
%along with Dahlquist's equivalence theorem for ordinary differential equations, 
where 
roughly the convergence is ensured when there is consistency and stability.

%We would expect something similar from PINNs, but this is not the case. This is in contrast to the many different approaches used to solve numerically differentail equations, where they discretise differential equations such as finite differences, finite elements, spectral methods, integral equation approaches, and so on.
%
When studying PINN to mimic this paradigm, the convergence and stability are related to how well the NN learns from physical laws and data.
%
In this conceptual framework, we use a NN, which is a parameterized approximation of problem solutions modeled by physical laws.
In this context, we will (i) introduce the concept of convergence for PINNs, (ii) revisit the main error analysis definitions in a statistical learning framework, and (iii) finally report results for the generalization error. 
%This needs an investigation of how the NN approximates the operator's solution rather than than how we discretise the operator itself. 

%Numerical diffusion and dispersion errors develop as a result of discretizing time and space and approximating PDEs with discretized differential equation solutions.
%
%PINNs do not use discretization, hence avoiding the typical artificial dispersion and diffusion mistakes; instead, they rely on the collocation method.
%This also allows to deal with noisy inputs or solutions.



\subsubsection{Convergence aspects}


The goal of a mathematical foundation for the PINN theory is to investigate the convergence of the computed $\hat{\bm{u}}_\theta (\bm{z})$ to the solution of problem~\eqref{eq:general_form}, $\bm{u}(\bm{z})$.


%\paragraph{Hypothesis class}
Consider a NN configuration with coefficients compounded in the vector  $\bm{\theta}$ and a cardinality equal to the number of coefficients of the NN, $\#\bm{\theta}$.
In such setting, we can consider the hypothesis class
$$
\mathcal{H}_n = \{ \bm{u}_\theta  : \#\bm{\theta} = n \}
$$
composed of all the predictors representing a  NN  whose number of coefficients of  the architecture is $n$.
The capacity of a PINN to be able to learn, is related to how big is $n$, i.e. the expressivity  of $\mathcal{H}_n$.

In such setting, a theoretical issue, is to investigate, how dose the sequence of compute predictors, $\hat{u}_\theta$, converges to the solution of the physical problem ~\eqref{eq:general_form}
$$
\hat{u}_{\theta(n)} \to   u  ,\qquad n\to \infty.
$$

A recent result in this direction was  obtained  by \cite{De2021_ApproximationFunctionsTanh_LanDRLM}
in which they proved that the difference $\hat{u}_{\theta(n)} - u$ converges to zero as the width of a predefined NN, with activation function tanh,  goes to infinity.
%
\\
Practically, the PINN requires choosing a network class $\mathcal{H}_n$  and a loss function given a collection of $N$-training data \citep{Shi2020_ConvergencePhysicsInformed_DarSDK}.
Since the quantity and quality of training data affect  $\mathcal{H}_n$, 
the goal is to minimize the loss, by finding a $u_{\theta^*} \in \mathcal{H}_n$, by training the $N$ using an optimization process.
Even if $\mathcal{H}_n$ includes the exact solution $u$ to PDEs and a global minimizer is established, there is no guarantee that the minimizer and the solution $u$ will coincide.
A first work related on PINN \citep{Shi2020_ConvergencePhysicsInformed_DarSDK}, the authors show that the sequence of minimizers $\hat{\bm{u}}_{\theta^*}$ strongly converges to the solution of a linear second-order elliptic and parabolic type PDE.

\subsubsection{Statistical Learning error analysis}

The entire learning process of a PINN can be considered as a statistical learning problem, and it involves mathematical foundations aspects for the error analysis \citep{Kut2022_MathematicsArtificialIntelligence_Kut}.
%\paragraph{Three errors}  %\paragraph{Statistical Learning Problem}
%
%The overall errors using neural networks may be classified into three categories: optimization error, generalization error, approximation error. \citep{Shi2020_ConvergencePhysicsInformed_DarSDK}
%
For a mathematical treatment of errors in PINN, it is important to take into account: optimization, generalization errors, and approximation error.
It is worth noting that the last one is dependent on the architectural design.  %\cite{Kut2022_MathematicsArtificialIntelligence_Kut}.
\\
Let be $N$ collocation points on $\bar{\Omega} = \Omega \cup \partial \Omega$,
a  NN approximation realized with $\theta$, denoted by $\hat{u}_\theta$, evaluated at points $z_i$, whose exact value is $h_i$.
Following the notation of \cite{Kut2022_MathematicsArtificialIntelligence_Kut}, the \emph{empirical risk} is defined as
\begin{equation}\label{eq:risk1} %(z):=
\widehat{\mathcal{R}}[u_\theta] := \frac{1}{N} \sum_{i=1}^{N} \|
\hat{u}_\theta(z_i) - h_i,
\|^2    
\end{equation}
and represents how well the NN is able to predict the exact value of the problem. The  empirical risk actually corresponds to the loss defined in~\ref{sec:ModelEstimation}, where $\hat{\bm{u}}_\theta = \mathcal{F} (\hat{\bm{u}}_\theta(\bm{z}_i))$  and $h_i=\bm{f}(\bm{z}_i)$ and similarly for the boundaries.

%of training performance
A continuum perspective is the \emph{risk} of using an approximator  $\hat{u}_\theta$, calculated as follows: 
\begin{equation} \label{eq:risk}
\mathcal{R}[\hat{u}_\theta] := \int_{\bar{\Omega}} (\hat{u}_\theta(z) - u(z))^2 \, dz,
\end{equation}
where the distance between the approximation $\hat{u}_\theta$ and the solution $u$ is obtained with the $L^2$-norm.
%
The final approximation computed by the PINN, after a training process of DNN, is $\hat{u}_\theta^*$. 
The main aim in error analysis, is to find suitable estimate for the risk of predicting $u$ i.e. $\mathcal{R}[\hat{u}_\theta^*]$.
\\
The training process, uses gradient-based optimization techniques to minimize a generally non convex cost function. 
In practice, the algorithmic optimization scheme will not always find a global minimum. 
%\citep{Shi2020_ConvergencePhysicsInformed_DarSDK}
So the error analysis takes into account the \emph{optimization error} defined as follows:
\begin{equation*}
\mathcal{E}_O :=
\widehat{\mathcal{R}}[\hat{u}_\theta^*] - \inf_{\theta \in \Theta} \widehat{\mathcal{R}}[u_\theta]
\end{equation*}
%TODO rivedi inglese
\\
Because the objective function is nonconvex, the optimization error is unknown.
Optimization frequently involves a variety of engineering methods and time-consuming fine-tuning, using, gradient-based optimization methods.
Several stochastic gradient descent methods have been proposed,and
many PINN use Adam and L-BFGS.
Empirical evidence suggests that gradient-based optimization techniques perform well in different challenging tasks; however, gradient-based optimization might not find a global minimum for many ML tasks, such as for PINN, and this is still an open problem \citep{Shi2020_ConvergencePhysicsInformed_DarSDK}. 




Moreover a measure of the prediction accuracy on unseen data in machine learning is the 
\emph{generalization error}:
\begin{equation*}
\mathcal{E}_G :=
\sup_{\theta \in \Theta} 
\lvert \mathcal{R}[u_\theta]  -\widehat{\mathcal{R}}[u_\theta] \rvert 
\end{equation*}
\\
%The generalization error, or sometimes, called estimation error (c) is one of two components (together with approximation error) that make up generalization error ???.
The generalization error measures how well the loss integral is approximated in relation to a specific trained neural network. 
%Generalization error is a measure of the prediction accuracy on unseen data in machine learning.
%PDE generalization error is the distance between a global loss minimizer and the PDE solution, which must be defined suitably to reflect its regularity.
One of the first  paper focused with convergence of generalization error is \cite{Shi2020_ConvergencePhysicsInformed_DarSDK}. 


About the ability of the NN to approximate the exact solution, the \emph{approximation error} is defined as
\begin{equation*}
\mathcal{E}_A :=
\inf_{\theta \in \Theta}\mathcal{R}[u_\theta]
\end{equation*}
\\
The approximation error is well studied in general, in fact we know that
one layer neural network with a high width can evenly estimate a function and its partial derivative as shown by \cite{Pin1999_ApproximationTheoryMlp_Pin}.


Finally, as stated in \cite{Kut2022_MathematicsArtificialIntelligence_Kut}, 
the global error between the trained deep neural network $\hat{u}_\theta^*$ and the correct solution function $u$ of problem~\eqref{eq:general_form}, can so be bounded by the previously defined error in the following way
\begin{equation} \label{eq:error}
\mathcal{R}[\hat{u}_\theta^*] 
\leq
\mathcal{E}_O
+
2 \mathcal{E}_G
+
\mathcal{E}_A
\end{equation}

% \begin{equation} \label{eq:error}
% \mathcal{R}[\hat{\bm{u}}_\theta^*] 
% \leq  
% \underbrace{%\left[
% \widehat{\mathcal{R}}[\hat{\bm{u}}_\theta^*] - \inf_{\theta \in \Theta} \widehat{\mathcal{R}}[u_\theta]%\right]
% }_{\text{Optimization error}} 
% +
% \underbrace{2 \sup_{\theta \in \Theta} 
% \lvert \mathcal{R}[u_\theta]  -\widehat{\mathcal{R}}[u_\theta] \rvert 
% }_{\text{Generalization error}}
% + 
% \underbrace{\inf_{\theta \in \Theta}\mathcal{R}[u_\theta].}_{\text{Approximation error}}
% \end{equation}


These considerations lead to the major research threads addressed in recent studies, which are currently being investigated for PINN and DNNs in general  \cite{Kut2022_MathematicsArtificialIntelligence_Kut}.


\subsubsection{Error analysis results for PINN}

%In this final subsection we will show the main results, on approximation error, and generalization error.



%\paragraph{Approximation error}
About the approximating error,
since it depends on the NN architecture, mathematical foundations results are generally discussed in papers deeply focused on this topic \cite{Cal2020_UniversalApproximators_Cal, Elb2021_DeepNeuralNetwork_PerEPGB}.
%TODO literature review .
%https://www.amazon.it/Deep-Learning-Architectures-Mathematical-Approach/dp/3030367207

However, a first argumentation strictly related to PINN is reported in \cite{Shi2020_ErrorEstimatesResidual_ZhaSZK}.
One of the main theoretical results %related to PINN
on $\mathcal{E}_A$, can be found in \cite{De2021_ApproximationFunctionsTanh_LanDRLM}.
They demonstrate that for a neural network with a tanh activation function and only two hidden layers, $\hat{u}_\theta$,  may approximate a function $u$ with a bound in a Sobolev space:
\begin{equation*}
\|\hat{u}_{\theta_N} - u\|_{W^{k,\infty}}  
\leq
C
\frac{\ln (cN)^k}{N^{s-k}}
\end{equation*}
where $N$ is the number of training points, $c,C>0$ are constants independent of $N$ and explicitly known, $u\in W^{ s,\infty} ([0,1]^d)$.
We remark that the NN has width $N^d$, and $\#\theta$ depends on both the number of training points $N$ and the dimension of the problem $d$.
\\

%Altri lavori?

%, for stability analysis, ??


%Regarding the generalization error... a first result in present in KARD...
%\paragraph{NS?} %[DR, Jagtap, and Mishra, 2021a]

Formal findings for generalization errors in PINN are provided specifically for a certain class of PDE.
In \cite{Shi2020_ConvergencePhysicsInformed_DarSDK} they provide convergence estimate for linear second-order elliptic and parabolic type PDEs, while in 
\cite{Shi2020_ErrorEstimatesResidual_ZhaSZK} they extend the results to all linear problems,  including hyperbolic equations.
\cite{Mis2022_EstimatesGeneralizationError_MolMM} gives an abstract framework for PINN on forward problem for PDEs, they estimate the generalization error by means of training error (empirical risk), and number of training points, such abstract framework is also addressed for inverse problems \citep{Mis2021_EstimatesGeneralizationError_MolMM}.
In \cite{De2022_ErrorEstimatesPhysics_JagDRJM}
the authors specifically address Navier-Stokes equations and show that
small training error imply a small generalization error, by proving that
\begin{equation*}
\mathcal{R}[\hat{u}_\theta] =  \|u - \hat{u}_{\theta}\|_{L^2}  
\leq
\left(
C
\widehat{\mathcal{R}}[u_\theta] 
+ 
\mathcal{O}\left(N^{-\frac{1}{d}}\right)
\right)^{\frac{1}{2}}.
\end{equation*}
%
This estimate suffer from the curse of dimensionality (CoD), that is to say, in order to reduce the error by a certain factor, the number of training points needed and the size of the neural network, scales up exponentially.

%\paragraph{curse of dimnsionality - Kolmogorov} 

%[Mishra, Molinaro, Tanios, 2021]  CoD %\cite{Mis2021_PhysicsInformedNeural_MolMM}


%[DR and Mishra, 2021a]  Kolmogorov \cite{De2021_ErrorAnalysisPhysics_MisDRM}
\cite{De2021_ErrorAnalysisPhysics_MisDRM} prove that for a Kolmogorov type PDE (i.e. heat equation or Black-Scholes equation), 
%
the following inequality holds, almost always,
\begin{equation*}
\mathcal{R}[\hat{u}_\theta]
\leq
\left(
C
\widehat{\mathcal{R}}[u_\theta] 
+ 
\mathcal{O}\left(N^{-\frac{1}{2}}\right)
\right)^{\frac{1}{2}},
\end{equation*}
and is not dependant on the dimension of the problem $d$.
%Many PDEs are high-dimensional, like pricing of options modeled by Heston or  Black-Scholes equations,  a  Kolmogorov PDE.
%\\

Finally,
\cite{Mis2021_PhysicsInformedNeural_MolMM}
investigates the radiative transfer equation,  which is noteworthy for its high-dimensionality, with the radiative intensity being a function of 7 variables (instead of 3, as common in many physical problems).
The authors prove also here that the generalization error is bounded by the training error and the number of training points, and the dimensional dependence is on a logarithmic factor:
\begin{equation*}
\mathcal{R}[\hat{u}_\theta]
\leq
\left(
C
\widehat{\mathcal{R}}[u_\theta] ^2
+ 
c\left( \frac{(\ln N)^{2d}}{N}    \right)
\right)^{\frac{1}{2}}.
\end{equation*}
The authors are able to show that PINN does not suffer from the dimensionality curse for this problem, observing that the training error does not depend on the dimension but only on the number of training points.
%PINNs can handle very high-dimensional PDEs without yielding to the curse of dimensionality.
%This means that even the most complex PDEs can be solved with PINNs, even if the problem is extremely high-dimensional. 










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Differential problems dealt with PINNs}\label{sec4}


% TODO Intro Raissi ut+Nx???
The first vanilla PINN \citep{Rai2019_PhysicsInformedNeural_PerRPK} was built to solve complex nonlinear PDE equations of the form $\bm{u}_t + \mathcal{F}_{\bm{x}}\bm{u} = 0$,
 where $\bm{x}$  is a vector of space coordinates, $t$ is a vector time coordinate, and $\mathcal{F}_{\bm{x}}$ is a nonlinear differential operator with respect to spatial coordinates.
First and mainly, the PINN architecture was shown to be capable of handling both forward and inverse problems.
Eventually, in the years ahead, PINNs have been employed to solve a wide variety of ordinary differential equations (ODEs), partial differential equations (PDEs), Fractional PDEs, and integro-differential equations (IDEs), as well as stochastic differential equations (SDEs). 
%
%
This section is dedicated to illustrate where research has progressed in addressing various sorts of equations, by grouping equations according to their form and addressing the primary work in literature that employed PINN to solve such equation. 
All PINN papers dealing with ODEs will be presented first. 
Then, works on steady-state PDEs such as Elliptic type equations, steady-state diffusion, and the Eikonal equation are reported.
The Navier--Stokes equations are followed by more dynamical problems such as heat transport, advection-diffusion-reaction system, hyperbolic equations, and Euler equations or quantum harmonic oscillator.
%a broad spectrum of mathematical physics problems such as conservation laws, diffusion processes, advection-diffusion-reaction systems, and kinetic equations 
Finally, while all of the previous PDEs can be addressed in their respective Bayesian problems, the final section provides insight into how uncertainly is addressed, as in  stochastic equations. 


%TODO intro che FAREMO PHISICAL LAW... prima ode, poi pde declinate con... e infine nv
%TODO le ns vengono trattate a parte perche le pinn nascono per le nv, e sono un aporggio ingolbato che mescolano diversi tipi di quazioni...
%TODO alla fine per uanto consapevoli che le stocastic pde, sono compresive in ongi apsetto visot che volgiamo essere una rferenza per chi implementea alla fine facciamo anche una sezione su baesian.... che contine le avarie PDE delicnate in un apsetto bayesiano.



\subsection{Ordinary Differential Equations}
ODEs can be used to simulate complex nonlinear systems  which are difficult to model using simply physics-based models \citep{Lai2021_StructuralIdentificationPhysics_MylLMNC}. A typical ODE system is written as 
\begin{equation*}
\frac{d \bm{u}(\bm{x}, t)}{dt} = f(\bm{u}(\bm{x},t),t)
\end{equation*}
where initial conditions can be specified as $\mathcal{B}(\bm{u}(t))=  \bm{g}(t)$, resulting in an initial value problem or boundary value problem with  $\mathcal{B}(\bm{u}(\bm{x})) = \bm{g}(\bm{x})$. 
A PINN approach is used by  \cite{Lai2021_StructuralIdentificationPhysics_MylLMNC} for structural identification, using Neural Ordinary Differential Equations (Neural ODEs). Neural ODEs can be considered as a continuous representation of ResNets (Residual Networks), by using a neural network to parameterize a  dynamical system in the form of ODE for an initial value problem (IVP): 
\begin{equation*}
\frac{d \bm{u}(t)}{dt} = f_{\theta}(\bm{u}(t),t)  ; \bm{u}(t_0) = \bm{u}_0
\end{equation*}
where $f$ is the neural network parameterized by the vector $\theta$.

The idea is to use Neural ODEs as learners of the governing dynamics of the systems, and so to structure of Neural ODEs into two parts: a physics-informed term and an unknown discrepancy term. The framework is tested using a spring-mass model as a 4-degree-of-freedom dynamical system with cubic nonlinearity, with also noisy measured data. Furthermore, they use experimental data to learn the governing dynamics of a structure equipped with a negative stiffness device  \citep{Lai2021_StructuralIdentificationPhysics_MylLMNC}.

\cite{Zha2020_PhysicsInformedMulti_LiuZLS} employ deep long short-term memory (LSTM) networks in the PINN approach to solve nonlinear structural system subjected to seismic excitation, like steel moment resistant frame and the  single degree-of-freedom Bouc–Wen model, a nonlinear system with rate-dependent hysteresis \citep{Zha2020_PhysicsInformedMulti_LiuZLS}. 
In general they tried to address the problems of nonlinear equation of motion :
\begin{equation*}
    \ddot{\mathbf{u}} + \mathbf{g} = -\boldsymbol{\Gamma}{a}_g
\end{equation*}
where $\mathbf{g}(t)=\textbf{M}^{-1}\mathbf{h}(t)$ denotes the mass-normalized restoring force, being \textbf{M} the mass matrices; \textbf{h} the total nonlinear restoring force, and $\boldsymbol{\Gamma}$ force distribution vector.

Directed graph models can be used to directly implement ODE as deep neural networks \citep{Via2021_EstimatingModelInadequacy_NasVNDY}, while using  an Euler RNN for numerical integration.

In \cite{Nas2020_TutorialSolvingOrdinary_FriNFV} is presented a tutorial on how to use Python to implement the integration of ODEs using recurrent neural networks.

ODE-net idea is used in \cite{Ton2021_SymplecticNeuralNetworks_XioTXH} for creating Symplectic Taylor neural networks. These NNs consist of two sub-networks, that use symplectic integrators instead of Runge-Kutta, as done originally in ODE-net, which are based on residual blocks calculated with the Euler method. Hamiltonian systems as Lotka–Volterra,  Kepler, and  Hénon–Heiles systems are also tested in the aforementioned paper.

\subsection{Partial Differential Equations}
Partial Differential Equations are the building bricks of a large part of models that are used to mathematically describe physics phenomenologies. Such models have been deeply investigated and often solved with the help of different numerical strategies. Stability and convergence of these strategies have been deeply investigated in literature, providing a solid theoretical framework to approximately solve differential problems. In this Section, the application of the novel methodology of PINNs on different typologies of Partial Differential models is explored. 

\subsubsection{Steady State PDEs}
In \cite{Kha2021_HpVpinnsVariational_ZhaKZK, Kha2019_VariationalPhysicsInformed_ZhaKZK}, a general steady state problem is addressed as:
\begin{align*}
%\label{Eq: Strong form }
 \mathcal{F}_s(\bm{u}(\bm{x}); \bm{q}) &= f(\bm{x})  \quad \bm{x}\in \Omega ,
 \\
 %\label{Eq: Strong form BC}
\mathcal{B}(\bm{u}(\bm{x})) &= 0  \quad \bm{x}\in \partial \Omega
\end{align*}
over the domain $\Omega \subset \mathbb{R}^d$ with dimensions $d$ and bounds $\partial \Omega$.  $\mathcal{F}_s$  typically contains differential and/or integro-differential operators with parameters $\bm{q}$ and $ f(\bm{x})$ indicates some forcing term.

In particular an Elliptic equation can generally be written by setting
\begin{equation*}
\mathcal{F}_s(u(x); \sigma, \mu)  = - div(\mu \nabla u)) + \sigma u
\end{equation*}

\cite{Tar2020_PhysicsInformedDeep_MarTMP}
consider a linear 
\begin{equation*}
\mathcal{F}_s(u(x); \sigma)  =   \nabla \cdot (K(\mathbf{x}) \nabla u(\mathbf{x})) = 0 
\end{equation*}
and non linear 
\begin{equation*}
\mathcal{F}_s(u(x); \sigma)  =   \nabla \cdot [K(u)\nabla u(\mathbf{x})] =0
\end{equation*}
diffusion equation with unknown diffusion coefficient $K$. The equation essentially describes an unsaturated flow in a homogeneous porous medium, where $u$ is the water pressure and $K(u)$ is the porous medium's conductivity.  It is difficult to measure $K(u)$ directly, so \cite{Tar2020_PhysicsInformedDeep_MarTMP} assume that only a finite number of measurements of $u$ are available. 

\cite{Tar2020_PhysicsInformedDeep_MarTMP} demonstrate that the PINN method outperforms the state-of-the-art maximum a posteriori probability method.
Moreover, they show that utilizing only capillary pressure data for unsaturated flow, PINNs can estimate the pressure-conductivity for unsaturated flow. 
One of the first novel approach, PINN based, was the variational physics-informed neural network (VPINN) introduced in \cite{Kha2019_VariationalPhysicsInformed_ZhaKZK},  which has the advantage of decreasing the order of the differential operator through integration-by-parts. The authors tested VPINN with the steady Burgers equation, and on the two dimensional Poisson’s equation.
VPINN \cite{Kha2019_VariationalPhysicsInformed_ZhaKZK} is also used to solve Schrödinger Hamiltonians, i.e. an elliptic reaction-diffusion operator \citep{Gru2021_DeepNeuralNetwork_HajGHL}.

In \cite{Hag2021_NonlocalPhysicsInformed_BekHBMJ} a nonlocal approach with the PINN framework is used  to solve two-dimensional quasi-static mechanics for linear-elastic and elastoplastic deformation. They define a loss function for elastoplasticity, and the input  variables to the feed-forward neural network are the displacements, while the output variables are the components of the strain tensor and stress tensor.  The localized deformation and strong gradients in the solution make the boundary value problem difficult solve.
The Peridynamic Differential Operator (PDDO) is used in a nonlocal approach with the PINN paradigm in \cite{Hag2021_NonlocalPhysicsInformed_BekHBMJ}.
They demonstrated that the PDDO framework can capture stress and strain concentrations using global functions.

In \cite{Dwi2020_PhysicsInformedExtreme_SriDS} the authors address different 1D-2D linear advection and/or diffusion steady-state problems from \cite{Ber2018_UnifiedDeepArtificial_NysBN}, by using their PIELM, a PINN combined with ELM (Extreme Learning Machine). A critical point is that the proposed PIELM only takes into account linear differential operators.

In  \cite{Ram2021_SpinnSparsePhysics_RamRR} they consider linear elliptic PDEs, such as the solution of the Poisson equation in both regular and irregular domains, by addressing non-smoothness in solutions.

The authors in \cite{Ram2021_SpinnSparsePhysics_RamRR} propose a class of partially interpretable sparse neural network architectures (SPINN), and this architecture is achieved by reinterpreting meshless representation of PDE solutions.

Laplace-Beltrami Equation is solved on 3D surfaces, like complex geometries, and high dimensional surfaces, by discussing the relationship between sample size, the structure of the PINN, and accuracy \citep{Fan2020_PhysicsInformedNeural_ZhaFZ}.

The PINN paradigm has also been applied to Eikonal equations, i.e.  are hyperbolic problems written as
\begin{equation*}
\begin{aligned}
    \| \nabla u (\bm{x})\|^2 & = \frac{1}{v^2(\bm{x})}, \qquad \forall \, \bm{x}\, \in \, \Omega,% \\
   %u(\bm{x_s}) & = 0,
\label{eq:eikonal}
\end{aligned}
\end{equation*}
where $v$ is a velocity and $u$ an unknown activation time. %$\bm{x_s}$ is the source point, and 
The Eikonal equation describes wave propagation, like the travel time of seismic wave \citep{Wah2021_PinneikEikonalSolution_HagWHA, Smi2021_EikonetSolvingEikonal_AziSAR} or cardiac activation electrical waves \citep{Sah2020_PhysicsInformedNeural_YanSCYP, Gra2021_LearningAtrialFiber_PezGPC}.

By implementing  EikoNet, for solving a 3D Eikonal equation, \cite{Smi2021_EikonetSolvingEikonal_AziSAR}  find the travel-time field in heterogeneous 3D structures; however, the proposed PINN model is only valid for a single fixed velocity model, hence changing the velocity, even slightly, requires retraining the neural network. EikoNet essentially predicts the time required to go from a source location to a receiver location, and it has a wide range of applications, like earthquake detection. 

PINN is also proved to outperform the first-order fast sweeping solution in accuracy tests \citep{Wah2021_PinneikEikonalSolution_HagWHA}, especially in the anisotropic model.

Another approach involves synthetic and patient data for learning heart tissue fiber orientations from electroanatomical maps, modeled with anisotropic Eikonal equation \citep{Gra2021_LearningAtrialFiber_PezGPC}. In their implementation the authors add to the loss function also a Total Variation regularization for the conductivity vector.

By neglecting anisotropy, cardiac activation mapping is also addressed by \cite{Sah2020_PhysicsInformedNeural_YanSCYP} where PINNs are used with randomized prior functions to quantify data uncertainty and create an adaptive sampling strategy for acquiring and creating activation maps.

Helmholtz equation for weakly inhomogeneous two-dimensional (2D) media under transverse magnetic polarization excitation
 is addressed in \cite{Che2020_PhysicsInformedNeural_LuCLK} as:
\begin{equation*}
\nabla^{2}{E_z\left(x,y\right)}+{\varepsilon_{r}}\left(x,y\right)k_{0}^{2}E_{z}=0,
\end{equation*}
whereas high frequency Helmholtz equation (frequency domain Maxwell’s equation) is solved in \cite{Fan2020_DeepPhysicalInformed_ZhaFZ}.

\subsubsection{Unsteady PDEs}
Unsteady PDEs usually describe the evolution in time of a physics phenomena. Also in this case, PINNs have proven their reliability in solving such type of problems resulting in a flexible methodology. 

\subsubsubsection{Advection-Diffusion-Reaction Problems}
Originally \cite{Rai2019_PhysicsInformedNeural_PerRPK} addressed unsteady state problem as:%
\begin{align*}
%\label{Eq: Strong form }
u_t &= \mathcal{F}_x(u(\bm{x})) \quad \bm{x}\in \Omega ,
 \\
 %\label{Eq: Strong form BC}
\mathcal{B}(u(\bm{x})) &= 0  \quad \bm{x}\in \partial \Omega
\end{align*}
where $\mathcal{F}_x$  typically contains differential operators of the variable $x$.
In particular a general advection-diffusion reaction  equation can be written by setting
\begin{equation*}
\mathcal{F}_x(u(x);  \bm{b},\mu, \sigma )  = - div(\mu \nabla u)) + \bm{b}\nabla u + \sigma u,
\end{equation*}
where, given the parameters $\bm{b},\mu, \sigma$,  $- div(\mu \nabla u))$ is the \emph{diffusion} term, while the advection term is $\bm{b}\nabla u$ which is also known as \emph{transport} term, and finally $\sigma u$ is the \emph{reaction} term.

\paragraph{Diffusion Problems}
For a composite material, \cite{Ami2021_PhysicsInformedNeural_HagANHC} study a system of equations, that models heat transfer with the known heat equation, 
\begin{equation*} 
    \frac{\partial T}{\partial t} = 
    a \frac{\partial^2 T}{\partial x^2}
    + b \frac{d \alpha}{d t}
\end{equation*}
where $a,b$ are parameters, and a second equation for  internal heat generation  expressed as a derivative of time of the degree of cure $\alpha \in (0,1)$ is present.

\cite{Ami2021_PhysicsInformedNeural_HagANHC} propose a PINN composed of two disconnected subnetworks and the use of a sequential training algorithm that automatically adapts the weights in the loss, hence increasing the model's prediction accuracy. 

Based on physics observations, an activation function with a positive output parameter and a non-zero derivative is selected for the temperature describing network's last layer, i.e. a Softplus activation function, that is a smooth approximation to the ReLU activation function.
The Sigmoid function is instead chosen for the last layer of the network that represents the degree of cure. %, defined in $\alpha \in (0,1)$.
Finally, because of its smoothness and non-zero derivative, the hyperbolic-tangent function is employed as the activation function for all hidden layers. 

Since accurate exotherm forecasts are critical in the processing of composite materials inside autoclaves, \cite{Ami2021_PhysicsInformedNeural_HagANHC} show that PINN correctly predict the maximum part temperature, i.e. exotherm, that occurs in the center of the composite material due to internal heat.

A more complex problem was addressed in \cite{Cai2021_PhysicsInformedNeural_WanCWW}, where the authors study a kind of free boundary problem, known as the Stefan problem.

The Stefan problems can be divided into two types: the direct Stefan problem, in its traditional form, entails determining the temperature distribution in a domain during a phase transition. The latter, inverse problem, is distinguished by a set of free boundary conditions known as Stefan conditions \cite{Wan2021_DeepLearningFree_PerWP}.

The authors characterize temperature distributions using a PINN model, that consists of a DNN to represent the unknown interface and another FCNN with two outputs, one for each phase. This leads to three residuals, each of which is generated using three neural networks, namely the two phases $u_{\theta}^{(1)}$, $u_{\theta}^{(2)}$, as well as the interface $s_{\beta}$ that takes the boundary conditions into consideration. 
The two sets of parameters $\theta$ and $\beta$ are minimized through the mean squared errors losses:
\begin{equation*}
\mathcal{L}_\mathcal{F}(\theta) =  \mathcal{L}_r^{(1)}(\theta) +  \mathcal{L}_r^{(2)}(\theta) 
\end{equation*}
enforces the two PDEs of the heat equation, one for each phase state:
\begin{equation*}
\mathcal{L}_r^{(k)}(\theta) =  \frac{1}{N_c} \sum_{i=1}^{N_c} \|
\frac{\partial u_{\theta}^{(k)}  }{\partial t} (x^i,t^i)
- \omega_k \frac{\partial^2 u_{\theta}^{(k)}}{\partial x^2}  (x^i,t^i)
\|^2, \qquad k=1,2.
\end{equation*}
on a set of randomly selected collocation locations $\{ (x^i,t^i)\}_{i=1}^{N_c}$,  and $\omega_1, \omega_2$ are two additional training parameters.
While, as for the boundary and initial conditions:
\begin{equation*}
\mathcal{L}_\mathcal{B}(\theta)  = 
\mathcal{L}_{s_{bc}}^{(1)}(\theta,\beta) +  \mathcal{L}_{s_{bc}}^{(2)}(\theta,\beta)+
\mathcal{L}_{s_{Nc}}(\theta,\beta) +  \mathcal{L}_{s_{0}}(\beta)
\end{equation*}
where $\mathcal{L}_{s_{bc}}^{(k)}$ are the boundary condition of $u^{(k)}$ on the moving boundary $s(t)$, $\mathcal{L}_{s_{Nc}}$ is the free boundary Stefan problem equation, and $\mathcal{L}_{s_{0}}$ is the initial condition on the free boundary function.
Finally, as for the data,
\begin{equation*}
\mathcal{L}_{data}(\theta)  = 
\frac{1}{N_d}\sum\limits_{i=1}^{N_d}
\|u_{\theta}(x^i_{data}, t^i_{data}) - u_i^*\|^2 ,
\end{equation*}
computes the error of the approximation $u(x,t)$ at known data points.

With the previous setup the authors in \cite{Cai2021_PhysicsInformedNeural_WanCWW} find an accurate solution, however, the basic PINN model fails to appropriately identify the unknown thermal diffusive values, for the inverse problem,  due to a local minimum in the training procedure.

So they employ a dynamic weights technique \citep{Wan2021_UnderstandingMitigatingGradient_TenWTP}, which mitigates problems that arise during the training of PINNs due to stiffness in the gradient flow dynamics.
%
The method significantly minimizes the relative prediction error, showing that the weights in the loss function are crucial and that choosing ideal weight coefficients can increase the performance of PINNs \citep{Cai2021_PhysicsInformedNeural_WanCWW}.

In \cite{Wan2021_DeepLearningFree_PerWP}, the authors conclude that the PINNs prove to be versatile in approximating complicated functions like the Stefan problem, despite the absence of adequate theoretical analysis like approximation error or numerical stability. 

\paragraph{Advection Problems}
In \cite{He2021_PhysicsInformedNeural_TarHT}, multiple advection-dispersion equations are addressed, like
\begin{equation*}
u_t + \nabla \cdot (-\kappa \nabla u + \vec{v} u)  = s
\end{equation*}
where $\kappa$ is the dispersion coefficient.

The authors find that the PINN method is accurate and produces results that are superior to those obtained using typical discretization-based methods. 
Moreover both \cite{Dwi2020_PhysicsInformedExtreme_SriDS} and \cite{He2021_PhysicsInformedNeural_TarHT}  solve the same 2D advection-dispersion equation,
\begin{equation*}
u_t + \nabla \cdot (-\kappa \nabla u + \vec{a} u) = 0
\end{equation*}
In this comparison, the PINN technique \citep{He2021_PhysicsInformedNeural_TarHT} performs better that the ELM method \citep{Dwi2020_PhysicsInformedExtreme_SriDS}, given the errors that emerge along borders, probably due to larger wights assigned to boundary and initial conditions in \cite{He2021_PhysicsInformedNeural_TarHT}.

Moreover, in \cite{Dwi2020_PhysicsInformedExtreme_SriDS}, an interesting case of PINN and PIELM failure in solving the linear advection equation is shown, involving PDE with sharp gradient solutions.

\cite{He2020_PhysicsInformedNeural_BarHBTT} solve Darcy and advection–dispersion equations proposing a Multiphysics-informed neural network  (MPINN) for subsurface transport problems, and also explore the influence of the neural network size on the accuracy of parameter and state estimates.

In \cite{Sch2021_ExtremeTheoryFunctional_FurSFL}, a comparison of two methods is shown, Deep-TFC and X-TFC, on how the former performs better in terms of accuracy when the problem becomes sufficiently stiff. The examples are mainly based on  1D time-dependent Burgers’ equation and the Navier–-Stokes (NS) equations.

In the example of the two-dimensional Burgers equation, \cite{Jag2020_ConservativePhysicsInformed_KhaJKK} demonstrate that by having an approximate a priori knowledge of the position of shock, one can appropriately partition the domain to capture the steep descents in solution. 
This is accomplished through the cPINN domain decomposition flexibility. 

While \cite{Arn2021_StateModelingControl_KinAK} addressed the Burgers equations in the context of model predictive control (MPC). 

In \cite{Men2020_PpinnPararealPhysics_LiMLZK} the authors study a two-dimensional diffusion-reaction equation that involves long-time integration and they use a parareal PINN (PPINN) to divide the time interval into equal-length sub-domains.
PPINN is composed of a fast coarse-grained (CG) solver and a finer solver given by PINN.

\subsubsubsection{Flow Problems}\\
Particular cases for unsteady differential problems are the ones connected to the motion of fluids. Navier-Stokes equations are widely present in literature, and connected to a large number of problems and disciplines. This outlines the importance that reliable strategies for solving them has for the scientific community. Many numerical strategies have been developed to solve this kind of problems. However, computational issues connected to specific methods, as well as difficulties that arise in the choice of the discrete spatio-temporal domain may affect the quality of numerical solution. PINNs, providing mesh-free solvers, may allow to overcome some issues of standard numerical methods, offering a novel perspective in this field.  

\paragraph{Navier-Stokes Equations}
Generally Navier-Stokes equations are written as
\begin{equation*}
\mathcal{F}_x(u(x);  \nu , p )  = - div[\nu (\nabla u+ \nabla u^T)] + (u+\nabla) u +\nabla p -\bm{f},
\end{equation*}
where, $u$ is the speed of the fluid, $p$ the pressure and $ \nu$ the viscosity \citep{Qua2013_NumericalModelsDifferential_Qua}.
The dynamic equation is coupled with
\begin{equation*}
div( u ) = 0 
\end{equation*}
for expressing mass conservation.

The Burgers equation, a special case of the Navier-Stokes equations, was covered in the previous section.

Using quick parameter sweeps, \cite{Art2021_ActiveTrainingPhysics_KinAK} demonstrate how PINNs may be utilized to determine the degree of narrowing in a tube. PINNs are trained using finite element data to estimate Navier-Stokes pressure and velocity fields throughout a parametric domain.
The authors present an active learning algorithm (ALA) for training PINNs to predict PDE solutions over vast areas of parameter space by combining ALA, a domain and mesh generator, and a traditional PDE solver with PINN.

PINNs are also applied on the drift-reduced Braginskii model by learning turbulent fields using limited electron pressure data \citep{Mat2021_UncoveringTurbulentPlasma_FraMFH}. The authors simulated synthetic plasma using the global drift-ballooning (GDB) finite-difference algorithm by solving a fluid model, ie.  two-fluid drift-reduced Braginskii equations. They also observe the possibility to infer 3D turbulent fields from only 2D observations and representations of the evolution equations. This can be used for fluctuations that are difficult to monitor or when plasma diagnostics are unavailable.

\cite{Xia2020_FlowsOverPeriodic_WuXWLD} review available turbulent flow databases and propose  benchmark datasets by systematically altering flow conditions. 

\cite{Zhu2021_MachineLearningMetal_LiuZLY} predict the temperature and melt pool fluid dynamics in 3D metal additive manufacturing AM processes.

The thermal-fluid model is characterized by Navier-Stokes equations (momentum and mass conservation), and energy conservation equations.

They approach the Dirichlet BC in a ``hard'' manner, employing a specific piece of the neural network to solely meet the prescribed Dirichlet BC; while Neumann BCs, that account for surface tension, are treated conventionally by adding the term to the loss function. They choose the loss weights based on the ratios of the distinct components of the loss function \citep{Zhu2021_MachineLearningMetal_LiuZLY}.

\cite{Che2021_DeepLearningMethod_ZhaCZ} solve fluid flows dynamics with Res-PINN, PINN paired with a Resnet blocks, that is used to improve the stability of the neural network. They validate the model with Burgers' equation and Navier-Stokes (N-S) equation, in particular, they deal with the cavity flow and flow past cylinder problems.
A curious phenomena observed by \cite{Che2021_DeepLearningMethod_ZhaCZ} is a difference in magnitude between the predicted and actual pressure despite the fact that the distribution of the pressure filed is essentially the same. 

To estimate the solutions of parametric Navier–Stokes equations, \cite{Sun2020_SurrogateModelingFluid_GaoSGPW} created a physics-constrained, data-free, FC-NN  for incompressible flows. The DNN is trained purely by reducing the residuals of the governing N-S conservation equations, without employing CFD simulated data. 
The boundary conditions are also hard-coded into the DNN architecture, since the aforementioned authors claim  that in data-free settings, ``hard'' boundary enforcement is preferable than ``soft'' boundary approach. %\citep{Sun2020_SurrogateModelingFluid_GaoSGPW}.

Three flow examples relevant to cardiovascular applications were used to evaluate the suggested approaches. %\citep{Sun2020_SurrogateModelingFluid_GaoSGPW}.

In particular, the Navier--Stokes equations are given \citep{Sun2020_SurrogateModelingFluid_GaoSGPW} as: 
%\begin{linenomath*}
\begin{equation*}
	%\label{eq:ns}
	\mathcal{F}(\mathbf{u}, p) = 0 := \left \{
	\begin{aligned}
	&\nabla \cdot \mathbf{u} = 0,  &\mathbf{x}, t \in \Omega, \boldsymbol{\gamma} \in \mathbb{R}^d,\\
	&\frac{\partial\mathbf{u}}{\partial t} + (\mathbf{u}\cdot\nabla)\mathbf{u} + \frac{1}{\rho}\nabla p - \nu\nabla^2\mathbf{u} + \mathbf{b}_f = 0, &\mathbf{x}, t \in  \Omega, \boldsymbol{\gamma} \in \mathbb{R}^d 
	\end{aligned} \right .
\end{equation*}
where $\boldsymbol{\gamma}$ is a parameter vector, and with
\begin{equation*}
    %\label{eq:bc}
	\begin{aligned}
	\mathcal{I}(\mathbf{x}, p, \mathbf{u}, \boldsymbol{\gamma}) &= 0, \qquad & &\mathbf{x} \in \Omega, t =0,\boldsymbol{\gamma} \in \mathbb{R}^d,\\
	\mathcal{B}(t, \mathbf{x}, p, \mathbf{u}, \boldsymbol{\gamma}) &= 0, \qquad & &\mathbf{x}, t \in \partial\Omega \times [0, T], \boldsymbol{\gamma} \in \mathbb{R}^d,
	\end{aligned}
\end{equation*}
where $\mathcal{I}$ and $\mathcal{B}$ are generic differential operators that determine the initial and boundary conditions.

The boundary conditions (IC/BC) are addressed individually in \cite{Sun2020_SurrogateModelingFluid_GaoSGPW}. The Neumann BC are formulated into the equation loss, i.e., in a soft manner, whereas the IC and Dirichlet BC are encoded in the DNN, i.e., in a hard manner.

As a last example, NSFnets \citep{Jin2021_NsfnetsNavierStokes_CaiJCLK} has been developed considering two alternative mathematical representations of the Navier--Stokes equations: the velocity-pressure (VP) formulation and the vorticity-velocity (VV) formulation.

\paragraph{Hyperbolic Equations}
Hyperbolic conservation law is used to simplify the  Navier–Stokes equations in hemodynamics \citep{Kis2020_MachineLearningCardiovascular_YanKYH}. 

Hyperbolic partial differential equations are also addressed by \cite{Abr2021_StudyFeedforwardNeural_FloAF}: in particular, they study the inviscid nonlinear Burgers' equation and 1D Buckley-Leverett two-phase problem. They actually try to address problems of the following type:
\begin{equation*}
\frac{\partial u}{\partial t}+
\frac{\partial H(u)}{\partial x} = 0, 
\quad x \in \mathbb{R}, \quad t > 0, \quad \quad \quad u(x,0) = u_0(x),
\label{ivp2b}
\end{equation*}
whose results were compared with those obtained by the Lagrangian-Eulerian and Lax-Friedrichs schemes.
While \cite{Pat2022_ThermodynamicallyConsistentPhysics_ManPMT} proposes a PINN for discovering thermodynamically consistent equations that ensure hyperbolicity for inverse problems in shock hydrodynamics.
\\

Euler equations are hyperbolic conservation laws that might permit discontinuous solutions such as shock and contact waves, and in particular a one dimensional Euler system is written as \citep{Jag2020_ConservativePhysicsInformed_KhaJKK}
\begin{equation*}
   \frac{\partial U}{\partial t} + \nabla\cdot {f}(U) = 0, \; x\in \Omega\subset \mathbb{R}^2,
\end{equation*}
where
\begin{equation*}
U = \begin{bmatrix}
           \rho \\
           \rho u \\
           \rho E
         \end{bmatrix}
\qquad
f = \begin{bmatrix}
           \rho u, \\
           p+ \rho u^2 \\
           p u + \rho u E
         \end{bmatrix}
\end{equation*}
given $\rho$ as the density, $p$ as the pressure,  $u$  the velocity, and $E$ the total energy. These equations regulate a variety of high-speed fluid flows, including transonic, supersonic, and hypersonic flows. \cite{Mao2020_PhysicsInformedNeural_JagMJK} can precisely capture such discontinuous flows solutions for one-dimensional Euler equations, which is a challenging task for existing numerical techniques. 
According to \cite{Mao2020_PhysicsInformedNeural_JagMJK}, appropriate clustering of training data points around a high gradient area can improve solution accuracy in that area and reduces error propagation to the entire domain. This enhancement suggests the use of a separate localized powerful network in the region with high gradient solution, resulting in the development of a collection of individual local PINNs with varied features that comply with the known prior knowledge of the solution in each sub-domain. 
As done in \cite{Jag2020_ConservativePhysicsInformed_KhaJKK}, cPINN splits  the domain into a number of small subdomains in which multiple neural networks with different architectures (known as sub-PINN networks) can be used to solve the same underlying PDE. 

Still in reference to \cite{Mao2020_PhysicsInformedNeural_JagMJK}, the authors solve the one-dimensional Euler equations and a two-dimensional oblique shock wave problem. The authors can capture the solutions with only a few scattered points distributed randomly around the discontinuities.
The above-mentioned paper employ density gradient and pressure $p(x, t)$ data, as well as conservation laws, to infer all states of interest (density, velocity, and pressure fields) for the inverse problem without employing any IC/BCs. They were inspired by the experimental photography technique of Schlieren.

They also illustrate that the position of the training points is essential for the training process.
The results produced by combining the data with the Euler equations in characteristic form outperform the results obtained using conservative forms. 

\subsubsubsection{Quantum Problems}
A 1D nonlinear Schr\"{o}dinger equation is addressed in  \cite{Rai2018_DeepHiddenPhysics_Rai}, and  \cite{Rai2019_PhysicsInformedNeural_PerRPK} as:
\begin{equation}\label{eq:Schrodinger}
    i\frac{\partial \psi}{\partial t} + 
    \frac{1}{2} \frac{\partial^2 \psi}{\partial x^2}
    + \lvert\psi\rvert^2 \psi = 0
%i \psi_t + 0.5 \psi_{xx} + \lvert\psi\rvert^2 \psi = 0,
\end{equation}
where $\psi(x,t)$ is the complex-valued solution.
This problem was chosen to demonstrate the PINN's capacity to handle complex-valued answers and we will develop this example in~section \ref{sec:Schr_example}.
Also a quantum harmonic oscillator (QHO) 
\begin{equation*}
i \frac{\partial \psi (\bm{x},t)}{\partial t} 
+
\frac{1}{2}\varDelta \psi (\bm{x},t) - V(\bm{x},t) = 0
\end{equation*}
is addressed in \cite{Sti2020_LargeScaleNeural_BetSBB}, with $V$ a scalar potential.
They propose a gating network that determines which MLP to use, while each MLP consists of linear layers and $\tanh$ activation functions; so the solution becomes a weighted sum of MLP predictions. 
The quality of the approximated solution of PINNs was comparable to that of state-of-the-art spectral solvers that exploit domain knowledge by solving the equation in the Fourier domain or by employing Hermite polynomials. 

Vector solitons, which are solitary waves with multiple components in the the coupled nonlinear Schrödinger equation (CNLSE) is addressed by
\cite{Mo2022_DataDrivenVector_LinMLZ}, who extended PINNs with a pre-fixed multi-stage training algorithm. These findings can be extendable to similar type of equations, such as equations with a rogue wave \citep{Wan2021_DataDrivenRogue_YanWY} solution or the Sasa-Satsuma equation and Camassa-Holm equation.

\subsection{Other Problems}
Physics Informed Neural Networks have been also applied to a wide variety of problems that go beyond classical differential problems. As examples, in the following, the application of such a strategy as been discussed regarding Fractional PDEs and Uncertainty estimation. 

\subsubsection{Differential Equations of Fractional Order}
Fractional PDEs can be used to model a wide variety of phenomenological properties found in nature with parameters that must be calculated from experiment data, however in the spatiotemporal domain, field or experimental measurements are typically scarce and may be affected by noise \citep{Pan2019_FpinnsFractionalPhysics_LuPLK}. 
Because automatic differentiation is not applicable to fractional operators, the construction of PINNs for fractional models is more complex. One possible solution is to calculate the fractional derivative using the L1 scheme \citep{Meh2019_DiscoveringUniversalVariable_PanMPSK}.

In the first example from \cite{Meh2019_DiscoveringUniversalVariable_PanMPSK} they solve a turbulent flow with one dimensional mean flow.

In \cite{Pan2019_FpinnsFractionalPhysics_LuPLK}, the authors focus on identifying the parameters of fractional PDEs with known overall form but unknown coefficients and unknown operators, by giving rise to fPINN.
They construct the loss function using a hybrid technique that includes both automatic differentiation for integer-order operators and numerical discretization for fractional operators. 
They also analyse the convergence of fractional advection-diffusion equations (fractional ADEs)

The solution proposed in \cite{Pan2019_FpinnsFractionalPhysics_LuPLK}, is then extended in \cite{Kha2021_IdentifiabilityPredictabilityInteger_CaiKCZ}
where they address also time-dependent fractional orders.
The formulation in \cite{Kha2021_IdentifiabilityPredictabilityInteger_CaiKCZ} uses separate neural network to represent each fractional order and use a large neural network to represent states. 

\subsubsection{Uncertainty Estimation}
In data-driven PDE solvers, there are several causes of uncertainty.
The quality of the training data has a significant impact on the solution's accuracy. 

To address forward and inverse nonlinear problems represented by partial differential equations (PDEs) with noisy data, \cite{Yan2021_BPinnsBayesian_MenYMK}  propose a Bayesian physics-informed neural network (B-PINN).
The Bayesian neural network acts as the prior in this Bayesian framework, while an Hamiltonian Monte Carlo (HMC) method or variational inference (VI) method can be used to estimate the posterior.

B-PINNs \citep{Yan2021_BPinnsBayesian_MenYMK} leverage both physical principles and scattered noisy observations to give predictions and quantify the aleatoric uncertainty coming from the noisy data.

\cite{Yan2021_BPinnsBayesian_MenYMK} test their network on some forward problems (1D Poisson equation, Flow in one dimension across porous material with a boundary layer, Nonlinear Poisson equation in one dimension and the 2D Allen-Cahn equation), while for inverse problems the 1D diffusion-reaction system with nonlinear source term and 2D nonlinear diffusion-reaction system are addressed.

\cite{Yan2021_BPinnsBayesian_MenYMK} use also the B-PINNs for a high-dimensional diffusion-reaction system, where the locations of three contaminating sources are inferred from a set of noisy data.

\cite{Yan2020_PhysicsInformedGenerative_ZhaYZK} considers the solution of elliptic stochastic differential equations (SDEs) that required approximations of three stochastic processes: the solution $u(x;\gamma)$, the forcing term $f(x;\gamma)$, and the diffusion coefficient $k(x;\gamma)$. 

In particular, \cite{Yan2020_PhysicsInformedGenerative_ZhaYZK}  investigates the following, time independent, SDE
\begin{equation*}
\begin{aligned}
\mathcal{F}_{\boldsymbol{x}}[u(\boldsymbol{x} ; \gamma) ; k(\boldsymbol{x} ; \gamma)] &=f(\boldsymbol{x} ; \gamma),% \quad \boldsymbol{x} \in \Omega, \quad \gamma \in \Omega
\\
B_{\boldsymbol{x}}[u(\boldsymbol{x} ; \gamma)] &=b(\boldsymbol{x} ; \gamma), %\quad \boldsymbol{x} \in \Gamma
\end{aligned}
\end{equation*}
where $k(x; \gamma )$ and $f (x; \gamma )$ are independent stochastic processes, with $k$ strictly positive.

Furthermore, they investigate what happens when there are a limited number of measurements from scattered sensors for the stochastic processes.
They show how the problem gradually transform from forward to mixed, and finally to inverse problem. This is accomplished by assuming that there are a sufficient number of measurements from some sensors for $f(x;\gamma)$, and then as the number of sensors measurements for $k(x;\gamma)$ is decreased, the number of sensors measurements on $u(x;\gamma)$ is increased, and thus a forward problem is obtained when there are only sensors for $k$ and not for $u$, while an inverse problem has to be solved when there are only sensors for $u$ and not for $k$.

In order to characterizing shape changes (morphodynamics) for cell-drug interactions, \cite{Cav2021_PhysicsInformedDeep_MosCMS} use kernel density estimation (KDE) for translating morphspace embeddings into probability density functions (PDFs). Then they use  a top-down Fokker-Planck model of diffusive development over Waddington-type landscapes, with a PINN learning such landscapes by fitting the PDFs to the Fokker–Planck equation.
The architecture includes a neural network for each condition to learn: the PDF, diffusivity, and landscape.
All parameters are fitted using approximate Bayesian computing with sequential Monte Carlo (aBc-SMC) methods: in this case, aBc selects parameters from a prior distribution and runs simulations; if the simulations match the data within a certain level of similarity, the parameters are saved.
So the posterior distribution is formed by the density over the stored parameters  \citep{Cav2021_PhysicsInformedDeep_MosCMS}.



%todo.. review table
\subsection{Solving a differential problem with PINN}\label{sec:Schr_example}
% Solving a differential sample problem with PINN
%TODO  Schrödinger  = Schrodinger  = Shr\"{o}dinger

Finally, this subsection discusses a realistic example of a 1D nonlinear Shr\"{o}dinger (NLS) problem, as seen in Figure~\ref{fig:Chebfun_NLS}.
%A  practical example of a 1D nonlinear Shr\"{o}dinger (NLS) problem is finally addressed in this subsection and shown in Figure~\ref{fig:Chebfun_NLS}.
The nonlinear problem is the same presented in \cite{Rai2018_DeepHiddenPhysics_Rai, Rai2017_PhysicsInformedDeep1_PerRPK}, used to demonstrate the PINN’s ability to deal with periodic boundary conditions and complex-valued solutions.


\begin{figure}[hbt!]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{U}
         %\caption{$y=x$}
         %\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Bench_Timings50_100_150}
         %\caption{$y=3sinx$}
         %\label{fig:three sin x}
     \end{subfigure}
        \caption{1D nonlinear Shr\"{o}dinger (NLS) solution module $\lvert\bar{\psi}\rvert$. The solution is generated with Chebfun open-source software \citep{Driscoll2014} and used as a reference solution. We also show the solution at three different time frames, $t=\frac{\pi}{8}, \frac{\pi}{4}, \frac{3\pi}{8}$. We expect the solution to be symmetric with respect to $\frac{\pi}{4}$. }
        \label{fig:Chebfun_NLS}
\end{figure}



%Mathematical problem definitions 
Starting from an initial state $\psi(x,0) = 2\ \text{sech}(x)$ and assuming periodic boundary conditions equation~\eqref{eq:Schrodinger} with all boundary conditions results in the initial boundary value problem, given a domain $\Omega = [-5, 5] \times (0,T]$  written as:
\begin{equation}\label{eq:IBVPSchrodinger}
  \begin{cases}
    {\begin{alignedat}{3}
    & i \psi_t + 0.5 \psi_{xx} + \lvert\psi\rvert^2 \psi = 0   & \qquad & (x,t)\in \Omega  \\
    %& i\frac{\partial \psi}{\partial t} + 
    %\frac{1}{2} \frac{\partial^2 \psi}{\partial x^2}
    %+ \lvert\psi\rvert^2 \psi = 0    & \Omega &\times (0,T]  \\
     & \psi(0,x) = 2\ \text{sech}(x) &&  x \in[-5, 5] \\
     & \psi(t,-5) = \psi(t,5)%, \quad \psi_x(t,-5) = \psi_x(t,5)
     &&  t \in (0,T]\\
     & \psi_x(t,-5) = \psi_x(t,5)
     &&  t \in (0,T]
    \end{alignedat}}
  \end{cases}
\end{equation}
where %$\Omega = [-5, 5]$ and
$T=\pi/2$.


%Solving Numerically:  benchmark
To assess the PINN’s accuracy, \cite{Rai2017_PhysicsInformedDeep1_PerRPK} created a high-resolution data set by simulating the Schrödinger equation using conventional spectral methods.
The authors integrated the Schrödinger equation up to a final time $T=\pi/2$ using the MATLAB based Chebfun open-source software
\citep{Driscoll2014}.


%Solving Numerically:  data to train the network
The PINN solutions are trained on a subset of measurements, which includes initial data, boundary data, and collocation points inside the domain. 
The initial time data, $t=0$, are  $\{x_0^i, \psi^i_0\}_{i=1}^{N_0}$, the boundary collocation points are $\{t^i_b\}_{i=1}^{N_b}$, and the collocation points on $\mathcal{F}(t,x)$ are $\{t_c^i,x_c^i\}_{i=1}^{N_c}$.
In \cite{Rai2017_PhysicsInformedDeep1_PerRPK}
a total of $N_0 = 50$ initial data points on $\psi(x,0)$  are randomly sampled from the whole high-resolution data-set to be included in the training set, as well as $N_b = 50$ randomly sampled boundary points to enforce the periodic boundaries.
Finally for the solution domain, it is assumed $N_c=20\ 000$ randomly sampled collocation points.


%Neural network structure
The neural network architecture has two inputs, one for the time $t$ and the other one the location $x$, while the output has also length 2 rather than 1, as it would normally be assumed, because the output of this NN is expected to find the real and imaginary parts of the solution.

%Training the network: annealing... 10 000,  

The network is trained in order to minimize the losses due to the initial and boundary conditions, $\mathcal{L}_\mathcal{B}$,
as well as to satisfy the Schrodinger equation on the collocation points, i.e.  $\mathcal{L}_\mathcal{F}$.
%and the equation not being satisfied on the collocation points, i.e.  $\mathcal{L}_\mathcal{F}$. 
Because we are interested in a model that is a surrogate for the PDE, no extra data, $\mathcal{L}_{data}=0$, is used.
In fact we only train the PINN with the known data point from the initial time step $t=0$.
So the losses of \eqref{eq:loss_pinn} are:
\begin{multline*}
\mathcal{L}_\mathcal{B}
= \frac{1}{N_0}\sum_{i=1}^{N_0} \lvert \psi(0,x_0^i) - \psi^i_0\rvert^2
+ \\
\frac{1}{N_b}\sum_{i=1}^{N_b} \left( \lvert \psi^i(t^i_b,-5) - \psi^i(t^i_b,5)\rvert^2 + \lvert \psi^i_x(t^i_b,-5) - \psi^i_x(t^i_b,5)\rvert^2 \right)
\end{multline*}
and
$$
\mathcal{L}_\mathcal{F} = \frac{1}{N_c}\sum_{i=1}^{N_c}\lvert \mathcal{F}(t_c^i,x_c^i)\rvert^2.
$$
%
The Latin Hypercube Sampling technique 
\citep{Stein1987LargeSP}
is used to create all randomly sampled points among the benchmark data prior to training the NN. 



%Numerical solutions error… L2… MSE…
%L2 norm loss function  ie least squares error (LSE)
In our training we use Adam, with a learning rate of $10^{-3}$, followed by a final  fine-tuning with LBFGS.
We then explore different settings and architectures as in Table~\ref{tab:NLS},
by analysing the  Mean Absolute Error (MAE) and Mean Squared Error (MSE).
%Solving Numerically:  setup
We used  the PyTorch implementation from \cite{Sti2020_LargeScaleNeural_BetSBB} which is accessible on GitHub. While the benchmark solutions are from the GitHub of \cite{Rai2017_PhysicsInformedDeep1_PerRPK}.


\cite{Rai2017_PhysicsInformedDeep1_PerRPK}
first used a DNN with  5 layers each with 100 neurons per layer and a hyperbolic tangent activation function in order to represent the unknown function $\psi$ for both real and imaginary parts.
In our test, we report the original configuration but we also analyze other network architectures and training point amounts. 


\begin{figure}[hbt!]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{diff_hs100}
         %\caption{$y=x$}
         %\label{fig:y equals x}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{PINN_Timings50}
         %\caption{$y=3sinx$}
         %\label{fig:three sin x}
     \end{subfigure}
        \caption{PINN solutions of the 1D nonlinear Shr\"{o}dinger (NLS) problem.
        As illustrated in the first image, the error is represented as the difference among the benchmark solution and PINN result, whereas the second image depicts the PINN's solution at three independent time steps: $t=\pi/8, \pi/4 , 3/8\pi$.
        In this case, we would have expected the solution to overlap at both $\pi/8$ and $ 3/8\pi$, but there isn't a perfect adherence; in fact, the MSE is $5,17\cdot 10^{-04}$ in this case, with some local peak error of $10^{-01}$ in the second half of the domain.
        }
        \label{fig:PINN_best_sol}
\end{figure}
%Fig Caption
%The benchmark solution is shown by the dashed vertical lines.
%The  absolute value of the complex valued function (modulus) is shown.


%Numerical solutions:  observation + Figure
%The main findings of our test are summarized in Figure~\ref{fig:PINN_best_sol}.
A comparison of the predicted and exact solutions at three different temporal snapshots is shown in Figures~\ref{fig:Chebfun_NLS}, and~\ref{fig:PINN_best_sol}.
All the different configurations reported in Table~\ref{tab:NLS} show similar patterns of Figure~\ref{fig:PINN_best_sol}, the only difference is in the order of magnitude of the error. 

In particular in such Figure, we show the best configuration obtained in our test, with an average value of
%$1.97 \cdot 10^{-3}$
%in the $\mathcal{L}_2$-norm.
$5,17\cdot 10^{-04}$ %in the best case 
according to the MSE.
Figure~\ref{fig:PINN_best_sol} first shows the modulus of the predicted spatio-temporal solution $\lvert  \psi(x,t)\rvert$, with respect to the benchmark solution, by plotting the error at each point.
This PINN has some difficulty predicting the central height around $(x,t)=(0,\pi/4)$, as well as in mapping value on in  $t \in (\pi/4, \pi/2)$ that are symmetric to the time interval $t \in (0, \pi/4)$.



%A final comparison is given at various time instants $t=0.59,0.79,0.98$.
Finally, in Table~\ref{tab:NLS}, we present the results of a few runs in which we analyze what happens to the training loss, relative $L_2$, MAE, and MSE when we vary the boundary data, and initial value data.
%
In addition, we can examine how the error grows as the solution progresses through time. 


% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
\begin{table}[hbt!]
\centering
\begin{subtable}[t]{1.01\textwidth}
\begin{tabular}{ccccccc}
\toprule
\textbf{$N_0$} & \textbf{$N_b$} &  \textbf{$N_c$} & \textbf{Training loss} & \textbf{relative $L_2$} & \textbf{MAE} & \textbf{MSE} \\
\midrule
40 & 50 & 20000  & $9\times 10^{-4}$ & $0.025 \pm 0.003$ & $0.065 \pm 0.004$ & $0.019 \pm 0.002$ \\
40 & 100 & 20000  & $1\times 10^{-3}$ & $0.024 \pm 0.002$ & $0.065 \pm 0.003$ & $0.019 \pm 0.002 $ \\
80 & 50 & 20000  & $6 \times 10^{-4}$ & $0.007 \pm 0.001$ & $0.035 \pm 0.004$ & $0.005 \pm 0.001$ \\
%80 & 100 & 10000 TD & $(6 \pm 2)\times 10^{-4}$ & $0.006 \pm 0.002$ & $0.035 \pm 0.006$ & $0.005 \pm 0.002$ \\
80 & 100 & 20000 & $6\times 10^{-4}$ & $0.006 \pm 0.002$ & $0.033 \pm 0.005$ & $0.005 \pm 0.002$ \\
\bottomrule
\end{tabular}
\caption{Case where the NN is fixed with 100 neurons per layer and four hidden layers.
Only the number the number of training points on boundary conditions are doubled. 
}
\label{tab:table1_a}
\end{subtable}
%%%%%%%%%%%%%%%%%%%%%%%%%
\hspace{\fill}
%%%%%%%%%%%%%%%%%%%%%%%%
\begin{subtable}[t]{0.99\textwidth}
\begin{tabular}{cccc}
\toprule
\textbf{time $t$} & \textbf{relative $L_2$} & \textbf{MAE} & \textbf{MSE} \\
\midrule                                      
0   & $(5\pm1)\times 10^{-4} $ & $0.012 \pm 0.002$ & $(4\pm 1)\times 10^{-4}$ \\
0.39   & $(15\pm 5)\times 10^{-4}$ & $0.015 \pm 0.002$ & $(12\pm 4)\times 10^{-3}$  \\
0.79  & $0.009 \pm 0.003$ & $0.038 \pm 0.006$ & $0.007 \pm 0.003$  \\
1.18  & $0.01 \pm 0.004$ & $0.051 \pm 0.009$ & $0.009 \pm 0.003$   \\
1.56  & $0.005 \pm 0.001$ & $0.044 \pm 0.005$ & $0.004 \pm 0.001$  \\
\bottomrule
\end{tabular}
\caption{Error behavior at various time steps for the best occurrence in Table, when \subref{tab:table1_a},  where $N_0 = 80$, $N_b=100$, $N_c=20000$.}
\label{tab:table1_b}
\end{subtable}
\caption{
Two subtables exploring the effect of the amount of data points on convergence for PINN and the inherent problems of vanilla pINN to adhere to the solution for longer time intervals, in solving the 1D nonlinear Shr\"{o}dinger (NLS) equation~\eqref{eq:IBVPSchrodinger}.
In \subref{tab:table1_a}, the NN consists of four layers, each of which contains $100$ neurons, and we can observe how increasing the number of training points over initial or boundary conditions will decrease error rates.
Furthermore, doubling the initial condition points has a much more meaningful influence impact than doubling the points on the spatial daily domain in this problem setup. 
%
In  \subref{tab:table1_b}, diven the best NN, we can observe that a vanilla PINN has difficulties in maintaining a strong performance overall the whole space-time domain, especially for longer times, this issue is a matter of research discussed in \ref{subsec_SciML}. 
%
%IN  \subref{tab:table1_b} we try to indenfity a convercenve patterns similar to what is proved Mishra... TODO.
%By fixing the number of traiing points on the border and intial condictoin, we modigy the NN architecture with 2 or 4 hidden layer, and by doubling the nuber of neruons per layer, form 100, to 200, and by  using a number of training points that goes from 10000 to 40000.
%
%
All errors, MAE, MSE and L2-norm Rel are average over 10 runs.
For all setups, the same optimization parameters are used, including training with $9000$ epochs using Adam with a learning rate of $0.001$ and a final L-BFGS fine-tuning step. 
%
%We alert the reader that these calculations may still be time consuming today for some of the configurations provided; with a.... GTX, a single run with a NN of 2 hidden layers, each with 1,000 neurons, can take approximately 15 minutes, and the RAM is still in training larger networks. 
%
%
}
\label{tab:NLS}
\end{table}







%CONCLUSION
%This is merely a simple example that we will use to illustrate a vanilla PINN for forward problem solving.
%Numerous other applications can be discussed, such as how to constrain the boundary condition in the network, or how to employ alternative neural networks or how to build up loss on known data to address inverse problems. 
%In the next section, we will look at applications where PINN has been employed, as well as an overview of numerous PINN packages that are ready to be used. 

\FloatBarrier






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{PINNs: Data, Applications and Software}

%Intro...
%
The preceding sections cover the neural network component of a PINN framework and which equations have been addressed in the literature.
This section first discusses the physical information aspect, which is how the data and model are managed to be effective in a PINN framework.
Following that, we'll look at some of the real-world applications of PINNs and how various software packages such as DeepXDE, NeuralPDE, NeuroDiffEq, and others were launched in 2019 to assist with PINNs' design. 
% Finally, we'll examine what packages are currently available to completely implement a PINN. We'll see how different software packages such as DeepXDE, NeuralPDE, NeuroDiffEq, and others were launched from 2019 to assist with PINNs' design. 
%


\subsection{Data}
%Data informed modeling
The PINN technique is based not only on the mathematical description of the problems, embedded in the loss or the NN, but also on the information used to train the model, which takes the form of training points, and impacts the quality of the predictions. Working with PINNs requires an awareness of the challenges to be addressed, i.e. knowledge of the key constitutive equations, and also experience in Neural Network building.

Moreover, the learning process of a PINN can be influenced by the relative magnitudes of the different physical parameters. For example to address this issue, \cite{Kis2020_MachineLearningCardiovascular_YanKYH}  used a non-dimensionalization and normalization technique.

However, taking into account the geometry of the problem can be done without effort.
PINNs do not require fixed meshes or grids, providing greater flexibility in solving high-dimensional problems on complex-geometry domains. 
%PINNs are able to deal with complex geometries since the training points can be arbitrarily chosen.
\\
\noindent
The training points for PINNs can be arbitrarily distributed in the spatio-temporal domain \citep{Pan2019_FpinnsFractionalPhysics_LuPLK}.
However, the distribution of training points influences the flexibility of PINNs. 
Increasing the number of training points obviously improves the approximation, although in some applications, the location of training places is crucial. 
Among the various methods for selecting training points, \cite{Pan2019_FpinnsFractionalPhysics_LuPLK} addressed lattice-like sampling, i.e. equispaced, and quasi-random sequences, such as the Sobol sequences or the Latin hypercube sampling.



Another key property of PINN is its ability to describe latent nonlinear state variables that are not observable.
For instance, \cite{Zha2020_PhysicsInformedMulti_LiuZLS} 
observed that when a variable's measurement was missing, the PINN implementation was capable of accurately predicting that variable. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Applications}

%\paragraph{Real word effect} %with or without data
In this section, we will explore the real-world applications of PINNs, focusing on the positive leapfrog applications and innovation that PINNs may bring to our daily lives, as well as the implementation of such applications,
such as the ability to collect data in easily accessible locations and simulate dynamics in other parts of the system, or applications to hemodynamics flows, elastic models, or  geoscience.


\paragraph{Hemodynamics}
%%%%%%%%%  TODO CODICE
Three flow examples in hemodynamics applications are presented in
\cite{Sun2020_SurrogateModelingFluid_GaoSGPW}, for addressing either stenotic flow and aneurysmal flow, with standardized vessel geometries and varying viscosity.
In the paper, they not only validate the results against CFD benchmarks, but they also estimate the solutions of the parametric Navier–Stokes equation without labeled data by designing a DNN that enforces the initial and boundary conditions and training the DNN by only minimizing the loss on conservation equations of mass and momentum. 
%%%%%%%%

In order to extract  velocity and pressure fields directly from images, 
\cite{Rai2020_HiddenFluidMechanics_YazRYK}
proposed the  Hidden fluid mechanics (HFM) framework.
The general structure could be applied for electric or magnetic fields in engineering and biology.
They specifically apply it to hemodynamics in a three-dimensional intracranial aneurysm. 


Patient-specific systemic artery network topologies can make precise predictions about flow patterns, wall shear stresses, and pulse wave propagation. 
Such typologies of systemic artery networks are used to estimate Windkessel model parameters \citep{Kis2020_MachineLearningCardiovascular_YanKYH}.  
PINN methodology is applied on a simplified form of the  Navier–Stokes equations, where a hyperbolic conservation law defines the evolution of blood velocity and cross-sectional area instead of mass and momentum conservation. 
\cite{Kis2020_MachineLearningCardiovascular_YanKYH} devise a new method for creating 3D simulations of blood flow:
they estimate pressure and retrieve flow information using data from medical imaging.
Pre-trained neural network models can quickly be changed to a new patient condition.
This allows Windkessel parameters to be calculated as a simple post-processing step, resulting in a straightforward approach for calibrating more complex models.
%
By processing noisy measurements of blood velocity and wall displacement, \cite{Kis2020_MachineLearningCardiovascular_YanKYH}  present a physically valid prediction of flow and pressure wave propagation derived directly from non-invasive MRI flow.
They train neural networks to provide output that is consistent with clinical data.

\paragraph{Flows problems} %Navier Stokes
%\paragraph{Real data: 3D}
%%%
\cite{Mat2021_UncoveringTurbulentPlasma_FraMFH} observe the possibility to infer 3D turbulent fields from only 2D data.
%%%
To infer unobserved field dynamics from partial observations of synthetic plasma,
they
%the authors of \cite{Mat2021_UncoveringTurbulentPlasma_FraMFH} 
simulate the drift-reduced Braginskii model using physics-informed neural networks (PINNs) trained to solve supervised learning tasks while preserving nonlinear partial differential equations.
This paradigm is applicable to the study of quasineutral plasmas in magnetized collisional situations and provides paths for the construction of plasma diagnostics using artificial intelligence.
This methodology has the potential to improve the direct testing of reduced turbulence models in both experiment and simulation in ways previously unattainable with standard analytic methods. 
%
As a result, this deep learning technique for diagnosing turbulent fields is simply transferrable, allowing for systematic application across magnetic confinement fusion experiments.
The methodology proposed in \cite{Mat2021_UncoveringTurbulentPlasma_FraMFH} can be adapted to many contexts in the interdisciplinary research (both computational and experimental) of magnetized collisional plasmas in propulsion engines and astrophysical environments.


%%%%%%%%%%%%%
\cite{Xia2020_FlowsOverPeriodic_WuXWLD} examine existing turbulent flow databases and proposes benchmark datasets by methodically changing flow conditions. 
%%%%%%%%%%%




%\paragraph{Flows} %Hyperbolic

In the context of  high-speed aerodynamic flows, 
\cite{Mao2020_PhysicsInformedNeural_JagMJK},
investigates Euler equations solutions approximated by PINN. %TODO review
The authors study both forward and inverse, 1D and 2D, problems.
As for inverse problems, they analyze two sorts of problems that cannot be addressed using conventional approaches. In the first problem they determine the density, velocity, and pressure using data from the density gradient; in the second problem, they determine the value of the parameter in a two-dimensional oblique wave equation of state by providing density, velocity, and pressure data. 
% By showing that the selection of the point's position is critical in the learning process.


Projecting solutions in time beyond the temporal area used in training is hard to address with the vanilla version of PINN, and such problem is discussed and tested in \cite{Kim2021_DpmNovelTraining_LeeKLL}.
The authors show that vanilla PINN performs poorly on extrapolation tasks in a variety of Burges' equations benchmark problems, and provide a novel NN with a different training approach.


%Hyperbolic 
PINN methodology is also used also to address the 1D Buckley-Leverett two-phase problem used in petroleum engineering that has a non-convex flow function with one inflection point, making the problem quite complex \citep{Abr2021_StudyFeedforwardNeural_FloAF}. Results are compared with those obtained by the Lagrangian-Eulerian and Lax-Friedrichs schemes.

Finally, Buckley-Leverett's problem is also addressed in \cite{Alm2022_PredictionPorousMedia_AbuAA}, where PINN is compared to an ANN without physical loss:
%Observing that 
when only early-time saturation profiles are provided as data, ANN cannot predict the solution.





%- Power systems?  Helmholtz
\paragraph{Optics and  Electromagnetic applications}
%TODO CODICE
The hybrid PINN from \cite{Fan2021_HighEfficientHybrid_Fan}, based on CNN and local fitting method, addresses applications such as the 3-D Helmholtz equation, quasi-linear PDE operators, and inverse problems.
Additionally, the author tested his hybrid PINN on an icosahedron-based mesh produced by Meshzoo for the purpose of solving surface PDEs. 

%
A PINN was also developed to address power system applications \citep{Misyris2020PhysicsInformedNN} by solving the swing equation, which has been simplified to an ODE. The authors went on to expand on their research results in a subsequent study \citep{Sti2021_PhysicsInformedNeural_MisSMC}.




In \citep{Kov2022_ConditionalPhysicsInformed_ExlKEK} a whole class of microelectromagnetic problems is addressed with a single PINN model, which learns the solutions of classes of eigenvalue problems, related to the nucleation field associated with defects in magnetic materials.

In \cite{Che2020_PhysicsInformedNeural_LuCLK}, the authors solve inverse scattering problems in photonic metamaterials and nano-optics. They use PINNs to retrieve the effective permittivity characteristics of a number of finite-size scattering systems involving multiple interacting nanostructures and multi-component nanoparticles.  %Helmholtz
Approaches for building electromagnetic metamaterials are explored in \cite{Fan2020_DeepPhysicalInformed_ZhaFZ}. 
E.g. cloaking problem is addressed in both \cite{Fan2020_DeepPhysicalInformed_ZhaFZ, Che2020_PhysicsInformedNeural_LuCLK}.
A survey of DL methodologies, including PINNs, applied to nanophotonics may be found in \cite{Wie2021_DeepLearningNano_ArbWAA}.



As a benchmark problem for DeepM\&Mnet, \cite{Cai2021_DeepmmnetInferringElectroconvection_WanCWL} 
choose electroconvection, which depicts electrolyte flow driven by an electric voltage and involves multiphysics, including mass, momentum, cation/anion transport, and electrostatic fields. %The computational cost of resolving the electric double layer is the main constraint.



\paragraph{Molecular dynamics and materials related applications} %Earthquakes? Waves

Long-range molecular dynamics simulation is addressed with multi-fidelity PINN (MPINN) by estimating nanofluid viscosity over a wide range of sample space using a very small number of  molecular dynamics simulations \cite{Isl2021_ExtractionMaterialProperties_ThaITMH}.
The authors were able to estimate system energy per atom, system pressure, and diffusion coefficients, in particular with the viscosity of argon-copper nanofluid.
%
%Nota NON CI STA un equazione nella loss di questo articolo!
PINNs can recreate fragile and non-reproducible particles, as demonstrated by \cite{Sti2021_ReconstructionNanoscaleParticles_SchSS}, whose network reconstructs the shape and orientation of silver nano-clusters from single-shot scattering images. 
An encoder-decoder architecture is used to turn 2D images into 3D object space. Following that, the loss score is computed in the scatter space rather than the object space.
The scatter loss is calculated by computing the mean-squared difference error between the network prediction and the target's dispersion pattern.
Finally, a binary loss is applied to the prediction in order to reinforce the physical concept of the problem's binary nature. 
They also discover novel geometric shapes that accurately mimic the experimental scattering data.

A multiscale application is done in \cite{Lin2021_OperatorLearningPredicting_LiLLL, Lin2021_SeamlessMultiscaleOperator_MaxLMLK},
where the authors describe tiny bubbles at both the continuum and atomic levels, the former level using the Rayleigh–Plesset equation and the latter using the dissipative particle dynamics technique.
In this paper, the DeepONet architecture \citep{Lu2021_LearningNonlinearOperators_JinLJP} is demonstrated to be capable of forecasting bubble growth on the fly across spatial and temporal scales that differ by four orders of magnitude. 




\paragraph{Geoscience and elastostatic problems}

%\paragraph{Elastic or Static models} %Earthquakes? Waves  Climate?  Buildings?
Based on Föppl–von Kármán (FvK) equations, \cite{Li2021_PhysicsGuidedNeural_BazLBZ}
test their model to four loading cases: in-plane tension with non-uniformly distributed stretching forces, central-hole in-plane tension, deflection out-of-plane, and compression buckling.
Moreover stress distribution and displacement field in solid mechanics problems was addressed by
\cite{Hag2021_SciannKerastensorflowWrapper_JuaHJ}. %TODO  rivedi

%\paragraph{Geoscience}
For earthquake hypocenter inversion, \cite{Smi2021_HyposviHypocentreInversion_RosSRAM}
 use Stein variational inference with a PINN trained to solve the Eikonal equation as a forward model, and then test the method against a database of Southern California earthquakes. 




\paragraph{Industrial application}
A broad range of applications, particularly in industrial processes, extends the concept of PINN by adapting to circumstances when the whole underlying physical model of the process is unknown. 


The process of lubricant deterioration is still unclear, and models in this field have significant inaccuracies; this is why
\cite{Yuc2021_HybridPhysicsInformed_ViaYV}
introduced a hybrid PINN for main bearing fatigue damage accumulation calibrated solely through visual inspections.
They use a combination of PINN and ordinal classifier called discrete ordinal classification (DOrC) approach.
The authors looked at a case study in which 120 wind turbines were inspected once a month for six months and found the model to be accurate and error-free. The grease damage accumulation model was also trained using noisy visual grease inspection data.
Under fairly conservative ranking, researchers can utilize the derived model to optimize regreasing intervals for a particular useful life goal.
%
\\
\noindent
As for, corrosion-fatigue crack growth and bearing fatigue are examples studied in \cite{Via2021_EstimatingModelInadequacy_NasVNDY}.

%%% From \cite{Cai2021_PhysicsInformedNeural_WanCWW}
For simulating heat transmission inside a channel,  Modulus from NVIDIA \citep{Hen2021_NvidiaSimnetAi_NarHNN}  trains on a parametrized geometry with many design variables. 
They specifically change the fin dimensions of the heat sink (thickness, length, and height) to create a design space for various heat sinks \citep{Modulus2021}.
When compared to traditional solvers, which are limited to single geometry simulations, the PINN framework can accelerate design optimization by parameterizing the geometry.
Their findings make it possible to perform more efficient design space search tasks for complex systems \citep{Cai2021_PhysicsInformedNeural_WanCWW}.

%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%
% Secondary
%%%%%%%%%%%%%%%%%%%%%%%%

%\paragraph{Replicability: Open data and Open source}



\subsection{Software} %Scripts

Several software packages, including DeepXDE \citep{lu2021deepxde}, NVIDIA Modulus (previously SimNet) \citep{Hen2021_NvidiaSimnetAi_NarHNN}, PyDEns  \citep{Kor2019_PydensPythonFramework_KhuKKT}, and NeuroDiffEq  \citep{chen2020neurodiffeq} were released in 2019 to make training PINNs easier and faster. 
The libraries all used feed-forward NNs and the automatic differentiation mechanism to compute analytical derivatives necessary to determine the loss function. 
The way packages deal with boundary conditions, whether as a hard constraint or soft constraint, makes a significant difference.
%
When boundary conditions are not embedded in the NN but are included in the loss, various losses must be appropriately evaluated.
Multiple loss evaluations and their weighted sum complicate hyper-parameter tuning, justifying the need for such libraries to aid in the design of PINNs. %
\\

More libraries have been built in recent years, and others are being updated on a continuous basis, making this a dynamic field of research and development. 
In this subsection, we will examine each library while also presenting a comprehensive synthesis in
%Let us investigate each library in this subsection while proposing a comprehensive synthesis in
Table~\ref{tab:software}.



\paragraph{DeepXDE} % \cite{lu2021deepxde}

DeepXDE \citep{lu2021deepxde} was one of the initial libraries built by one of the vanilla PINN authors. 
This library emphasizes its problem-solving capability, allowing it to combine diverse boundary conditions and solve problems on domains with complex geometries. 
They also present residual-based adaptive refinement (RAR), a strategy for optimizing the distribution of residual points during the training stage that is comparable to FEM refinement approaches. RAR works by adding more points in locations where the PDE residual is larger and continuing to add points until the mean residual is less than a threshold limit. 
DeepXDE also supports complex geometry domains based on the constructive solid geometry  (CSG) technique.
%DeepXDE is an implementation of PINNs with a residual-based adaptive refinement (RAR) method to improve the distribution of residual points throughout the training phase 
%
This package showcases five applications in its first version in 2019, all solved on scattered points: Poisson Equation across an L-shaped Domain, 2D Burgers Equation, first-order Volterra integrodifferential equation, Inverse Problem for the Lorenz System, and Diffusion-Reaction Systems.


Since the package's initial release, a significant number of other articles have been published that make use of it.  % and in particular, it has assisted in the resolution of:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
DeepXDE is used by the authors for:
for inverse scattering \citep{Che2020_PhysicsInformedNeural_LuCLK}, or
 deriving mechanical characteristics from materials with MFNNs \citep{Lu2020_ExtractionMechanicalProperties_DaoLDK}.
 %
 % for inverse scattering https://www.osapublishing.org/oe/fulltext.cfm?uri=oe-28-8-11618&id=429761
% MFNNs are implemented in DeepXDE https://www.pnas.org/content/117/13/7052
 % We use a feedforward neural network as the architecture of the two sub-networks, which are implemented in the Python library DeepXDE https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/seamless-multiscale-operator-neural-network-for-inferring-bubble-dynamics/D516AB0EF954D0FF56AD864DB2618E94
While other PINNs are implemented using DeepXDE, like hPINN \citep{Kha2021_HpVpinnsVariational_ZhaKZK}.
 % All the hPINN codes in this study are implemented by using the library DeepXDE
Furthermore, more sophisticated tools are built on DeepXDE, such as
DeepONets  \citep{Lu2021_LearningNonlinearOperators_JinLJP}, and its later extension
DeepM\&Mnet \citep{Cai2021_DeepmmnetInferringElectroconvection_WanCWL, Mao2021_DeepmmnetHypersonicsPredicting_LuMLM}.
DeepONets and their derivatives
%, e.g., DeepM\&Mnets \citep{Cai2021_DeepmmnetInferringElectroconvection_WanCWL},
are considered by the authors to have the significant potential in approximating operators and addressing multi-physics and multi-scale problems, %, however it is worth emphasising that the learned operator may be inconsistent with the underlying physical laws.
%% Lu DeepXDE  used by
% All versions of DeepONets are implemented in DeepXDE  https://www.nature.com/articles/s42256-021-00302-5?proof=t%3B
% In this paper, we apply the “unstacked” architecture for DeepONet, which is composed of a branch network and a trunk network. DeepONets are implemented in DeepXDE https://www.sciencedirect.com/science/article/pii/S0021999121001911
% DeepONet https://www.sciencedirect.com/science/article/pii/S0021999121005933
%%
like inferring bubble dynamics
 %with DeepONets, an architecture of the two sub-networks based on DeepXDE
\citep{Lin2021_OperatorLearningPredicting_LiLLL, Lin2021_SeamlessMultiscaleOperator_MaxLMLK}.
%
% Computing the nucleation field associated with defects in magnetic materials, which is a significant difficulty in classical micromagnetics is also done with DeepONet \citep{Kov2022_ConditionalPhysicsInformed_ExlKEK}.


Finally, DeepXDE is utilized for medical ultrasonography applications to simulate a linear wave equation with a single time-dependent sinusoidal source function \citep{Alk2021_ModelingForwardWave_LiuALA},
%DeepXDE https://ieeexplore.ieee.org/abstract/document/9593574
and the open-source library is also employed for the Buckley-Leverett problem 
\citep{Alm2022_PredictionPorousMedia_AbuAA}.
%  We use an open-source library (DeepXDE) to implement our model, as this deep learning-based library is easily customized and adapted to investigate different types of PDEs https://www.sciencedirect.com/science/article/pii/S0920410521008597
%
%
%  PINN  was  implemented  by  using  DeepXDE,  https://arxiv.org/abs/2110.05531
%%%%%%%%%%%%%%%%%%%%%%%%%%%
A list of research publications that made use of DeepXDE is available online
\footnote{https://deepxde.readthedocs.io/en/latest/user/research.html}
.



\paragraph{NeuroDiffEq} %\cite{chen2020neurodiffeq}

NeuroDiffEq \citep{chen2020neurodiffeq} is a PyTorch-based library for solving differential equations with neural networks, which is being used at Harvard IACS. 
NeuroDiffEq solves traditional PDEs (such as the heat equation and the Poisson equation) in 2D by imposing strict constraints, i.e. by fulfilling initial/boundary conditions via NN construction, which makes it a PCNN.
They employ a strategy that is similar to the trial function approach \citep{Lag1998_ArtificialNeuralNetworks_LikLLF}, but with a different form of the trial function. 
%
However, because NeuroDiffEq enforces explicit boundary constraints rather than adding the corresponding losses, they appear to be inadequate for arbitrary bounds that the library does not support \citep{Bal2021_DistributedMultigridNeural_BotBBK}. 

%\cite{Pra2021_AnnsBasedMethod_BakPBMM}




%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chen  Neurodiffeq
% used by
%https://arxiv.org/abs/2007.11133 
%https://arxiv.org/abs/2106.12891
%https://arxiv.org/abs/2111.04207  NeurIPS 2021
%https://arxiv.org/abs/2101.06100
%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Modulus} %\cite{Hen2021_NvidiaSimnetAi_NarHNN}
Modulus \citep{Modulus2021}, previously known as NVIDIA SimNet  \citep{Hen2021_NvidiaSimnetAi_NarHNN}, from Nvidia, is a toolset for academics and engineers that aims to be both an extendable research platform and a problem solver for real-world and industrial challenges.

It is a  PINN toolbox with support to Multiplicative Filter Networks and a gradient aggregation method for larger batch sizes.
Modulus also offers Constructive Solid Geometry (CSG) and Tessellated Geometry (TG) capabilities, allowing it to parameterize a wide range of geometries. 

In terms of more technical aspects of package implementations, Modulus uses an integral formulation of losses rather than a summation as is typically done.
Furthermore, global learning rate annealing is used to fine-tune the weights parameters $\omega$ in the loss equation~\ref{eq:loss_pinn}.
Unlike many other packages, Modulus appears to be capable of dealing with a wide range of PDE in either strong or weak form.
Additionally, the toolbox supports a wide range of NN, such as Fourier Networks and the DGM architecture, which is similar to the LSTM architecture. 


Nvidia showcased the PINN-based code to address multiphysics problems like heat 
transfer in sophisticated parameterized heat sink geometry \citep{Che2021_RecentAdvanceMachine_SeeCS},  3D blood flow in Intracranial Aneurysm or address data assimilation and inverse problems on a flow passing a 2D cylinder \citep{Modulus2021}.
Moreover, Modulus solves the heat transport problem more quickly than previous solvers.




\paragraph{SciANN}  %\cite{Hag2021_SciannKerastensorflowWrapper_JuaHJ}\textbf{}
SciANN \citep{Hag2021_SciannKerastensorflowWrapper_JuaHJ} is an implementation of PINN as a high-level Keras wrapper.
Furthermore, the SciANN repository collects a wide range of examples so that others can replicate the results and use those examples to build other solutions, such as elasticity, structural mechanics, and vibration applications. 
%SciANN is an implementation of PINN as a high-level Keras wrapper.
%With Tensorow and Keras, it inherits graph-based automatic differentiation, and features like as seamless running on CPU and GPU. %TODO extend presentation
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
SciANN is used by the same authors also for  creating a  nonlocal PINN method in \cite{Hag2021_NonlocalPhysicsInformed_BekHBMJ}, or
for a PINN multi-network model applied on solid mechanics \citep{Hag2021_PhysicsInformedDeep_RaiHRM}.
%% Haghighat SciANN used by
%%% https://www.sciencedirect.com/science/article/pii/S0045782521003431#!  In this paper, we rely on SciANN [20], a recent implementation of PINN as a high-level Keras [41] wrapper for physics-informed deep learning and scientific computations. Experimenting with all of the previously mentioned network choices can be easily done, with minimal coding, in SciANN [20], [14].
%% https://www.sciencedirect.com/science/article/pii/S0045782521000773 We set up the problem using the SciANN [40] framework, a high-level Keras [37] wrapper for physics-informed deep learning and scientific computations. Experimenting with all of the previously mentioned network choices can be easily done in SciANN with minimal coding (see [40] for some examples).
%
Although tests for simulating 2D flood, on Shallow Water Equations, are conducted using SciANN 
%Although tests are conducted using SciANN in simulating 2D flood 
\citep{Jam2021_MachineLearningAccelerating_HagJHI}, the authors wrote the feedforward step into a separate function to avoid the overhead associated with using additional libraries. 
% https://onlinelibrary.wiley.com/doi/full/10.1002/hyp.14064 The neural network tests were conducted using the SciANN package, which is a high-level ANN package for scientific computations and physics-informed deep learning in Python (Haghighat & Juanes, 2019). The curve-fitting and local inertial models within the Itzï source code took advantage of multicore CPU for parallel processing (Courty et al., 2017). Although SciANN provided parallel processing capabilities utilizing graphical processing units (GPUs), we trained the ANN model on a desktop without a dedicated GPU. To reduce the overhead involved in using additional libraries, we wrote the feedforward step in a separate Python function, and hence eliminated the need for the SciANN package during the flood prediction tests, which resulted in flood simulations that were around five times faster.
%
A general comparison of many types of mesh-free surrogate models based on machine learning (ML) and deep learning (DL) methods is presented in \cite{Hof2021_MeshFreeSurrogate_GeiHGOK}, where SciANN is used among other toolsets.
% https://www.mdpi.com/2076-3417/11/20/9411   YES PINNs were implemented with the sciann API version 0.5.5.0 in Python 3.8.5. We used the PDEs from [21,22], but instead of the inversion part, we trained our PINNs additionally with plastic strain data, same as for the rest of the surrogate models.
%
% https://doi.org/10.1016/j.cma.2020.113552  we rely on SciANN [20]  https://www.sciencedirect.com/science/article/abs/pii/S0045782520307374?via%3Dihub
%
%% Haghighat SciANN used by
% https://arxiv.org/abs/2104.01588
%
%% Haghighat SciANN used by
%https://arxiv.org/abs/2106.03362 We apply the Keras/Tensorflow wrapper SciAnn [32] for implementing physics informed neural networks. We sample training points quasi-uniformly, applying the Sobol sequence as implemented in the Python library scikit-optimize [34].
%%%%%%%%%%%%%%%%%%%%%%%%%%%
Finally, the PINN framework for solving the Eikonal equation by \cite{Wah2021_PinneikEikonalSolution_HagWHA} was implemented using SciAnn.
%
%The SciANN repository also collects examples so that others can duplicate the results, like applications on elasticity, structural mechanics and vibrations.
%https://github.com/sciann/sciann-applications

\paragraph{PyDENs} %\cite{Kor2019_PydensPythonFramework_KhuKKT}

PyDENs \citep{Kor2019_PydensPythonFramework_KhuKKT} is an open-source neural network PDE solver that allows to define and configure the solution of heat and wave equations.
It impose initial/boundary conditions in the NN, making it a PCNN.
%; however, the DNN performance analysis seems still limited.
%
After the first release in 2019, the development appears to have stopped in 2020. 




\paragraph{NeuralPDE.jl}  %\cite{Zub2021_NeuralpdeAutomatingPhysics_McCZMM}

NeuralPDE.jl is part of SciML, a collection of tools for scientific machine learning and differential equation modeling.
In particular SciML (Scientific Machine Learning) \citep{Rac2021_UniversalDifferentialEquations_MaRMM}  is a program written in Julia that combines physical laws and scientific models with machine learning techniques.



\paragraph{ADCME} % \cite{Xu2020_AdcmeLearningSpatially_DarXD}

%
ADCME \citep{Xu2020_AdcmeLearningSpatially_DarXD} can be used to develop numerical techniques and connect them to neural networks: in particular, ADCME was developed by extending and enhancing the functionality of TensorFlow. 
%
%Moreover ADCME is based on TensorFlow and extends and enhances its functionality. 
In \cite{Xu2020_AdcmeLearningSpatially_DarXD}, ADCME is used to solve different examples, like nonlinear elasticity, Stokes problems, and Burgers' equations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%
Furthermore, ADCME is used by \cite{Xu2021_SolvingInverseProblems_DarXD} for solving inverse problems in stochastic models by using a neural network to approximate the unknown distribution.
%% Xu  ADCME  used by
%https://www.sciencedirect.com/science/article/pii/S0045782521003078
%%%%%%%%%%%%%%%%%%%%%%%%%%%



\paragraph{Nangs} % \cite{Ped2019_SolvingPartialDifferential_MarPMP}
Nangs \citep{Ped2019_SolvingPartialDifferential_MarPMP} is a Python library that uses the PDE's independent variables as NN  (inputs), it then computes the derivatives of the dependent variables (outputs), with the derivatives they calculate the PDEs loss function used during the unsupervised training procedure. 
%
It has been applied and tested on a 1D and 2D advection-diffusion problem.
After a release in 2020, the development appears to have stopped. 
Although, NeuroDiffEq and Nangs libraries were found to outperform PyDEns in solving higher-dimensional PDEs
\citep{Pra2021_AnnsBasedMethod_BakPBMM}.




\paragraph{TensorDiffEq}  %\cite{mcclenny2021tensordiffeq}
TensorDiffEq \citep{mcclenny2021tensordiffeq} is a Scientific Machine Learning PINN based toolkit on Tensorflow for Multi-Worker Distributed Computing.
Its primary goal is to solve PINNs (inference) and inverse problems (discovery) efficiently through scalability.
It implements a Self-Adaptive PINN to increase the weights when the corresponding loss is greater; this task is accomplished by training the network to simultaneously minimize losses and maximize weights.


\paragraph{IDRLnet}  %\cite{Pen2021_IdrlnetPhysicsInformed_ZhaPZZ}

IDRLnet \citep{Pen2021_IdrlnetPhysicsInformed_ZhaPZZ} is a Python's PINN toolbox inspired by Nvidia SimNet  \citep{Hen2021_NvidiaSimnetAi_NarHNN}. It provides a  way to mix geometric objects, data sources, artificial neural networks, loss metrics, and optimizers.
It can also solve noisy inverse problems, variational minimization problem, and integral differential equations. 




\paragraph{Elvet}  %\cite{Ara2021_ElvetNeuralNetwork_CriACS}
Elvet \citep{Ara2021_ElvetNeuralNetwork_CriACS} is a Python library for solving differential equations and variational problems.
It can solve systems of coupled ODEs or PDEs (like the quantum harmonic oscillator) and variational problems involving minimization of a given functional (like the catenary or geodesics solutions-based problems).


\paragraph{Other packages} %methods
% Neural Tangents \cite{neuraltangents2020} 
% GPyTorch \cite{gardner2018gpytorch}
Packages that are specifically created for PINN can not only solve problems using PINN, but they can also be used to provide the groundwork for future research on PINN developments.
%
However, there are other packages that can take advantage of future research developments, such as techniques based on kernel methods \citep{Kar2021_PhysicsInformedMachine_KevKKL}.
Indeed, rather than switching approaches on optimizers, losses, and so on, an alternative approach with respect to PINN framework is to vary the way the function is represented.
Throughout this aspect, rather than using Neural Networks, a kernel method based on Gaussian process could be used. 
The two most noteworthy Gaussian processes toolkit are the Neural Tangents \citep{neuraltangents2020} kernel (NTK), based on JAX, and GPyTorch \citep{gardner2018gpytorch}, written using PyTorch. 
Neural Tangents handles infinite-width neural networks, allowing for the specification of intricate hierarchical neural network topologies. While GPyTorch models Gaussian processes based on Blackbox Matrix-Matrix multiplication using a specific preconditioner to accelerate convergence. 





%%%
%Memo CAPTION TABLE: The table is a list of PINN software
% Major software libraries specifically designed for physics-informed machine learning
%
%%  {tab:software}
%   p{2cm}|p{2cm}|p{2cm}|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|
%
%|l|p{7cm}|p{7cm}|  p{2cm}|p{2cm}|p{2cm}|p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|


%|l|l|l|l|l|p{3cm}|p{2cm}|p{2cm}|l|
%  @{}  l l l l l p{3cm} p{2cm} p{2cm} l @{}


\afterpage{
\begin{landscape}
% Please add the following required packages to your document preamble:
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table}[hbt]
\centering
\resizebox{1.67\textwidth}{!}{% 1.72
\begin{tabular}{@{}  l l l l p{3cm} p{2cm} p{2cm} l @{}}
\toprule
\textbf{Software Name} &
  \textbf{Backend} &
  %\textbf{Dependency} &
  \textbf{Available for} &
  \textbf{Usage} &
  \textbf{License} &
  \textbf{First release} &
  \textbf{Latest version} &
  \textbf{Code Link} \\ \midrule
DeepXDE \cite{lu2021deepxde} &
  TensorFlow &
  %TensorFlow 1.x or 2.x or PyTorch &
  Python &
  Solver &
  Apache-2.0 License &
  v0.1.0  - Jun 13, 2019 &
  v1.4.0 May 20, 2022 &
  \gitlink{lululxvi}{deepxde} \\ \midrule
PyDEns \cite{Kor2019_PydensPythonFramework_KhuKKT} &
  Tensorflow &
  %Tensorflow &
  Python &
  Solver &
  Apache-2.0 License &
  v alpha - Jul 14, 2019 &
  v1.0.2 - Jan 20, 2022&
  \gitlink{analysiscenter}{pydens} \\ \midrule
NVIDIA Modulus \cite{Hen2021_NvidiaSimnetAi_NarHNN} &
  TensorFlow &
  %cuDNN, Tensorflow, Horovod &
  Python based API &
  Solver &
  Proprietary &
 v21.06 on Nov 9, 2021 &  % 
 v22.03 on Apr 25, 2022 &  %
  \myref{https://developer.nvidia.com/modulus-downloads}{modulus} \\ \midrule
NeuroDiffEq \cite{chen2020neurodiffeq} &
  PyTorch &
  %PyTorch &
  Python &
  Solver &
  MIT License &
  v alpha - Mar 24, 2019 &
  v0.5.2 on Dec 12, 2021 &
  \gitlink{NeuroDiffGym}{neurodiffeq} \\ \midrule
SciANN \cite{Hag2021_SciannKerastensorflowWrapper_JuaHJ} &
  TensorFlow &
  %TensorFlow and Keras &
  Python 2.7-3.6 &
  Wrapper &
  MIT License &
  v alpha - Jul 21, 2019 &
  v0.6.5.0 Sep 9 21 &
  \gitlink{sciann}{sciann} \\ \midrule
NeuralPDE \cite{Zub2021_NeuralpdeAutomatingPhysics_McCZMM} &
  Julia &
  %Julia &
  Julia &
  Solver &
  MIT License   & %\ MIT "Expat" License
  v0.0.1 - Jun 22, 2019 &
  v4.9.0 - May 26, 2022 &
  \gitlink{SciML}{NeuralPDE.jl} \\ \midrule
ADCME \cite{Xu2020_AdcmeLearningSpatially_DarXD} &
  Julia TensorFlow &
  %Julia &
  Julia &
  Wrapper &
  MIT License &
  v alpha - Aug 27, 2019 &
  v0.7.3 May 22, 2021 &
  \gitlink{kailaix}{ADCME.jl} \\ \midrule
Nangs \cite{Ped2019_SolvingPartialDifferential_MarPMP} &
  Pytorch &
  %Pytorch &
  Python &
  Solver &
  Apache-2.0 License &
  v0.0.1 - Jan 9, 2020 &
  v2021.12.6 - Dec 5, 2021 &
  \gitlink{juansensio}{nangs} \\ \midrule
TensorDiffEq \cite{mcclenny2021tensordiffeq} &
  Tensorflow 2.x &
  %Tensorflow 2.x &
  Python &
  Solver &  %TODO rev?
  MIT License &
  v0.1.0 - Feb 03, 2021 &
  v0.2.0 - Nov 17 2021 &
  \gitlink{tensordiffeq}{TensorDiffEq} \\ \midrule
IDRLnet \cite{Pen2021_IdrlnetPhysicsInformed_ZhaPZZ} &
  PyTorch, Sympy &
  %PyTorch, Sympy &
  Python &
  Solver &  %TODO rev?
  Apache-2.0 License &
  v0.0.1-alpha Jul 05, 2021 &
  v0.0.1  Jul 21, 2021 &
  \gitlink{idrl-lab}{idrlnet} \\ \midrule
Elvet \cite{Ara2021_ElvetNeuralNetwork_CriACS} &
  Tensorflow &
  %Tensorflow &
  Python &
  Solver & %TODO rev?
  MIT License &
  v0.1.0 Mar 26, 2021 &
  v1.0.0 Mar 29, 2021 &
  \myref{https://gitlab.com/elvet/elvet}{elvet} \\ \midrule
GPyTorch \cite{gardner2018gpytorch} &
  PyTorch &
  % &
  Python &
  Wrapper &
  MIT License &
  v0.1.0 alpha Oct 02, 2018 &
  v1.6.0  Dec 04, 2021 &
  \gitlink{cornellius-gp}{gpytorch} \\ \midrule
Neural Tangents \cite{neuraltangents2020} &
  JAX &
  % &
  Python &
  Wrapper &
  Apache-2.0 License &
  v0.1.1  Nov 11, 2019 &
  v0.5.0 - Feb 23, 2022 &
  \gitlink{google}{neural-tangents} \\ \bottomrule
\end{tabular}%
}
\caption{Major software libraries specifically designed for physics-informed machine learning}
\label{tab:software}
\end{table}
\end{landscape}
}%afterpage






\FloatBarrier





%% NO CIT>
% McClenny  TensorDiffEq
% Peng  IDRLnet
% Koryagin  PyDEns
% Zubov  NeuralPDE
% Araz      Elvet


%%  Gardner  Gpytorch
% used by + TODO  NONE PINN
% 



%%  Novak  Neural Tangents
% used by 80+ TODO  NONE PINN
% https://scholar.google.com/scholar?start=10&hl=it&as_sdt=2005&sciodt=0,5&cites=4030630874639258770&scipsc=





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{What is next}
%\section{What's next in the PINN Theoretical Framework?}\label{secTw}
%https://www.youtube.com/watch?v=qYmkUXH7TCY

\section{PINN Future Challenges and directions}%\label{sec_Disc}

What comes next in the PINN theoretical or applied setup is unknown.
%
What we can assess here are the paper's incomplete results, which papers assess the most controversial aspects of PINN,  where we see unexplored areas, and where we identify intersections with other disciplines. 


Although several articles have been published that have enhanced PINNs capabilities, there are still numerous unresolved issues, such as various applications to real-world situations and equations. 
%
%
These span from more theoretical considerations (such as convergence and stability) to implementation issues (boundary conditions management, neural networks design, general PINN architecture design, and optimization aspects).
%
PINNs and other DL methods using physics prior have the potential to be an effective way of solving high-dimensional PDEs, which are significant in physics, engineering, and finance.
%
PINNs, on the other hand, struggle to accurately approximate the solution of PDEs when compared to other numerical methods designed for a specific PDE, in particular, they can fail to learn complex physical phenomena, like solutions that exhibit multi-scale, chaotic, or turbulent behavior.
%These and other issues must be addressed if we are to move beyond simple copy-paste and create a more intimate connection between scientific and ML methodologies.



\subsection{Overcoming theoretical difficulties in PINN}

A PINN can be thought of as a three-part modular structure, with an approximation (the neural network),  a module to define how we want to correct the approximation (the physics informed network, i.e. the loss), and the module that manages the minimization of the losses.
The NN architecture defines how well the NN can approximate a function, and the error we make in approximating is called approximation error, as seen in Section~\ref{sec:Theory}. 
Then, how we decide to iteratively improve the approximator will be determined by how we define the loss and how many data points we are integrating or calculating the sum, with the quality of such deviation measured as the generalization error. 
Finally, the quality of iterations in minimizing the losses is dependent on the optimization process, which gives the optimization error.

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Convergence} % issues  - reserch topic
%%%%%%%%%%%%%%%%%%%%%%%%
All of these factors raise numerous questions for future PINN research, the most important of which is whether or not PINN converges to the correct solution of a differential equation. 
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Approximation -  Stability} % issues  - reserch topic
%%%%%%%%%%%%%%%%%%%%%%%%
The approximation errors must tend to zero to achieve stability, which is influenced by the network topology.
The outcomes of this research are extremely limited. 
%
For example, the relative error for several neural architectures was calculated by altering the number of hidden layers and the number of neurons per layer in
\cite{Mo2022_DataDrivenVector_LinMLZ}.
%
In another example, \cite{Ble2021_ThreeWaysSolve_ErnBE} shows the number of successes (i.e. when training loss that is less than a threshold) after ten different runs and for different network topologies (number of layers, neurons, and activation function).
%using 10 randomly initialized sets of training data. 
%
Instead, \cite{Mis2021_EstimatesGeneralizationError_MolMM} obtain error estimates and identify possible methods by which PINNs can approximate PDEs. %and establish a solid mathematical foundation for PINNs' approximations.
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{NN} % issues  - reserch topic
%%%%%%%%%%%%%%%%%%%%%%%%
It seems that initial hidden layers may be in charge of encoding low-frequency components (fewer points are required to represent low-frequency signals) and the subsequent hidden layers may be in charge of representing higher-frequency components 
\citep{Mar2021_OldNewCan_Mar}.
This could be an extension of the Frequency-principle, F-principle \citep{Zhi2020_FrequencyPrincipleFourier_XuZX9}, according to which DNNs fit target functions from low to high frequencies during training, implying a low-frequency bias in DNNs and explaining why DNNs do not generalize well on randomized datasets. 
For PINN, large-scale features should arise first while small-scale features will require multiple training epochs. 



%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Generalization - Loss} % 
%%%%%%%%%%%%%%%%%%%%%%%%
The effects of initialization and loss function on DNN learning, specifically on generalization error, should be investigated. 
Many theoretical results treat loos estimation based on quadrature rules, on points selected randomly and identically distributed.
There are some PINN approaches that propose to select collocations points in specific areas of the space-time domain \citep{Nab2021_EfficientTrainingPhysics_GlaNGM}; this should be investigated as well. 
Finally, dynamic loss weighting for PINN appears to be a promising research direction \citep{Nan2022_DevelopingDigitalTwins_HenNHN}. 



%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Optimization} % 
%%%%%%%%%%%%%%%%%%%%%%%%
Optimization tasks are required to improve the performances of NNs, which also holds for PINN. However, given the physical integration, this new PINN methodology will require additional theoretical foundations on optimization and numerical analysis, and dynamical systems theory. 
According to \citep{Wan2021_UnderstandingMitigatingGradient_TenWTP, Wan2022_WhenWhyPinns_YuWYP}, a key issue is to understand the relationship between PDE stiffness and the impact of algorithms as the gradient descent on the  PINNs.


%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Course of dimensionality} 
%%%%%%%%%%%%%%%%%%%%%
Another intriguing research topic is why PINN does not suffer from dimensionality.
According to the papers published on PINN, they can scale up easily independently of the size of the problems \citep{Mis2021_PhysicsInformedNeural_MolMM}.
The computational cost of PINN does not increase exponentially as the problem dimensionality increases; this property is common to neural network architecture, and there is no formal explanation for such patterns \citep{De2021_ErrorAnalysisPhysics_MisDRM}. 
%
\cite{Bau2019_DeepLearningAs_KohBK} recently demonstrated that least-squares estimates based on FNN can avoid the curse of dimensionality in nonparametric regression.
While \cite{Zub2021_NeuralpdeAutomatingPhysics_McCZMMa} demonstrates the capability of the PINN technique with quadrature methods in solving high dimensional problems.



%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Expressiveness versus optimization difficulty} % 
%%%%%%%%%%%%%%%%%%%%%%%%
In PINN the process of learning gives rise to a predictor, $u_\theta$, which minimizes the empirical risk (loss).
In machine learning theory, the prediction error can be divided into two components: bias error and variance error.
%
The bias-variance trade-off appears to contradict the empirical evidence of recent machine-learning systems when neural networks trained to interpolate training data produce near-optimal test results.
It is known that a model should balance underfitting and overfitting, as in the typical U curve, according to the bias-variance trade-off.
However, extremely rich models like neural networks are trained to exactly fit (i.e., interpolate) the data.
\cite{Bel2019_ReconcilingModernMachine_HsuBHMM}, demonstrate the existence of a  double-descent risk curve across a wide range of models and datasets, and they offer a mechanism for its genesis. 
The behavior of PINN in such a framework of Learning Theory for Deep Learning remains to be investigated and could lead to further research questions. 
%
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Functional Spaces} % issues  - reserch topic
%%%%%%%%%%%%%%%%%%%%%%%%
In particular, the function class $\mathcal{H}$ of the hypothesis space in which PINN is optimized might be further examined by specifying such space based on the type of differential equations that it is solving and thus taking into account the physics informed portion of the network. 

%https://arxiv.org/abs/2109.09444
%https://proceedings.neurips.cc/paper/2021/hash/d0921d442ee91b896ad95059d13df618-Abstract.html
%hilbert spaces, 



%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Possible failure modes for physics-informed neural networks} %
%%%%%%%%%%%%%%%%%%%%%%%%
In general, PINN can fail to approximate a solution, not due to the lack of expressivity in the NN architecture but due to soft PDE constraint optimization problems \cite{Kri2021_CharacterizingPossibleFailure_GhoKGZ}.
%https://openreview.net/pdf?id=a2Gr9gNFD-J
%Possible failure modes for physics-informed neural networks
%Soft PDE regularization and optimization difficulties
%Expressivity versus optimization difficulty





\subsection{Improving implementation aspects in PINN} 
When developing a PINN, the PINN designer must be aware that there may be additional configurations that need to be thoroughly investigated in the literature, as well as various tweaks to consider and good practices that can aid in the development of the PINN, by systematically addressing each of the PINN's three modules.
%
From neural network architecture options to activation function type.
In terms of loss development, the approaching calculation of the loss integral, as well as the implementation of the physical problem from adimensionalization  or the solution space constrains.
Finally, the best training procedure should be designed.
%
How to implement these aspects at the most fundamental level, for each physical problem appears to be a matter of ongoing research, giving rise to a diverse range of papers,  which we addressed in this survey, and the missing aspects are summarized in this subsection. 



%\subsubsection{Neural Network module}

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Neural Network architectures} %  \paragraph{Studies on architectures}
%%%%%%%%%%%%%%%%%%%%%%%%
Regarding neural networks architectures, there is still a lack of research for non FFNN types, like CNN and RNN, and what involves their theoretical impact on PINNs \citep{Wan2022_WhenWhyPinns_YuWYP}; moreover, as for the FFNNs
many questions remain, like implementations ones regarding the selection of network size \citep{Hag2021_NonlocalPhysicsInformed_BekHBMJ}.
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Other architectures} % 
%%%%%%%%%%%%%%%%%%%%%%%%
By looking at Table~\ref{tab:NeuralNetworks}, only a small subset of Neural Networks has been instigated as the network architecture for a PINN, and on a quite restricted set of problems.
Many alternative architectures are proposed in the DL literature \citep{Dar2020_SurveyDeepLearning_KumDKAK}, 
and PINN development can benefit from this wide range of combinations in such research fields. 
%To solve PDEs, wang2021eigenvector suggests Fourier feature networks (tancik2020ffn).
%
A possible idea could be to apply Fourier neural operator (FNO) \citep{Wen2022_UFnoanEnhanced_LiWLA}, in order to learn a generalized functional space \citep{Raf2022_DsfaPinnDeep_RafRRC}. 
N-BEATS \citep{Ore2020_NBeatsNeural_CarOCCB}, a deep stack of fully connected layers connected with forward and backward residual links, could be used for  time series based phenomena.
Transformer architecture instead could handle long-range dependencies by modeling global relationships in complex physical problems \citep{Kas2021_PhysicsInformedMachine_MusKMA}.
%
Finally, sinusoidal representation networks (SIRENs) \cite{Sit2020_ImplicitNeuralRepresentations_MarSMB} that are highly suited for expressing complicated natural signals and their derivatives, could be used in PINN. 
Some preliminary findings are already available \citep{Won2022_LearningSinusoidalSpaces_OoiWOGO, Hua2021_SolvingPartialDifferential_LiuHLS}.
%
%
A research line is to study if it is better to increase the breadth or depth of the FFNN to improve PINN outcomes.
Some general studies on DNN make different assertions about whether expanding width has the same benefits as increasing depth.
This may prompt the question of whether there is a minimum depth/width below which a network cannot understand the physics \citep{Tor2020_TheoryTrainingDeep_ViaTRVSA}.
%
The interoperability of PINN will also play an important role in future research \citep{Rud2022_InterpretableMachineLearning_CheRCC}.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{activation function} % issues  - reserch topic
%%%%%%%%%%%%%%%%%%%%%%%%
A greater understanding of PINN's activation function is needed.
\cite{Jag2020_AdaptiveActivationFunctions_KawJKK} show that scalable  activation function may be tuned to maximize network performance, in terms of convergence rate and solution correctness.
%
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{activation functions} % 
%%%%%%%%%%%%%%%%%%%%%%%%
%Or further understanding of mathematical relations among activation functions and the PINNs best implementation.
%
%
%\subsubsection{Loss module}
%
%\paragraph{Improving loss} 
%
%%%%%%%%
% can PINN
%%%%%%%%%%%%%
%https://ml4physicalsciences.github.io/2021/files/NeurIPS_ML4PS_2021_50.pdf
%https://www.sciencedirect.com/science/article/pii/S0045782522001906#b2
%TODO coupled-automatic–numerical differentiation method
Further research can look into alternative or hybrid methods of differentiating the differential equations. 
To speed up PINN training, the loss function in \cite{Chi2022_CanPinnFast_WonCWO}
is defined using numerical differentiation and automatic differentiation.
The proposed can-PINN, i.e. coupled-automatic–numerical differentiation PINN,  shows to be more sample efficient and more accurate than traditional PINN; because PINN with automatic differentiation can only achieve high accuracy with many collocation points. 
%
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Data}\label{secData} %Equations /  Applications
%%%%%%%%%%%%%%%%%%%%%%%%
While the PINNs training points can be distributed spatially and temporally, making them highly versatile, on the other hand, the position of training locations affects the quality of the results.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Geometry} % 
%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
One downside of PINNs is that the boundary conditions must be established during the training stage, which means that if the boundary conditions change, a new network must be created \citep{Wie2021_DeepLearningNano_ArbWAA}. 
%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Automatic differentiation}  % 
%%%%%%%%%%%%%%%%%%%%%%%%
%TODO
%It seems that AD forward mode is not as well optimized or tested as to its reverse mode \citeP{Mar2019_ReviewAutomaticDifferentiation_Mar}.
%
%
As for the loss, it is important to note that a NN will always focus on minimizing the largest loss terms in the weighted equation, therefore all loss terms must be of the same order of magnitude; increasing emphasis on one component of the loss may affect other parts of it.
%
There does not appear to be an objective method for determining the weight variables in the loss equation, nor does there appear to be a mechanism to assure that a certain equation can be solved to a predetermined tolerance before training begins; these are topics that still need to be researched \citep{Nan2021_ProgressTowardsSolving_HenNHN}. 



%\subsubsection{Optimization module}

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Optimization techniques} 
%%%%%%%%%%%%%%%%%%%%%%%%
%Insights on optimization gradient.
%Another issue is the optimization of neural networks; it may be able to find more accurate solutions. %However,
It seems there has been a lack of research on optimization tasks for PINNs. Not many solutions appear to be implemented,  apart from standard solutions such as Adam %SGD
and BFGS algorithms \citep{Won2021_CanTransferNeuroevolution_GupWGO}.
%
The Adam algorithm generates a workflow that can be studied using dynamical systems theory, giving a gradient descent dynamic.
More in detail,  to reduce stiffness in gradient flow dynamics, studying the limiting neural tangent kernel is needed.
%
Given that there has been great work to solve optimization problems or improve these approaches in machine learning, 
there is still room for improvement in PINNs optimization techniques \citep{Sun2020_SurveyOptimizationMethods_CaoSCZZ}.
The L-BFGS-B is the most common BFGS used in PINNs, and it is now the most critical PINN technology \cite{Mar2021_OldNewCan_Mar}.
%
\noindent
\\
Moreover,  the impact of learning rate on PINN training behavior has not been fully investigated. 
%In network training, for example, the learning rate is a key hyper-parameter
%The training behavor and predictions of a theory-trained network have not been thoroughly examined.
%the impacts of the learning rate on the training behavior and predictions of a PINN have not been thoroughly examined. 
%
Finally, gradient normalization is another key research topic \citep{Nan2022_DevelopingDigitalTwins_HenNHN}. It is an approach that dynamically assigns weights to different constraints to remove the dominance of any component of the global loss function. 





%\subsubsection{Final remarks}

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Global error estimation} % 
%%%%%%%%%%%%%%%%%%%%%%%%

It is necessary to investigate an error estimation for PINN. 
%https://arxiv.org/abs/2203.17055
One of the few examples comes from \cite{Hil2022_CertifiedMachineLearning_UngHU}, where
using an ODE, they construct an upper bound on the PINN prediction error.
They suggest adding an additional weighting parameter to the physics-inspired part of the loss function, which allows for balancing the error contribution of the initial condition and the ODE residual. 
Unfortunately, only a toy example is offered in this article, and a detailed analysis of the possibility of providing lower bounds on the error estimator needs still to be addressed, as well as an extension to PDEs.



%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{forward and inverse} % 
%%%%%%%%%%%%%%%%%%%%%%%%









\subsection{PINN in the SciML framework}\label{subsec_SciML}
PINNs, and SciML in general, hold a lot of potential for applying machine learning to critical scientific and technical challenges. 
%
However, many questions remain unsolved, particularly if neural networks are to be used as a replacement for traditional numerical methods such as finite difference or finite volume.
%
%
In
\cite{Kri2021_CharacterizingPossibleFailure_GhoKGZ}
the authors analyze two basic PDE problems of diffusion and convection and show that PINN can fail to learn the ph problem physics when convection or viscosity coefficients are high.
They found that the PINN loss landscape becomes increasingly complex for large coefficients.
This is partly due to an optimization problem, because of the PINN soft constraint.
However, lower errors are obtained when posing the problem as a sequence-to-sequence learning task instead of solving for the entire space-time at once.
%
These kinds of challenges must be solved if PINN has to be used beyond basic copy-paste by creating in-depth relations between the scientific problem and the machine learning approaches. 
\noindent
\\



%\paragraph{Extension to solve other problems rather than differential equations}
%Extension to solve other problems rather than differential equations
Moreover, unexpected uses of PINN can result from applying this framework to different domains.
PINN has been employed as linear solvers for the Poisson equation \cite{Mar2021_OldNewCan_Mar}, by bringing attention to the prospect of using PINN as linear solvers that are as quick and accurate as other high-performance solvers such as PETSc solvers. 
%
%\paragraph{Replacement of classical numerical methods}
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{PINN VS classic / or both} % PINN alternatives...
%%%%%%%%%%%%%%%%%%%%%%%% 
PINNs appear to have some intriguing advantages over more traditional numerical techniques, such as the Finite Element Method (FEM), as explained in \cite{lu2021deepxde}.
Given that PINNs approximate functions and their derivatives nonlinearly, whereas FEM approximates functions linearly, PINNs appear to be well suited for broad use in a wide variety of engineering applications.
However, one of the key disadvantages is the requirement to train the NN, which may take significantly longer time than more extensively used numerical methods.
%, that can also provide more detailed theoretical knowledge and error estimates.


%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{PINN vs others}  %PINN is good
%%%%%%%%%%%%%%%%%%%%%%%%
%PINNs have been shown to perform better than conventional numerical techniques in many areas. 
%
%Numerical techniques for approximating PDEs include finite difference, finite element (FEM), finite volume, and spectral methods.
%Unlike many classic computing approaches, PINNs’ computational cost does not grow with the number of grid points.
%Furthermore, the trained PINN network may be used to predict values on simulated grids of various resolutions without retraining.
%
%Since FEM is the most frequently used numerical method, it is compared to PINN in this paragraph, based on the results in \cite{lu2021deepxde}. %
%%\paragraph{FEM and PINN}
%In FEM, the solution is approximated by a piecewise polynomial with unknown point values, while in PINNs, the surrogate model is a neural network with weights and biases.
%
%Moreover, FEM usually requires a mesh generation, whereas PINN, being mesh-free, can employ a grid or random points.
%Finally, PINN approximates the function and its derivatives nonlinearly, whereas FEM does it linearly.% \citep{lu2021deepxde}.
%
%PDEs are converted to algebraic systems using the stiffness and mass matrices, while PINN embeds them within the loss function.
%In the last stage, a linear solver solves the algebraic system in FEM, but a gradient-based optimizer learns the network in PINN.



%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{online / offline} % 
%%%%%%%%%%%%%%%%%%%%%%%%
On the other hand, PINNs appear to be useful in a paradigm distinct from that of standard numerical approaches.
PINNs can be deployed in an online-offline fashion, with a single PINN being utilized for rapid evaluations of dynamics in real-time, improving predictions.% scenarios.
%
Moving from 2D to 3D poses new obstacles for PINN. As training complexity grows in general, there is a requirement for better representation capacity of neural networks, a demand for a larger batch size that can be limited by GPU memory, and an increased training time to convergence \citep{Nan2021_ProgressTowardsSolving_HenNHN}. 
%
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Scientific computing} % 
%%%%%%%%%%%%%%%%%%%%%%%%
Another task is to incorporate PINN into more traditional scientific programs and libraries written in Fortran and C/C++, as well as to integrate PINN solvers into legacy HPC applications \citep{Mar2021_OldNewCan_Mar}.
PINN could also be implemented on Modern HPC Clusters, by using  Horovod \citep{Ser2018_HorovodFastEasy_DelSDB}.
%https://arxiv.org/abs/1802.05799
%https://ieeexplore.ieee.org/abstract/document/9409305 no pINN
%https://ieeexplore.ieee.org/abstract/document/9409305
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Writing the problem} % 
%%%%%%%%%%%%%%%%%%%%%%%%
%Being able to use SymPy.   Normalize the problem before.
Additionally, when developing the mathematical model that a PINN will solve, the user should be aware of pre-normalizing the problem. At the same time, packages can assist users in dealing with such problems by writing the PDEs in a symbolic form, for example, using SymPy.
\noindent
\\


%\paragraph{Complex problem setups}
PINNs have trouble propagating information from the initial condition or boundary condition to unseen areas of the interior or to future times as an iterative solver \citep{Jin2021_NsfnetsNavierStokes_CaiJCLK, Dwi2020_PhysicsInformedExtreme_SriDS}.
This aspect has recently been addressed by \cite{Wan2022_RespectingCausalityIs_SanWSP}
that provided a re-formulation of PINNs loss functions that may explicitly account for physical causation during model training. They assess that PINN training algorithms should be designed to respect how information propagates in accordance with the underlying rules that control the evolution of a given system.
With the new implementation they observe considerable accuracy gains, as well as the possibility to assess the convergence of a PINNs model, and so PINN, can run for the chaotic Lorenz system, the Kuramoto–Sivashinsky equation in the chaotic domain, and the Navier–Stokes equations in the turbulent regime. However there is still research to be done for hybrid/inverse problems, where observational data should be considered as point sources of information, and PDE residuals should be minimized at those points before propagating information outwards. 
%
Another approach is to use ensemble agreement as to the criterion for incorporating new points in the loss calculated from PDEs \citep{Hai2022_ImprovedTrainingPhysics_IliHI}.
The idea is that in the neighborhood of observed/initial data, all ensemble members converge to the same solution, whereas they may be driven towards different incorrect solutions further away from the observations, especially or large time intervals.
\noindent
\\



%\paragraph{Applications} 
PINN can also have a significant impact on our daily lives,
%
as for the example, from \cite{Yuc2021_HybridPhysicsInformed_ViaYV}, where PINNs are used to anticipate grease maintenance; in the industry 4.0 paradigm, they can assist engineers in simulating materials and constructions or analyzing in real-time buildings structures by embedding elastostatic trained PINNs \citep{Hag2021_PhysicsInformedDeep_RaiHRM, Min2020_SurrogateModelComputational_TroMNTKNRZ}.
%
PINNs also fail to solve PDEs with high-frequency or multi-scale structure 
\citep{Wan2022_WhenWhyPinns_YuWYP, Wan2021_UnderstandingMitigatingGradient_TenWTP, Fuk2020_LimitationsPhysicsInformed_TchFT}.
%
%
The region of attraction of a specific equilibria of a given autonomous dynamical system could also be investigated with PINN \citep{Sch2021_LearningEstimateRegions_HaySH}.

However, to employ PINN in a safety-critical scenario it will still be important to analyze stability and focus on the method's more theoretical component. 
%
Many application areas still require significant work, such as the cultural heritage sector, the healthcare sector, fluid dynamics, particle physics, and the modeling of general relativity with PINN. 
\noindent
\\
It will be important to develop a PINN methodology for stiff problems, as well as use PINN in digital twin applications such as real-time control, cybersecurity, and machine health monitoring \citep{Nan2021_ProgressTowardsSolving_HenNHN}. 
%
%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Applications to address}\label{sec13} %Equations /  Applications
%%%%%%%%%%%%%%%%%%%%%%%%
Finally, there is currently a lack of PINNs applications in multi-scale applications, particularly in climate modeling \citep{Irr2021_TowardsNeuralEarth_BoeIBS},
although the PINN methodology has proven capable of addressing its capabilities in numerous applications such as bubble dynamics on multiple scales
\citep{Lin2021_OperatorLearningPredicting_LiLLL, Lin2021_SeamlessMultiscaleOperator_MaxLMLK}.



\subsection{PINN in the AI framework}
PINN could be viewed as a building block in a larger AI framework, or other AI technologies could help to improve the PINN framework. 
\\
\noindent


%\paragraph{Deep Reinforcement Learning} 
For more practical applications, PINNs can be used as a tool for engaging deep reinforcement learning (DRL) that combines reinforcement Learning (RL) and deep learning.
RL enables agents to conduct experiments to comprehend their environment better, allowing them to acquire high-level causal links and reasoning about causes and effects
\citep{Aru2017_DeepReinforcementLearning_DeiADBB}.
%
The main principle of reinforcement learning is to have an agent learn from its surroundings through exploration and by defining a reward \citep{Shr2019_ReviewDeepLearning_MahSM}.
%
%Using PINNs as a building block with DRL, the training is carried out while discovering real-world scenarios, and actuators and sensors interacting with the surroundings can be used to train PINN networks via transfer learning paradigms further.
%
In the DRL framework, the PINNs can be used as agents.
In this scenario, information from the environment could be directly embedded in the agent using knowledge from actuators, sensors, and the prior-physical law,  like in a  transfer learning paradigm.
\\
\noindent


%\paragraph{Symbolic AI} 
PINNs can also be viewed as an example of merging deep learning with symbolic artificial intelligence.
%
The symbolic paradigm is based on the fact that intelligence arises by manipulating abstract models of representations and interactions. 
This approach has the advantage to discover features of a problem using logical inference, but it lacks the easiness of adhering to real-world data, as in DL.
A fulfilling combination of symbolic intelligence and DL would provide the best of both worlds. The model representations could be built up directly from a small amount of data with some priors \citep{Gar2019_ReconcilingDeepLearning_ShaGS}.
%
In the PINN framework, the physical injection of physical laws could be treated as symbolic intelligence by adding reasoning procedures. 
\\
\noindent


%\paragraph{Casual ML} 
Causal Models are intermediate descriptions that abstract physical models while answering statistical model questions 
\citep{Sch2021_CausalRepresentationLearning_LocSLB}.
Differential equations model allows to forecast a physical system's future behavior, assess the impact of interventions, and predict statistical dependencies between variables.
On the other hand, a statistical model often doesn't refer to dynamic processes, but rather how some variables allow the prediction of others when the experimental conditions remain constant.
In this context, a causal representation learning, merging Machine learning and graphical causality, is a novel research topic, and given the need to model physical phenomena given known data, it may be interesting to investigate whether PINN, can help to determine causality when used to solve hybrid (mixed forward and inverse) problems. 





\section{Conclusion}\label{sec_Disc}
%Summary, challenges and conclusion
%Discussion


%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Summary} 
%%%%%%%%%%%%%%%%%%%%%%%%
This review can be considered an in-depth study of an innovation process over the last four years rather than a simple research survey in the field of PINNs.
%This review can be considered as a record of an innovation process during the last four years, rather than as a research survey on PINNs. 
%
%
Raissi's first research \citep{Rai2017_PhysicsInformedDeep1_PerRPK,Rai2017_PhysicsInformedDeep2_PerRPK}, which developed the PINN framework, focused on implementing a PINN to solve known physical models.
These innovative papers helped PINN methodology gain traction and justify its original concept even more.
%
Most of the analyzed studies have attempted to personalize the PINNs by modifying the activation functions, gradient optimization procedures, neural networks, or loss function structures.
A border extension of PINNs original idea brings to use in the physical loss function bare minimum information of the model, without using a typical PDE equation, and on the other side to embed directly in the NN structure the validity of initial or boundary conditions.
Only a few have looked into alternatives to automatic differentiation \citep{Fan2021_HighEfficientHybrid_Fan} or at convergence problems \citep{Wan2021_UnderstandingMitigatingGradient_TenWTP, Wan2022_WhenWhyPinns_YuWYP}.
%
Finally, a core subset of publications has attempted to take this process to a new meta-level by proposing all-inclusive frameworks for many sub-types of physical problems or multi-physics systems \citep{Cai2021_DeepmmnetInferringElectroconvection_WanCWL}.
%
The brilliance of the first PINN articles \citep{Rai2017_PhysicsInformedDeep1_PerRPK,Rai2017_PhysicsInformedDeep2_PerRPK} lies in resurrecting the concept of optimizing a problem with a physical constraint by approximating the unknown function with a neural network \citep{Dis1994_NeuralNetworkBased_PhaDP} and then extending this concept to a hybrid data-equation driven approach within modern research. 
%
Countless studies in previous years have approximated the unknown function using ways different than neural networks, such as the kernel approaches \citep{Owh2019_KernelFlowsLearning_YooOY}, or other approaches that have used PDE functions as constraints in an optimization problem \citep{hinze2008optimization}.

However, PINNs are intrinsically driven by physical information, either from the data point values or the physical equation.
The former can be provided at any point in the domain but is usually only as initial or boundary data.
More importantly, the latter are the collocation points where the NN is forced to obey the physical model equation. 


%\paragraph{Conclusion}\label{secConcl}

%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{Synthesis of research question} %hypothesis or research question
%%%%%%%%%%%%%%%%%%%%%%%% 

We examined the literature on PINNs in this paper, beginning with the first papers from \cite{Rai2017_PhysicsInformedDeep1_PerRPK,Rai2017_PhysicsInformedDeep2_PerRPK} and continuing with the research on including physical priors on Neural Networks.
%
This survey looks at PINNs, as a collocation-based method for solving differential questions with Neural networks.
Apart from the vanilla PINN solution we look at most of its variants, like variational PINN (VPINN) as well as in its soft form, with loss including the initial and boundary conditions, and its hard form version with boundary conditions encoded in the Neural network structure.


This survey explains the PINN pipeline, analyzing each building block; first, the neural networks, then the loss construction based on the physical model and feedback mechanism.
%
Then, an overall analysis of examples of equations in which the PINN methodology has been used, and finally, an insight into PINNs on where they can be concretely applied and the packages available.


%%%%%%%%%%%%%%%%%%%%%%%%
%\paragraph{What we have observed: limitations and relevance of PINNs} 
%%%%%%%%%%%%%%%%%%%%%%%%% 
%
Finally, we can conclude that
numerous improvements are still possible; most notably, in unsolved theoretical issues. There is still potential for development in training PINNs optimally and extending PINNs to solve multiple equations.
%












%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%

\bibliography{revBib}%, revBib_hist, revBib_ntx, revBib_cdc, revBib_dl common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl

%% Default %%
%%\input sn-sample-bib.tex%

\end{document}
