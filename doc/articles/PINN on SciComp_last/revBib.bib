%% JabRef style:  [auth3][year]_[shorttitle]_[auth3_2][authorsAlpha]
%% PINN review bibliography
%%% [auth3][year]_[shorttitle:titlecase]_[auth3_2][authorsAlpha]






@misc{Modulus2021,
    author = "{NVIDIA Corporation}",
    title = {{Modulus User Guide}},
    howpublished = {\url{https://developer.nvidia.com/modulus-user-guide-v2106}},
    note = {Release v21.06 -- November 9, 2021} ,
    year=2021,
}


@Book{Qua2013_NumericalModelsDifferential_Qua,
  author    = {Quarteroni, Alfio},
  publisher = {Springer Publishing Company, Incorporated},
  title     = {Numerical Models for Differential Problems},
  year      = {2013},
  edition   = {2nd},
  isbn      = {8847055210},
}



@Article{Mis2021_EstimatesGeneralizationError_MolMM,
  author    = {Siddhartha Mishra and Roberto Molinaro},
  journal   = {{IMA} Journal of Numerical Analysis},
  title     = {Estimates on the generalization error of physics-informed neural networks for approximating a class of inverse problems for {PDEs}},
  year      = {2021},
  month     = {jun},
  doi       = {10.1093/imanum/drab032},
  groups    = {Discussion},
  issn = {0272-4979},
  publisher = {Oxford University Press ({OUP})},
  url       = {https://doi.org/10.1093/imanum/drab032},
}


@Article{Bar1993_UniversalApproximationBounds_Bar,
  author  = {Barron, A.R.},
  journal = {IEEE Transactions on Information Theory},
  title   = {Universal approximation bounds for superpositions of a sigmoidal function},
  year    = {1993},
  number  = {3},
  pages   = {930-945},
  volume  = {39},
  doi     = {10.1109/18.256500},
  groups  = {Discussion},
}



@Article{Yar2017_ErrorBoundsApproximations_Yar,
  author   = {Dmitry Yarotsky},
  journal  = {Neural Networks},
  title    = {Error bounds for approximations with deep ReLU networks},
  year     = {2017},
  issn     = {0893-6080},
  pages    = {103-114},
  volume   = {94},
  abstract = {We study expressive power of shallow and deep neural networks with piece-wise linear activation functions. We establish new rigorous upper and lower bounds for the network complexity in the setting of approximations in Sobolev spaces. In particular, we prove that deep ReLU networks more efficiently approximate smooth functions than shallow networks. In the case of approximations of 1D Lipschitz functions we describe adaptive depth-6 network architectures more efficient than the standard shallow architecture.},
  doi      = {https://doi.org/10.1016/j.neunet.2017.07.002},
  groups   = {Discussion},
  keywords = {Deep ReLU networks, Approximation complexity},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608017301545},
}



@Article{Gos2020_TransferLearningEnhanced_AniGACR,
  author   = {Somdatta Goswami and Cosmin Anitescu and Souvik Chakraborty and Timon Rabczuk},
  journal  = {Theoretical and Applied Fracture Mechanics},
  title    = {Transfer learning enhanced physics informed neural network for phase-field modeling of fracture},
  year     = {2020},
  issn     = {0167-8442},
  pages    = {102447},
  volume   = {106},
  doi      = {https://doi.org/10.1016/j.tafmec.2019.102447},
  groups   = {Main},
  keywords = {Physics informed, Deep neural network, Variational energy, Phase-field, Brittle fracture},
  url      = {https://www.sciencedirect.com/science/article/pii/S016784421930357X},
}


@Article{Zhu2019_PhysicsConstrainedDeep_ZabZZKP,
  author   = {Yinhao Zhu and Nicholas Zabaras and Phaedon-Stelios Koutsourelakis and Paris Perdikaris},
  journal  = {Journal of Computational Physics},
  title    = {Physics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data},
  year     = {2019},
  issn     = {0021-9991},
  pages    = {56-81},
  volume   = {394},
  doi      = {https://doi.org/10.1016/j.jcp.2019.05.024},
  groups   = {Main},
  keywords = {Physics-constrained, Normalizing flow, Conditional generative model, Reverse KL divergence, Surrogate modeling, Uncertainty quantification},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999119303559},
}


@Article{Yan2020_PhysicsInformedGenerative_ZhaYZK,
  author    = {Yang, Liu and Zhang, Dongkun and Karniadakis, George Em},
  journal   = {SIAM Journal on Scientific Computing},
  title     = {Physics-informed generative adversarial networks for stochastic differential equations},
  year      = {2020},
  number    = {1},
  pages     = {A292--A317},
  volume    = {42},
  doi       = {10.1137/18M1225409},
  eprint    = {https://doi.org/10.1137/18M1225409},
  groups    = {Main},
  publisher = {SIAM},
  url       = {https://doi.org/10.1137/18M1225409},
}


@Article{Gen2020_ModelingDynamicsPde_ZabGZ,
  author   = {Nicholas Geneva and Nicholas Zabaras},
  journal  = {Journal of Computational Physics},
  title    = {Modeling the dynamics of PDE systems with physics-constrained deep auto-regressive networks},
  year     = {2020},
  issn     = {0021-9991},
  pages    = {109056},
  volume   = {403},
  doi      = {https://doi.org/10.1016/j.jcp.2019.109056},
  groups   = {Main},
  keywords = {Physics-informed machine learning, Auto-regressive model, Deep neural networks, Convolutional encoder-decoder, Uncertainty quantification, Dynamic partial differential equations},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999119307612},
}


@Article{Kis2020_MachineLearningCardiovascular_YanKYH,
  author     = {Kissas, Georgios and Yang, Yibo and Hwuang, Eileen and Witschey, Walter R. and Detre, John A. and Perdikaris, Paris},
  journal    = {Computer Methods in Applied Mechanics and Engineering},
  title      = {Machine learning in cardiovascular flows modeling: {Predicting} arterial blood pressure from non-invasive {4D} flow {MRI} data using physics-informed neural networks},
  year       = {2020},
  issn       = {0045-7825},
  month      = jan,
  pages      = {112623},
  volume     = {358},
  doi        = {10.1016/j.cma.2019.112623},
  groups     = {Main},
  keywords   = {Deep neural networks, Blood flow modeling, Pulse wave propagation, Data-driven modeling, Non-invasive diagnostics},
  language   = {en},
  shorttitle = {Machine learning in cardiovascular flows modeling},
  url        = {https://www.sciencedirect.com/science/article/pii/S0045782519305055},
}

@Article{Cai2021_DeepmmnetInferringElectroconvection_WanCWL,
  author     = {Cai, Shengze and Wang, Zhicheng and Lu, Lu and Zaki, Tamer A. and Karniadakis, George Em},
  journal    = {Journal of Computational Physics},
  title      = {{DeepM}\&{Mnet}: {Inferring} the electroconvection multiphysics fields based on operator approximation by neural networks},
  year       = {2021},
  issn       = {0021-9991},
  month      = jul,
  pages      = {110296},
  volume     = {436},
  doi        = {10.1016/j.jcp.2021.110296},
  keywords   = {Mutiphysics, Multiscale modeling, Deep learning, Data assimilation, Operator approximation, DeepONet},
  language   = {en},
  shorttitle = {{DeepM}\&{Mnet}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999121001911},
}


@Article{Mat2021_UncoveringTurbulentPlasma_FraMFH,
  author    = {Mathews, Abhilash and Francisquez, Manaure and Hughes, Jerry W. and Hatch, David R. and Zhu, Ben and Rogers, Barrett N.},
  journal   = {Physical Review E},
  title     = {Uncovering turbulent plasma dynamics via deep learning from partial observations},
  year      = {2021},
  issn      = {2470-0045},
  month     = aug,
  number    = {2},
  volume    = {104},
  doi       = {10.1103/physreve.104.025205},
  groups    = {Main},
  language  = {English},
  publisher = {American Physical Society (APS)},
  url       = {https://www.osti.gov/pages/biblio/1813020},
}



@Article{Pan2019_FpinnsFractionalPhysics_LuPLK,
  author     = {Pang, Guofei and Lu, Lu and Karniadakis, George Em},
  journal    = {SIAM Journal on Scientific Computing},
  title      = {{fPINNs}: {Fractional} {Physics}-{Informed} {Neural} {Networks}},
  year       = {2019},
  issn       = {1064-8275},
  month      = jan,
  number     = {4},
  pages      = {A2603--A2626},
  volume     = {41},
  doi        = {10.1137/18M1229845},
  publisher  = {Society for Industrial and Applied Mathematics},
  shorttitle = {{fPINNs}},
  url        = {https://epubs.siam.org/doi/abs/10.1137/18M1229845},
}



@Article{Yan2021_BPinnsBayesian_MenYMK,
  author     = {Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
  journal    = {Journal of Computational Physics},
  title      = {B-{PINNs}: {Bayesian} physics-informed neural networks for forward and inverse {PDE} problems with noisy data},
  year       = {2021},
  issn       = {0021-9991},
  month      = jan,
  pages      = {109913},
  volume     = {425},
  doi        = {10.1016/j.jcp.2020.109913},
  keywords   = {Nonlinear PDEs, Noisy data, Bayesian physics-informed neural networks, Hamiltonian Monte Carlo, Variational inference},
  language   = {en},
  shorttitle = {B-{PINNs}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999120306872},
}




@Article{Jag2020_ConservativePhysicsInformed_KhaJKK,
  author     = {Jagtap, Ameya D. and Kharazmi, Ehsan and Karniadakis, George Em},
  journal    = {Computer Methods in Applied Mechanics and Engineering},
  title      = {Conservative physics-informed neural networks on discrete domains for conservation laws: {Applications} to forward and inverse problems},
  year       = {2020},
  issn       = {0045-7825},
  month      = jun,
  pages      = {113028},
  volume     = {365},
  doi        = {10.1016/j.cma.2020.113028},
  groups     = {Main},
  keywords   = {cPINN, Mortar PINN, Domain decomposition, Machine learning, Conservation laws, Inverse problems},
  language   = {en},
  shorttitle = {Conservative physics-informed neural networks on discrete domains for conservation laws},
  url        = {https://www.sciencedirect.com/science/article/pii/S0045782520302127},
}








@Article{Kha2021_HpVpinnsVariational_ZhaKZK,
  author     = {Kharazmi, Ehsan and Zhang, Zhongqiang and Karniadakis, George E. M.},
  journal    = {Computer Methods in Applied Mechanics and Engineering},
  title      = {hp-{VPINNs}: {Variational} physics-informed neural networks with domain decomposition},
  year       = {2021},
  issn       = {0045-7825},
  month      = feb,
  pages      = {113547},
  volume     = {374},
  doi        = {10.1016/j.cma.2020.113547},
  groups     = {Main},
  keywords   = {Physics-informed learning, VPINNs, Variational neural network, Domain decomposition, Automatic differentiation, -refinement, Partial differential equations},
  language   = {en},
  shorttitle = {hp-{VPINNs}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0045782520307325},
}





@InCollection{Kol2021_PhysicsInformedNeural_DAKDJH,
  author    = {Kollmannsberger, Stefan and D’Angella, Davide and Jokeit, Moritz and Herrmann, Leon},
  publisher = {Springer International Publishing},
  title     = {Physics-{Informed} {Neural} {Networks}},
  booktitle = {Deep Learning in Computational Mechanics},
  year      = {2021},
  address   = {Cham},
  editor    = {Kollmannsberger, Stefan and D'Angella, Davide and Jokeit, Moritz and Herrmann, Leon},
  isbn      = {9783030765873},
  pages     = {55--84},
  series    = {Studies in {Computational} {Intelligence}},
  doi       = {10.1007/978-3-030-76587-3_5},
  groups    = {Main},
  language  = {en},
  url       = {https://doi.org/10.1007/978-3-030-76587-3_5},
}


@Article{Wan2021_UnderstandingMitigatingGradient_TenWTP,
  author    = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  journal   = {SIAM Journal on Scientific Computing},
  title     = {Understanding and {Mitigating} {Gradient} {Flow} {Pathologies} in {Physics}-{Informed} {Neural} {Networks}},
  year      = {2021},
  issn      = {1064-8275},
  month     = jan,
  number    = {5},
  pages     = {A3055--A3081},
  volume    = {43},
  doi       = {10.1137/20M1318043},
  groups    = {Main},
  keywords  = {deep learning, differential equations, optimization, stiff dynamics, computational physics, 68T99, 65M99, 68U20},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://epubs.siam.org/doi/abs/10.1137/20M1318043},
}




@Article{Men2020_PpinnPararealPhysics_LiMLZK,
  author   = {Xuhui Meng and Zhen Li and Dongkun Zhang and George Em Karniadakis},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {PPINN: Parareal physics-informed neural network for time-dependent PDEs},
  year     = {2020},
  issn     = {0045-7825},
  pages    = {113250},
  volume   = {370},
  doi      = {https://doi.org/10.1016/j.cma.2020.113250},
  groups   = {Main},
  keywords = {Deep neural network, Machine learning, Parallel-in-time, Long-time integration, Multiscale, PINN},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782520304357},
}






%%%%%%%%%%%%%%%%%%%
%Applications  - START
%%%%%%%%%%%%%%%%%



@Article{Fan2021_HighEfficientHybrid_Fan,
  author   = {Fang, Zhiwei},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  title    = {A {High}-{Efficient} {Hybrid} {Physics}-{Informed} {Neural} {Networks} {Based} on {Convolutional} {Neural} {Network}},
  year     = {2021},
  issn     = {2162-2388},
  pages    = {1--13},
  doi      = {10.1109/TNNLS.2021.3070878},
  groups   = {Applications},
  keywords = {Neural networks, Machine learning, Training, Learning systems, Inverse problems, Handheld computers, Geometry, Convolutional neural network (CNN), finite volume method, finite-difference method, hybrid physics-informed neural network (hybrid PINN), local fitting method.},
}







@Article{Irr2021_TowardsNeuralEarth_BoeIBS,
  author    = {Irrgang, Christopher and Boers, Niklas and Sonnewald, Maike and Barnes, Elizabeth A. and Kadow, Christopher and Staneva, Joanna and Saynisch-Wagner, Jan},
  journal   = {Nature Machine Intelligence},
  title     = {Towards neural {Earth} system modelling by integrating artificial intelligence in {Earth} system science},
  year      = {2021},
  issn      = {2522-5839},
  month     = aug,
  number    = {8},
  pages     = {667--674},
  volume    = {3},
  copyright = {2021 Springer Nature Limited},
  doi       = {10.1038/s42256-021-00374-3},
  file      = {Full Text PDF:https\://www.nature.com/articles/s42256-021-00374-3.pdf:application/pdf},
  groups    = {Applications},
  keywords  = {Applied mathematics, Climate and Earth system modelling, Climate sciences, Computational science, Environmental sciences},
  language  = {en},
  publisher = {Nature Publishing Group},
  url       = {https://www.nature.com/articles/s42256-021-00374-3},
}




@Article{Che2020_PhysicsInformedNeural_LuCLK,
  author    = {Chen, Yuyao and Lu, Lu and Karniadakis, George Em and Negro, Luca Dal and Negro, Luca Dal and Negro, Luca Dal and Negro, Luca Dal},
  journal   = {Optics Express},
  title     = {Physics-informed neural networks for inverse problems in nano-optics and metamaterials},
  year      = {2020},
  issn      = {1094-4087},
  month     = apr,
  number    = {8},
  pages     = {11618--11633},
  volume    = {28},
  copyright = {\&\#169; 2020 Optical Society of America},
  doi       = {10.1364/OE.384875},
  file      = {Full Text PDF:https\://www.osapublishing.org/viewmedia.cfm?uri=oe-28-8-11618&seq=0:application/pdf},
  groups    = {Applications},
  language  = {EN},
  publisher = {Optical Society of America},
  url       = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-28-8-11618},
}



@Article{Fan2020_DeepPhysicalInformed_ZhaFZ,
  author   = {Fang, Zhiwei and Zhan, Justin},
  journal  = {IEEE Access},
  title    = {Deep {Physical} {Informed} {Neural} {Networks} for {Metamaterial} {Design}},
  year     = {2020},
  issn     = {2169-3536},
  pages    = {24506--24513},
  volume   = {8},
  doi      = {10.1109/ACCESS.2019.2963375},
  groups   = {Applications},
  keywords = {Metamaterials, Maxwell equations, Frequency-domain analysis, Neurons, Artificial neural networks, Training, PINN, activation function, metamaterial design, electromagnetic cloaking, Maxwell’s equation},
}


@Article{Wie2021_DeepLearningNano_ArbWAA,
  author     = {Wiecha, Peter R. and Arbouet, Arnaud and Arbouet, Arnaud and Girard, Christian and Girard, Christian and Muskens, Otto L. and Muskens, Otto L.},
  journal    = {Photonics Research},
  title      = {Deep learning in nano-photonics: inverse design and beyond},
  year       = {2021},
  issn       = {2327-9125},
  month      = may,
  number     = {5},
  pages      = {B182--B200},
  volume     = {9},
  doi        = {10.1364/PRJ.415960},
  groups     = {Applications},
  keywords   = {Analog optical computing, Diffraction limit, Localized surface plasmon resonance, Photonic devices, Spatial light modulators, Stochastic gradient descent},
  language   = {EN},
  publisher  = {Optical Society of America},
  shorttitle = {Deep learning in nano-photonics},
  url        = {https://www.osapublishing.org/prj/abstract.cfm?uri=prj-9-5-B182},
}


@article{Misyris2020PhysicsInformedNN,
  title={Physics-Informed Neural Networks for Power Systems},
  author={George S. Misyris and Andreas Venzke and Spyros Chatzivasileiadis},
  journal={2020 IEEE Power \& Energy Society General Meeting (PESGM)},
  year={2020},
  pages={1-5}
}



@Article{Isl2021_ExtractionMaterialProperties_ThaITMH,
  author   = {Islam, Mahmudul and Thakur, Md Shajedul Hoque and Mojumder, Satyajit and Hasan, Mohammad Nasim},
  journal  = {Computational Materials Science},
  title    = {Extraction of material properties through multi-fidelity deep learning from molecular dynamics simulation},
  year     = {2021},
  issn     = {0927-0256},
  month    = feb,
  pages    = {110187},
  volume   = {188},
  doi      = {10.1016/j.commatsci.2020.110187},
  groups   = {Applications},
  keywords = {Molecular dynamics, Multi-fidelity models, Deep neural networks, Nanofluid, Material properties},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0927025620306789},
}



@inproceedings{neuraltangents2020,
    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://github.com/google/neural-tangents}
}


@Article{Che2021_RecentAdvanceMachine_SeeCS,
  author   = {Cheung, Ka Chun and See, Simon},
  journal  = {CCF Transactions on High Performance Computing},
  title    = {Recent advance in machine learning for partial differential equation},
  year     = {2021},
  issn     = {2524-4930},
  number   = {3},
  pages    = {298--310},
  volume   = {3},
  abstract = {Machine learning method has been applied to solve different kind of problems in different areas due to the great success in several tasks such as computer vision, natural language processing and robotic in recent year. In scientific computing community, it is well-known that solving partial differential equations, which are naturally derived from physical rules that describe some of phenomena, is a challenging task in terms of computational efficiency and model accuracy. On the other hand, machine learning models are data-driven that purely reply on learning the pattern of the data distribution. Researcher recently proposed a few new frameworks to solve certain kind of partial differential equations with machine learning technique. In this paper, we discuss two newly developed machine learning based methods for solving partial differential equations.},
  doi      = {10.1007/s42514-021-00076-7},
  groups   = {Software},
  refid    = {Cheung2021},
  url      = {https://doi.org/10.1007/s42514-021-00076-7},
}

@Article{Mos2021_FiniteBasisPhysics_MarMMN,
  author     = {Moseley, Ben and Markham, Andrew and Nissen-Meyer, Tarje},
  journal    = {arXiv:2107.07871 [physics]},
  title      = {Finite {Basis} {Physics}-{Informed} {Neural} {Networks} ({FBPINNs}): a scalable domain decomposition approach for solving differential equations},
  year       = {2021},
  month      = jul,
  note       = {arXiv: 2107.07871},
  annote     = {Comment: 27 pages, 13 figures},
  groups     = {Main},
  keywords   = {Physics - Computational Physics, Computer Science - Machine Learning},
  shorttitle = {Finite {Basis} {Physics}-{Informed} {Neural} {Networks} ({FBPINNs})},
  url        = {http://arxiv.org/abs/2107.07871},
}


@InProceedings{Bal2021_DistributedMultigridNeural_BotBBK,
  author    = {Balu, Aditya and Botelho, Sergio and Khara, Biswajit and Rao, Vinay and Sarkar, Soumik and Hegde, Chinmay and Krishnamurthy, Adarsh and Adavani, Santi and Ganapathysubramanian, Baskar},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  title     = {Distributed Multigrid Neural Solvers on Megavoxel Domains},
  year      = {2021},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series    = {SC '21},
  articleno = {49},
  doi       = {10.1145/3458817.3476218},
  isbn      = {9781450384421},
  keywords  = {multigrid, physics aware neural networks, neural PDE solvers, distributed training},
  location  = {St. Louis, Missouri},
  numpages  = {14},
  url       = {https://doi.org/10.1145/3458817.3476218},
}


@Article{Mar2019_ReviewAutomaticDifferentiation_Mar,
  author   = {Margossian, Charles C.},
  journal  = {WIREs Data Mining and Knowledge Discovery},
  title    = {A review of automatic differentiation and its efficient implementation},
  year     = {2019},
  issn     = {1942-4795},
  number   = {4},
  pages    = {e1305},
  volume   = {9},
  abstract = {Derivatives play a critical role in computational statistics, examples being Bayesian inference using Hamiltonian Monte Carlo sampling and the training of neural networks. Automatic differentiation (AD) is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions. The implementation of AD, however, requires some care to insure efficiency. Modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management. Among these techniques are operation overloading, region-based memory, and expression templates. There also exist several mathematical techniques which can yield high performance gains when applied to complex algorithms. For example, semi-analytical derivatives can reduce by orders of magnitude the runtime required to numerically solve and differentiate an algebraic equation. Open and practical problems include the extension of current packages to provide more specialized routines, and finding optimal methods to perform higher-order differentiation. This article is categorized under: Algorithmic Development {\textgreater} Scalable Statistical Methods},
  doi      = {10.1002/widm.1305},
  groups   = {Main},
  keywords = {automatic differentiation, computational statistics, numerical methods},
  language = {en},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1305},
}

%TODO Prep  arxiv type!
%journal  = {Preprint},
@Article{Pra2021_AnnsBasedMethod_BakPBMM,
  author     = {Pratama, Danang Adi and Bakar, Maharani Abu and Man, Mustafa and Mashuri, M.},
  title      = {{ANNs}-{Based} {Method} for {Solving} {Partial} {Differential} {Equations} : {A} {Survey}},
  journal  = {Preprint},
  year       = {2021},
  month      = feb,
  doi        = {10.20944/preprints202102.0160.v1},
  groups     = {Software},
  language   = {en},
  publisher  = {Preprints},
  shorttitle = {{ANNs}-{Based} {Method} for {Solving} {Partial} {Differential} {Equations}},
  url        = {https://www.preprints.org/manuscript/202102.0160/v1},
}



@Article{Wan2022_WhenWhyPinns_YuWYP,
  author     = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
  journal    = {Journal of Computational Physics},
  title      = {When and why {PINNs} fail to train: {A} neural tangent kernel perspective},
  year       = {2022},
  issn       = {0021-9991},
  month      = jan,
  pages      = {110768},
  volume     = {449},
  doi        = {10.1016/j.jcp.2021.110768},
  groups     = {Main},
  keywords   = {Physics-informed neural networks, Spectral bias, Multi-task learning, Gradient descent, Scientific machine learning},
  language   = {en},
  shorttitle = {When and why {PINNs} fail to train},
  url        = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
}



@Article{Kar2021_PhysicsInformedMachine_KevKKL,
  author    = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  journal   = {Nature Reviews Physics},
  title     = {Physics-informed machine learning},
  year      = {2021},
  issn      = {2522-5820},
  month     = jun,
  number    = {6},
  pages     = {422--440},
  volume    = {3},
  copyright = {2021 Springer Nature Limited},
  doi       = {10.1038/s42254-021-00314-5},
  groups    = {Main},
  keywords  = {Applied mathematics, Computational science},
  language  = {en},
  publisher = {Nature Publishing Group},
  url       = {https://www.nature.com/articles/s42254-021-00314-5},
}




@Article{Won2021_CanTransferNeuroevolution_GupWGO,
  author   = {Wong, Jian Cheng and Gupta, Abhishek and Ong, Yew-Soon},
  journal  = {IEEE Computational Intelligence Magazine},
  title    = {Can {Transfer} {Neuroevolution} {Tractably} {Solve} {Your} {Differential} {Equations}?},
  year     = {2021},
  issn     = {1556-6048},
  month    = may,
  number   = {2},
  pages    = {14--30},
  volume   = {16},
  doi      = {10.1109/MCI.2021.3061854},
  groups   = {nxt},
  keywords = {Neural networks, Differential equations, Network architecture, Search problems, Probabilistic logic, Optimization, Convergence},
}


@Article{Sun2020_SurveyOptimizationMethods_CaoSCZZ,
  author   = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
  journal  = {IEEE Transactions on Cybernetics},
  title    = {A {Survey} of {Optimization} {Methods} {From} a {Machine} {Learning} {Perspective}},
  year     = {2020},
  issn     = {2168-2275},
  month    = aug,
  number   = {8},
  pages    = {3668--3681},
  volume   = {50},
  doi      = {10.1109/TCYB.2019.2950779},
  groups   = {Discussion},
  keywords = {Machine learning, Optimization methods, Stochastic processes, Machine learning algorithms, Linear programming, Task analysis, Approximate Bayesian inference, deep neural network (DNN), machine learning, optimization method, reinforcement learning (RL)},
}

%%%%%%%%%%%%%
%CDC
%%%%%%%%%%%%




@Article{Wan2021_TheoryGuidedAuto_ChaWCZ,
  author   = {Wang, Nanzhe and Chang, Haibin and Zhang, Dongxiao},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Theory-guided Auto-Encoder for surrogate construction and inverse modeling},
  year     = {2021},
  issn     = {0045-7825},
  pages    = {114037},
  volume   = {385},
  abstract = {A Theory-guided Auto-Encoder (TgAE) framework is proposed for surrogate construction, and is further used for uncertainty quantification and inverse modeling tasks. The framework is built based on the Auto-Encoder (or Encoder-Decoder) architecture of the convolutional neural network (CNN) via a theory-guided training process. In order to incorporate physical constraints for achieving theory-guided training, the governing equations of the studied problems can be discretized by the finite difference scheme, and then be embedded into the training of the CNN. The residual of the discretized governing equations, as well as the data mismatch, constitute the loss function of the TgAE. The trained TgAE can be utilized to construct a surrogate that approximates the relationship between the model parameters and model responses with limited labeled data. Several subsurface flow cases are designed to test the performance of the TgAE. The results demonstrate that satisfactory accuracy for surrogate modeling and higher efficiency for uncertainty quantification tasks can be achieved with the TgAE. The TgAE also shows good extrapolation ability for cases with different correlation lengths and variances. Furthermore, inverse modeling tasks are also implemented with the TgAE surrogate, and satisfactory results are obtained.},
  doi      = {10.1016/j.cma.2021.114037},
  groups   = {nxt},
  keywords = {Theory-guided Auto-Encoder (TgAE), Surrogate construction, Uncertainty quantification, Inverse modeling, Auto-Encoder, Convolutional neural network (CNN)},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782521003686},
}



@Article{Fan2020_PhysicsInformedNeural_ZhaFZ,
  author     = {Fang, Zhiwei and Zhan, Justin},
  journal    = {IEEE Access},
  title      = {A {Physics}-{Informed} {Neural} {Network} {Framework} for {PDEs} on {3D} {Surfaces}: {Time} {Independent} {Problems}},
  year       = {2020},
  issn       = {2169-3536},
  pages      = {26328--26335},
  volume     = {8},
  doi        = {10.1109/ACCESS.2019.2963390},
  groups     = {nxt},
  keywords   = {Artificial neural networks, Three-dimensional displays, Surface treatment, Differential operators, Neurons, Numerical stability, Power system stability, PINN, PDEs on surfaces, Laplace-Beltrami operator},
  shorttitle = {A {Physics}-{Informed} {Neural} {Network} {Framework} for {PDEs} on {3D} {Surfaces}},
}


@Article{Zha2020_PhysicsInformedMulti_LiuZLS,
  author   = {Zhang, Ruiyang and Liu, Yang and Sun, Hao},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Physics-informed multi-{LSTM} networks for metamodeling of nonlinear structures},
  year     = {2020},
  issn     = {0045-7825},
  month    = sep,
  pages    = {113226},
  volume   = {369},
  doi      = {10.1016/j.cma.2020.113226},
  groups   = {PaperWithCode},
  keywords = {Physics-informed deep learning, Long short-term memory (LSTM), Metamodeling, Nonlinear structures, PhyLSTM, PhyLSTM},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782520304114},
}




@Article{Wah2021_PinneikEikonalSolution_HagWHA,
  author     = {Waheed, Umair bin and Haghighat, Ehsan and Alkhalifah, Tariq and Song, Chao and Hao, Qi},
  journal    = {Computers \& Geosciences},
  title      = {{PINNeik}: {Eikonal} solution using physics-informed neural networks},
  year       = {2021},
  issn       = {0098-3004},
  month      = oct,
  pages      = {104833},
  volume     = {155},
  doi        = {10.1016/j.cageo.2021.104833},
  groups     = {PaperWithCode},
  keywords   = {Eikonal equation, Physics-informed neural networks, Seismic modeling, Traveltimes},
  language   = {en},
  shorttitle = {{PINNeik}},
  url        = {https://www.sciencedirect.com/science/article/pii/S009830042100131X},
  urldate    = {2021-11-03},
}




@Article{Via2021_EstimatingModelInadequacy_NasVNDY,
  author   = {Viana, Felipe A. C. and Nascimento, Renato G. and Dourado, Arinan and Yucesan, Yigit A.},
  journal  = {Computers \& Structures},
  title    = {Estimating model inadequacy in ordinary differential equations with physics-informed neural networks},
  year     = {2021},
  issn     = {0045-7949},
  month    = mar,
  pages    = {106458},
  volume   = {245},
  doi      = {10.1016/j.compstruc.2020.106458},
  groups   = {PaperWithCode},
  keywords = {Physics-informed machine learning, Scientific machine learning, Uncertainty quantification, Recurrent neural networks, Directed graph models},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045794920302613},
}



@InProceedings{Sti2020_LargeScaleNeural_BetSBB,
  author    = {Stiller, Patrick and Bethke, Friedrich and Böhme, Maximilian and Pausch, Richard and Torge, Sunna and Debus, Alexander and Vorberger, Jan and Bussmann, Michael and Hoffmann, Nico},
  booktitle = {Driving {Scientific} and {Engineering} {Discoveries} {Through} the {Convergence} of {HPC}, {Big} {Data} and {AI}},
  title     = {Large-{Scale} {Neural} {Solvers} for {Partial} {Differential} {Equations}},
  year      = {2020},
  address   = {Cham},
  editor    = {Nichols, Jeffrey and Verastegui, Becky and Maccabe, Arthur 'Barney' and Hernandez, Oscar and Parete-Koon, Suzanne and Ahearn, Theresa},
  pages     = {20--34},
  publisher = {Springer International Publishing},
  series    = {Communications in {Computer} and {Information} {Science}},
  doi       = {10.1007/978-3-030-63393-6_2},
  groups    = {PaperWithCode},
  isbn      = {9783030633936},
  language  = {en},
}


@Article{Yuc2021_HybridPhysicsInformed_ViaYV,
  author   = {Yucesan, Yigit A. and Viana, Felipe A. C.},
  journal  = {Computers in Industry},
  title    = {Hybrid physics-informed neural networks for main bearing fatigue prognosis with visual grease inspection},
  year     = {2021},
  issn     = {0166-3615},
  month    = feb,
  pages    = {103386},
  volume   = {125},
  doi      = {10.1016/j.compind.2020.103386},
  groups   = {PaperWithCode},
  keywords = {Physics-informed neural networks, Scientific machine learning, Uncertainty quantification},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0166361520306205},
}



%%%%%%%%%%%%%%%%%%%
%Elliptic  - START
%%%%%%%%%%%%%%%%%



@Article{Gru2021_DeepNeuralNetwork_HajGHL,
  author    = {Grubišić, Luka and Hajba, Marko and Lacmanović, Domagoj},
  journal   = {Entropy},
  title     = {Deep {Neural} {Network} {Model} for {Approximating} {Eigenmodes} {Localized} by a {Confining} {Potential}},
  year      = {2021},
  month     = jan,
  number    = {1},
  pages     = {95},
  volume    = {23},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi       = {10.3390/e23010095},
  groups    = {nxt},
  keywords  = {Anderson localization, deep neural networks, residual error estimates, physics informed neural networks},
  language  = {en},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {https://www.mdpi.com/1099-4300/23/1/95},
}



@Article{Kha2019_VariationalPhysicsInformed_ZhaKZK,
  author   = {Kharazmi, E. and Zhang, Z. and Karniadakis, G. E.},
  journal  = {arXiv:1912.00873 [physics, stat]},
  title    = {Variational {Physics}-{Informed} {Neural} {Networks} {For} {Solving} {Partial} {Differential} {Equations}},
  year     = {2019},
  month    = nov,
  note     = {arXiv: 1912.00873},
  annote   = {Comment: 24 pages, 12 figures},
  groups   = {Main},
  keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Physics - Computational Physics, Statistics - Machine Learning},
  url      = {http://arxiv.org/abs/1912.00873},
}





@Article{Hag2021_NonlocalPhysicsInformed_BekHBMJ,
  author   = {Haghighat, Ehsan and Bekar, Ali Can and Madenci, Erdogan and Juanes, Ruben},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {A nonlocal physics-informed deep learning framework using the peridynamic differential operator},
  year     = {2021},
  issn     = {0045-7825},
  month    = nov,
  pages    = {114012},
  volume   = {385},
  doi      = {10.1016/j.cma.2021.114012},
  groups   = {nxt},
  keywords = {Deep learning, Peridynamic Differential Operator, Physics-Informed Neural Networks, Surrogate models},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782521003431},
}



@Article{Dwi2020_PhysicsInformedExtreme_SriDS,
  author   = {Dwivedi, Vikas and Srinivasan, Balaji},
  journal  = {Neurocomputing},
  title    = {Physics {Informed} {Extreme} {Learning} {Machine} ({PIELM})–{A} rapid method for the numerical solution of partial differential equations},
  year     = {2020},
  issn     = {0925-2312},
  month    = may,
  pages    = {96--118},
  volume   = {391},
  doi      = {10.1016/j.neucom.2019.12.099},
  keywords = {Partial differential equations, Physics informed neural networks, Extreme learning machine, Advection-Diffusion equation},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231219318144},
}




@Article{Tar2020_PhysicsInformedDeep_MarTMP,
  author   = {Tartakovsky, A. M. and Marrero, C. Ortiz and Perdikaris, Paris and Tartakovsky, G. D. and Barajas-Solano, D.},
  journal  = {Water Resources Research},
  title    = {Physics-{Informed} {Deep} {Neural} {Networks} for {Learning} {Parameters} and {Constitutive} {Relationships} in {Subsurface} {Flow} {Problems}},
  year     = {2020},
  issn     = {1944-7973},
  number   = {5},
  pages    = {e2019WR026731},
  volume   = {56},
  annote   = {e2019WR026731 10.1029/2019WR026731},
  doi      = {10.1029/2019WR026731},
  groups   = {nxt},
  keywords = {deep neural networks, physics-informed machine learning, parameter estimation, learning constitutive relationships, unsaturated flow, MAP},
  language = {en},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019WR026731},
}


%%%%%%%%%%%%%%%%%%%
%Elliptic   - END
%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%
%Advection Diffusion Reaction  - START
%%%%%%%%%%%%%%%%%


@Article{Ami2021_PhysicsInformedNeural_HagANHC,
  author   = {Amini Niaki, Sina and Haghighat, Ehsan and Campbell, Trevor and Poursartip, Anoush and Vaziri, Reza},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Physics-informed neural network for modelling the thermochemical curing process of composite-tool systems during manufacture},
  year     = {2021},
  issn     = {0045-7825},
  month    = oct,
  pages    = {113959},
  volume   = {384},
  doi      = {10.1016/j.cma.2021.113959},
  groups   = {nxt},
  keywords = {Physics-informed neural networks, Deep learning, Composites processing, Exothermic heat transfer, Resin reaction, Surrogate modelling},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782521002966},
}





@Article{Wan2021_DeepLearningFree_PerWP,
  author   = {Wang, Sifan and Perdikaris, Paris},
  journal  = {Journal of Computational Physics},
  title    = {Deep learning of free boundary and {Stefan} problems},
  year     = {2021},
  issn     = {0021-9991},
  month    = mar,
  pages    = {109914},
  volume   = {428},
  doi      = {10.1016/j.jcp.2020.109914},
  groups   = {nxt},
  keywords = {Keywords Physics-informed neural networks, Phase transitions, Partial differential equations, Scientific machine learning},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999120306884},
}


@Article{He2021_PhysicsInformedNeural_TarHT,
  author   = {He, QiZhi and Tartakovsky, Alexandre M.},
  journal  = {Water Resources Research},
  title    = {Physics-Informed Neural Network Method for Forward and Backward Advection-Dispersion Equations},
  year     = {2021},
  note     = {e2020WR029479 2020WR029479},
  number   = {7},
  pages    = {e2020WR029479},
  volume   = {57},
  doi      = {10.1029/2020WR029479},
  groups   = {nxt},
  keywords = {physics-informed machine learning, forward advection-dispersion equations, backward advection-dispersion equations, data assimilation, source identification},
  url      = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020WR029479},
}


%%%%%%%%%%%%%%%%%%%
%Advection Diffusion Reaction  - END
%%%%%%%%%%%%%%%%%



@Article{Cai2021_PhysicsInformedNeural_WanCWW,
  author   = {Cai, Shengze and Wang, Zhicheng and Wang, Sifan and Perdikaris, Paris and Karniadakis, George Em},
  journal  = {Journal of Heat Transfer},
  title    = {Physics-{Informed} {Neural} {Networks} for {Heat} {Transfer} {Problems}},
  year     = {2021},
  issn     = {0022-1481},
  month    = apr,
  number   = {6},
  volume   = {143},
  doi      = {10.1115/1.4050542},
  groups   = {nxt},
  url      = {https://doi.org/10.1115/1.4050542},
}



@Article{Mo2022_DataDrivenVector_LinMLZ,
  author   = {Mo, Yifan and Ling, Liming and Zeng, Delu},
  journal  = {Physics Letters A},
  title    = {Data-driven vector soliton solutions of coupled nonlinear {Schrödinger} equation using a deep learning algorithm},
  year     = {2022},
  issn     = {0375-9601},
  month    = jan,
  pages    = {127739},
  volume   = {421},
  doi      = {10.1016/j.physleta.2021.127739},
  groups   = {Applications},
  keywords = {Deep learning, Coupled nonlinear Schrödinger equation, Nondegenerate vector soliton, Two-soliton solutions, Data-driven solutions},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0375960121006034},
}



@Article{Wan2021_DataDrivenRogue_YanWY,
  author   = {Wang, Li and Yan, Zhenya},
  journal  = {Physics Letters A},
  title    = {Data-driven rogue waves and parameter discovery in the defocusing nonlinear {Schrödinger} equation with a potential using the {PINN} deep learning},
  year     = {2021},
  issn     = {0375-9601},
  month    = jul,
  pages    = {127408},
  volume   = {404},
  doi      = {10.1016/j.physleta.2021.127408},
  groups   = {Applications},
  keywords = {Defocusing NLS equation with the time-dependent potential, Initial-boundary value conditions, Physics-informed neural networks, Deep learning, Data-driven rogue waves and parameter discovery},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0375960121002723},
}

%%%%%%%%%%%%%%%%%%%
%NS  - START
%%%%%%%%%%%%%%%%%



@Article{Jin2021_NsfnetsNavierStokes_CaiJCLK,
  author     = {Jin, Xiaowei and Cai, Shengze and Li, Hui and Karniadakis, George Em},
  journal    = {Journal of Computational Physics},
  title      = {{NSFnets} ({Navier}-{Stokes} flow nets): {Physics}-informed neural networks for the incompressible {Navier}-{Stokes} equations},
  year       = {2021},
  issn       = {0021-9991},
  month      = feb,
  pages      = {109951},
  volume     = {426},
  doi        = {10.1016/j.jcp.2020.109951},
  groups     = {Applications},
  keywords   = {PINNs, Turbulence, Velocity-pressure formulation, Vorticity-velocity formulation, Ill-posed problems, Transfer learning},
  language   = {en},
  shorttitle = {{NSFnets} ({Navier}-{Stokes} flow nets)},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999120307257},
}




@Article{Sun2020_SurrogateModelingFluid_GaoSGPW,
  author   = {Sun, Luning and Gao, Han and Pan, Shaowu and Wang, Jian-Xun},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data},
  year     = {2020},
  issn     = {0045-7825},
  month    = apr,
  pages    = {112732},
  volume   = {361},
  doi      = {10.1016/j.cma.2019.112732},
  groups   = {PaperWithCode},
  keywords = {Physics-informed machine learning, Label-free, Neural networks, Uncertainty quantification, Cardiovascular flows, Navier-Stokes},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S004578251930622X},
}





@Article{Art2021_ActiveTrainingPhysics_KinAK,
  author   = {Arthurs, Christopher J. and King, Andrew P.},
  journal  = {Journal of Computational Physics},
  title    = {Active training of physics-informed neural networks to aggregate and interpolate parametric solutions to the {Navier}-{Stokes} equations},
  year     = {2021},
  issn     = {0021-9991},
  month    = aug,
  pages    = {110364},
  volume   = {438},
  doi      = {10.1016/j.jcp.2021.110364},
  groups   = {PaperWithCode},
  keywords = {Navier-Stokes, Deep learning, Active learning, Training, Computing methods},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S002199912100259X},
}


@Article{Xia2020_FlowsOverPeriodic_WuXWLD,
  author     = {Xiao, Heng and Wu, Jin-Long and Laizet, Sylvain and Duan, Lian},
  journal    = {Computers \& Fluids},
  title      = {Flows over periodic hills of parameterized geometries: {A} dataset for data-driven turbulence modeling from direct simulations},
  year       = {2020},
  issn       = {0045-7930},
  month      = mar,
  pages      = {104431},
  volume     = {200},
  doi        = {10.1016/j.compfluid.2020.104431},
  groups     = {PaperWithCode},
  keywords   = {Physics-informed machine learning, Turbulence modeling, Separated flows},
  language   = {en},
  shorttitle = {Flows over periodic hills of parameterized geometries},
  url        = {https://www.sciencedirect.com/science/article/pii/S0045793020300074},
}


%%%%%%%%%%%%%%%%%%%
%ODE  - START
%%%%%%%%%%%%%%%%%



@Article{Lai2021_StructuralIdentificationPhysics_MylLMNC,
  author   = {Zhilu Lai and Charilaos Mylonas and Satish Nagarajaiah and Eleni Chatzi},
  journal  = {Journal of Sound and Vibration},
  title    = {Structural identification with physics-informed neural ordinary differential equations},
  year     = {2021},
  issn     = {0022-460X},
  pages    = {116196},
  volume   = {508},
  doi      = {https://doi.org/10.1016/j.jsv.2021.116196},
  groups   = {nxt},
  keywords = {Neural ordinary differential equations, Physics-informed machine learning, Scientific machine learning, Discrepancy modeling, Structural identification, Structural damage detection, Structural health monitoring},
  url      = {https://www.sciencedirect.com/science/article/pii/S0022460X21002686},
}


@Article{Nas2020_TutorialSolvingOrdinary_FriNFV,
  author   = {Renato G. Nascimento and Kajetan Fricke and Felipe A.C. Viana},
  journal  = {Engineering Applications of Artificial Intelligence},
  title    = {A tutorial on solving ordinary differential equations using Python and hybrid physics-informed neural network},
  year     = {2020},
  issn     = {0952-1976},
  pages    = {103996},
  volume   = {96},
  doi      = {https://doi.org/10.1016/j.engappai.2020.103996},
  groups   = {PaperWithCode},
  keywords = {Physics-informed neural network, Scientific machine learning, Uncertainty quantification, Hybrid model python implementation},
  url      = {https://www.sciencedirect.com/science/article/pii/S095219762030292X},
}



@Article{Ton2021_SymplecticNeuralNetworks_XioTXH,
  author   = {Yunjin Tong and Shiying Xiong and Xingzhe He and Guanghan Pan and Bo Zhu},
  journal  = {Journal of Computational Physics},
  title    = {Symplectic neural networks in Taylor series form for Hamiltonian systems},
  year     = {2021},
  issn     = {0021-9991},
  pages    = {110325},
  volume   = {437},
  doi      = {https://doi.org/10.1016/j.jcp.2021.110325},
  groups   = {PaperWithCode},
  keywords = {Machine learning, Hamiltonian system, Physics-informed neural network, Taylor series expansion},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999121002205},
}


%% NO actauly a PINN 





@Article{Hu2021_PhysicsGuidedDeep_HuHHVZ,
  author   = {Hu, Xinyue and Hu, Haoji and Verma, Saurabh and Zhang, Zhi-Li},
  journal  = {IEEE Transactions on Power Systems},
  title    = {Physics-{Guided} {Deep} {Neural} {Networks} for {Power} {Flow} {Analysis}},
  year     = {2021},
  issn     = {1558-0679},
  month    = may,
  number   = {3},
  pages    = {2082--2092},
  volume   = {36},
  doi      = {10.1109/TPWRS.2020.3029557},
  groups   = {Code_nPINN},
  keywords = {Mathematical model, Neural networks, Power systems, Numerical models, Physics, Knowledge engineering, Computational modeling, Power flow analysis, data-driven analysis, neural networks, physics-guided learning, power flow solver},
}


%%%%%%%%%%%%%%%%%%%%%%
% DL
%%%%%%%%%%%%%%%%%%%%%


@InBook{Cal2020_ConvolutionalNetworks_Cal,
  author    = {Calin, Ovidiu},
  pages     = {517--542},
  publisher = {Springer International Publishing},
  title     = {Convolutional Networks},
  year      = {2020},
  address   = {Cham},
  isbn      = {978-3-030-36721-3},
  abstract  = {Convolutional neural networks (CNN) are feedforward neural networks with shared weights and sparse interactions, that is, most weights are equal to zero. Given their fewer number of parameters, convolution networks are more efficient to train than any similarly sized fully-connected layer networks, with only minor negative consequences on their performance.},
  booktitle = {Deep Learning Architectures: A Mathematical Approach},
  doi       = {10.1007/978-3-030-36721-3_16},
  groups    = {DeepLearning},
  url       = {https://doi.org/10.1007/978-3-030-36721-3_16},
}


@InCollection{Cat2018_SpecificNetworkDescriptions_ChaCC,
  author    = {Caterini, Anthony L. and Chang, Dong Eui},
  publisher = {Springer International Publishing},
  title     = {Specific {Network} {Descriptions}},
  booktitle = {Deep Neural Networks in a Mathematical Framework},
  year      = {2018},
  address   = {Cham},
  editor    = {Caterini, Anthony L. and Chang, Dong Eui},
  isbn      = {9783319753041},
  pages     = {35--58},
  series    = {{SpringerBriefs} in {Computer} {Science}},
  doi       = {10.1007/978-3-319-75304-1_4},
  groups    = {DeepLearning},
  language  = {en},
  url       = {https://doi.org/10.1007/978-3-319-75304-1_4},
}

@InCollection{Cat2018_GenericRepresentationNeural_ChaCC,
  author    = {Caterini, Anthony L. and Chang, Dong Eui},
  publisher = {Springer International Publishing},
  title     = {Generic {Representation} of {Neural} {Networks}},
  booktitle = {Deep Neural Networks in a Mathematical Framework},
  year      = {2018},
  address   = {Cham},
  editor    = {Caterini, Anthony L. and Chang, Dong Eui},
  isbn      = {9783319753041},
  pages     = {23--34},
  series    = {{SpringerBriefs} in {Computer} {Science}},
  doi       = {10.1007/978-3-319-75304-1_3},
  groups    = {DeepLearning},
  language  = {en},
  url       = {https://doi.org/10.1007/978-3-319-75304-1_3},
}

@book{caterini2018deep,
  title={Deep Neural Networks in a Mathematical Framework},
  author={Caterini, A.L. and Chang, D.E.},
  isbn={9783319753041},
  series={SpringerBriefs in Computer Science},
  url={https://books.google.it/books?id=aM5SDwAAQBAJ},
  year={2018},
  publisher={Springer International Publishing}
}



@Article{Ber2019_SurveyDeepLearning_BucBBCC,
  author         = {Berman, Daniel S. and Buczak, Anna L. and Chavis, Jeffrey S. and Corbett, Cherita L.},
  journal        = {Information},
  title          = {A Survey of Deep Learning Methods for Cyber Security},
  year           = {2019},
  issn           = {2078-2489},
  number         = {4},
  volume         = {10},
  abstract       = {This survey paper describes a literature review of deep learning (DL) methods for cyber security applications. A short tutorial-style description of each DL method is provided, including deep autoencoders, restricted Boltzmann machines, recurrent neural networks, generative adversarial networks, and several others. Then we discuss how each of the DL methods is used for security applications. We cover a broad array of attack types including malware, spam, insider threats, network intrusions, false data injection, and malicious domain names used by botnets.},
  article-number = {122},
  doi            = {10.3390/info10040122},
  groups         = {DeepLearning},
  url            = {https://www.mdpi.com/2078-2489/10/4/122},
}


@Article{Alo2019_StateArtSurvey_TahATY,
  author         = {Alom, Md Zahangir and Taha, Tarek M. and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Hasan, Mahmudul and Van Essen, Brian C. and Awwal, Abdul A. S. and Asari, Vijayan K.},
  journal        = {Electronics},
  title          = {A State-of-the-Art Survey on Deep Learning Theory and Architectures},
  year           = {2019},
  issn           = {2079-9292},
  number         = {3},
  volume         = {8},
  abstract       = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.},
  article-number = {292},
  doi            = {10.3390/electronics8030292},
  groups         = {DeepLearning},
  url            = {https://www.mdpi.com/2079-9292/8/3/292},
}

@Article{Muh2021_DeepLearningApplication_AseMAC,
  author   = {Muhammad, Amina N. and Aseere, Ali M. and Chiroma, Haruna and Shah, Habib and Gital, Abdulsalam Y. and Hashem, Ibrahim Abaker Targio},
  journal  = {Neural Computing and Applications},
  title    = {Deep learning application in smart cities: recent development, taxonomy, challenges and research prospects},
  year     = {2021},
  issn     = {1433-3058},
  number   = {7},
  pages    = {2973--3009},
  volume   = {33},
  abstract = {The purpose of smart city is to enhance the optimal utilization of scarce resources and improve the resident’s quality of live. The smart cities employed Internet of Things (IoT) to create a sustainable urban life. The IoT devices such as sensors, actuators, and smartphones in the smart cities generate data. The data generated from the smart cities are subjected to analytics to gain insight and discover new knowledge for improving the efficiency and effectiveness of the smart cities. Recently, the application of deep learning in smart cities has gained a tremendous attention from the research community. However, despite raise in popularity and achievements made by deep learning in solving problems in smart cities, no survey has been dedicated mainly on the application of deep learning in smart cities to show recent progress and direction for future development. To bridge this gap, this paper proposes to conduct a dedicated survey on the applications of deep learning in smart cities. In this paper, recent progress, new taxonomies, challenges and opportunities for future research opportunities on the application of deep learning in smart cities have been unveiled. The paper can provide opportunities for experts in the research community to propose a novel approach for developing the research area. On the other hand, new researchers interested in the research area can use the paper as an entry point.},
  doi      = {10.1007/s00521-020-05151-8},
  groups   = {DeepLearning},
  refid    = {Muhammad2021},
  url      = {https://doi.org/10.1007/s00521-020-05151-8},
}

@Article{Che2018_RiseDeepLearning_EngCEW,
  author   = {Hongming Chen and Ola Engkvist and Yinhai Wang and Marcus Olivecrona and Thomas Blaschke},
  journal  = {Drug Discovery Today},
  title    = {The rise of deep learning in drug discovery},
  year     = {2018},
  issn     = {1359-6446},
  number   = {6},
  pages    = {1241-1250},
  volume   = {23},
  abstract = {Over the past decade, deep learning has achieved remarkable success in various artificial intelligence research areas. Evolved from the previous research on artificial neural networks, this technology has shown superior performance to other machine learning algorithms in areas such as image and voice recognition, natural language processing, among others. The first wave of applications of deep learning in pharmaceutical research has emerged in recent years, and its utility has gone beyond bioactivity predictions and has shown promise in addressing diverse problems in drug discovery. Examples will be discussed covering bioactivity prediction, de novo molecular design, synthesis prediction and biological image analysis.},
  doi      = {https://doi.org/10.1016/j.drudis.2018.01.039},
  groups   = {DeepLearning},
  url      = {https://www.sciencedirect.com/science/article/pii/S1359644617303598},
}

@Article{Vin2019_DeepLearningApproach_AlaVAS,
  author  = {Vinayakumar, R. and Alazab, Mamoun and Soman, K. P. and Poornachandran, Prabaharan and Al-Nemrat, Ameer and Venkatraman, Sitalakshmi},
  journal = {IEEE Access},
  title   = {Deep Learning Approach for Intelligent Intrusion Detection System},
  year    = {2019},
  pages   = {41525-41550},
  volume  = {7},
  doi     = {10.1109/ACCESS.2019.2895334},
  groups  = {DeepLearning},
}

@Article{LeC2015_DeepLearning_BenLBH,
  author   = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal  = {Nature},
  title    = {Deep learning},
  year     = {2015},
  issn     = {1476-4687},
  number   = {7553},
  pages    = {436--444},
  volume   = {521},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  doi      = {10.1038/nature14539},
  groups   = {DeepLearning},
  refid    = {LeCun2015},
  url      = {https://doi.org/10.1038/nature14539},
}

@Book{Goodfellow-et-al-2016,
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  title     = {Deep Learning},
  year      = {2016},
  note      = {\url{http://www.deeplearningbook.org}},
  groups    = {DeepLearning},
}

@Article{Sen2020_ReviewDeepLearning_BasSBS,
  author   = {Saptarshi Sengupta and Sanchita Basak and Pallabi Saikia and Sayak Paul and Vasilios Tsalavoutis and Frederick Atiah and Vadlamani Ravi and Alan Peters},
  journal  = {Knowledge-Based Systems},
  title    = {A review of deep learning with special emphasis on architectures, applications and recent trends},
  year     = {2020},
  issn     = {0950-7051},
  pages    = {105596},
  volume   = {194},
  abstract = {Deep learning (DL) has solved a problem that a few years ago was thought to be intractable — the automatic recognition of patterns in spatial and temporal data with an accuracy superior to that of humans. It has solved problems beyond the realm of traditional, hand-crafted machine learning algorithms and captured the imagination of practitioners who are inundated with all types of data. As public awareness of the efficacy of DL increases so does the desire to make use of it. But even for highly trained professionals it can be daunting to approach the rapidly increasing body of knowledge in the field. Where does one start? How does one determine if a particular DL model is applicable to their problem? How does one train and deploy them? With these questions in mind, we present an overview of some of the key DL architectures. We also discuss some new automatic architecture optimization protocols that use multi-agent approaches. Further, since guaranteeing system uptime is critical to many applications, a section dwells on using DL for fault detection and mitigation. This is followed by an exploratory survey of several areas where DL emerged as a game-changer: fraud detection in financial applications, financial time-series forecasting, predictive and prescriptive analytics, medical image processing, power systems research and recommender systems. The thrust of this review is to outline emerging applications of DL and provide a reference to researchers seeking to use DL in their work for pattern recognition with unparalleled learning capacity and the ability to scale with data.},
  doi      = {https://doi.org/10.1016/j.knosys.2020.105596},
  groups   = {DeepLearning},
  keywords = {Deep neural network architectures, Supervised learning, Unsupervised learning, Testing neural networks, Applications of deep learning, Evolutionary computation},
  url      = {https://www.sciencedirect.com/science/article/pii/S095070512030071X},
}




@Article{Ald2020_DeepLearningApproaches_DerADE,
  author   = {Arwa Aldweesh and Abdelouahid Derhab and Ahmed Z. Emam},
  journal  = {Knowledge-Based Systems},
  title    = {Deep learning approaches for anomaly-based intrusion detection systems: A survey, taxonomy, and open issues},
  year     = {2020},
  issn     = {0950-7051},
  pages    = {105124},
  volume   = {189},
  abstract = {The massive growth of data that are transmitted through a variety of devices and communication protocols have raised serious security concerns, which have increased the importance of developing advanced intrusion detection systems (IDSs). Deep learning is an advanced branch of machine learning, composed of multiple layers of neurons that represent the learning process. Deep learning can cope with large-scale data and has shown success in different fields. Therefore, researchers have paid more attention to investigating deep learning for intrusion detection. This survey comprehensively reviews and compares the key previous deep learning-focused cybersecurity surveys. Through an extensive review, this survey provides a novel fine-grained taxonomy that categorizes the current state-of-the-art deep learning-based IDSs with respect to different facets, including input data, detection, deployment, and evaluation strategies. Each facet is further classified according to different criteria. This survey also compares and discusses the related experimental solutions proposed as deep learning-based IDSs. By analysing the experimental studies, this survey discusses the role of deep learning in intrusion detection, the impact of intrusion detection datasets, and the efficiency and effectiveness of the proposed approaches. The findings demonstrate that further effort is required to improve the current state-of-the art. Finally, open research challenges are identified, and future research directions for deep learning-based IDSs are recommended.},
  doi      = {https://doi.org/10.1016/j.knosys.2019.105124},
  groups   = {DeepLearning},
  keywords = {Intrusion detection, Anomaly detection, Deep learning},
  url      = {https://www.sciencedirect.com/science/article/pii/S0950705119304897},
}








@Article{Aru2017_DeepReinforcementLearning_DeiADBB,
  author     = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal    = {IEEE Signal Processing Magazine},
  title      = {Deep {Reinforcement} {Learning}: {A} {Brief} {Survey}},
  year       = {2017},
  issn       = {1558-0792},
  month      = nov,
  number     = {6},
  pages      = {26--38},
  volume     = {34},
  doi        = {10.1109/MSP.2017.2743240},
  groups     = {DeepLearning},
  keywords   = {Artificial intelligence, Signal processing algorithms, Visualization, Machine learning, Learning (artificial intelligence), Neural networks},
  shorttitle = {Deep {Reinforcement} {Learning}},
}



@Article{Shr2019_ReviewDeepLearning_MahSM,
  author   = {Shrestha, Ajay and Mahmood, Ausif},
  journal  = {IEEE Access},
  title    = {Review of {Deep} {Learning} {Algorithms} and {Architectures}},
  year     = {2019},
  issn     = {2169-3536},
  pages    = {53040--53065},
  volume   = {7},
  doi      = {10.1109/ACCESS.2019.2912200},
  groups   = {DeepLearning},
  keywords = {Deep learning, Training, Computer architecture, Feature extraction, Recurrent neural networks, Feedforward neural networks, Machine learning algorithm, optimization, artificial intelligence, deep neural network architectures, convolution neural network, backpropagation, supervised and unsupervised learning},
}








@Book{hinze2008optimization,
  author    = {Hinze, Michael and Pinnau, Ren{\'e} and Ulbrich, Michael and Ulbrich, Stefan},
  publisher = {Springer Science \& Business Media},
  title     = {Optimization with PDE constraints},
  year      = {2008},
  volume    = {23},
  groups    = {Discussion},
}

%%%%%%%%%%%%%%%%%%%%%%
% hIST
%%%%%%%%%%%%%%%%%


@article{bellman1966dynamic,
  title={Dynamic programming},
  author={Bellman, Richard},
  journal={Science},
  volume={153},
  number={3731},
  pages={34--37},
  year={1966},
  publisher={American Association for the Advancement of Science}
}


@article{Stein1987LargeSP,
  title={Large sample properties of simulations using latin hypercube sampling},
  author={Michael L. Stein},
  journal={Technometrics},
  year={1987},
  volume={29},
  pages={143-151}
}

@Article{Hua2011_ExtremeLearningMachines_WanHWL,
  author     = {Huang, Guang-Bin and Wang, Dian Hui and Lan, Yuan},
  journal    = {International Journal of Machine Learning and Cybernetics},
  title      = {Extreme learning machines: a survey},
  year       = {2011},
  issn       = {1868-808X},
  month      = jun,
  number     = {2},
  pages      = {107--122},
  volume     = {2},
  doi        = {10.1007/s13042-011-0019-y},
  groups     = {hist},
  language   = {en},
  shorttitle = {Extreme learning machines},
  url        = {https://doi.org/10.1007/s13042-011-0019-y},
  urldate    = {2022-01-10},
}



@book{Driscoll2014,
	Author = {Driscoll, T. A and Hale, N. and Trefethen, L. N.},
	Publisher = {Pafnuty Publications},
	Title = {Chebfun Guide},
	Url = {http://www.chebfun.org/docs/guide/},
	Year = {2014},
	Bdsk-Url-1 = {http://www.chebfun.org/docs/guide/}}
	
	
@Article{Rai2018_DeepHiddenPhysics_Rai,
  author  = {Maziar Raissi},
  journal = {Journal of Machine Learning Research},
  title   = {Deep Hidden Physics Models: Deep Learning of Nonlinear Partial Differential Equations},
  year    = {2018},
  number  = {25},
  pages   = {1-24},
  volume  = {19},
  groups  = {Main},
  url     = {http://jmlr.org/papers/v19/18-046.html},
}



@Article{Owh2019_KernelFlowsLearning_YooOY,
  author     = {Owhadi, Houman and Yoo, Gene Ryan},
  journal    = {Journal of Computational Physics},
  title      = {Kernel {Flows}: {From} learning kernels from data into the abyss},
  year       = {2019},
  issn       = {0021-9991},
  month      = jul,
  pages      = {22--47},
  volume     = {389},
  doi        = {10.1016/j.jcp.2019.03.040},
  groups     = {Main},
  keywords   = {Learning Kernels, Kriging, Deep learning, Data driven dynamical system, Support vector machine, Reproducing Kernel Hilbert space},
  language   = {en},
  shorttitle = {Kernel {Flows}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999119302232},
}


@Article{Psi1992_HybridNeuralNetwork_UngPU,
  author   = {Psichogios, Dimitris C. and Ungar, Lyle H.},
  journal  = {AIChE Journal},
  title    = {A hybrid neural network-first principles approach to process modeling},
  year     = {1992},
  issn     = {1547-5905},
  number   = {10},
  pages    = {1499--1511},
  volume   = {38},
  doi      = {10.1002/aic.690381003},
  groups   = {hist},
  language = {en},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aic.690381003},
}



@Article{Lag1998_ArtificialNeuralNetworks_LikLLF,
  author   = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
  journal  = {IEEE Transactions on Neural Networks},
  title    = {Artificial neural networks for solving ordinary and partial differential equations},
  year     = {1998},
  issn     = {1941-0093},
  month    = sep,
  number   = {5},
  pages    = {987--1000},
  volume   = {9},
  doi      = {10.1109/72.712178},
  groups   = {hist},
  keywords = {Artificial neural networks, Differential equations, Boundary conditions, Partial differential equations, Boundary value problems, Neural networks, Feedforward neural networks, Moment methods, Finite element methods, Digital signal processors},
}


@Article{Lee1990_NeuralAlgorithmSolving_KanLK,
  author   = {Lee, Hyuk and Kang, In Seok},
  journal  = {Journal of Computational Physics},
  title    = {Neural algorithm for solving differential equations},
  year     = {1990},
  issn     = {0021-9991},
  month    = nov,
  number   = {1},
  pages    = {110--131},
  volume   = {91},
  doi      = {10.1016/0021-9991(90)90007-N},
  groups   = {hist},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/002199919090007N},
}




@InProceedings{Kon2018_GeneralizationEquivarianceConvolution_TriKT,
  author    = {Kondor, Risi and Trivedi, Shubhendu},
  title     = {On the {Generalization} of {Equivariance} and {Convolution} in {Neural} {Networks} to the {Action} of {Compact} {Groups}},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  year      = {2018},
  month     = jul,
  pages     = {2747--2755},
  series = 	 {Proceedings of Machine Learning Research},
  volume = 	 {80},
  publisher = {PMLR},
  groups    = {hist},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v80/kondor18a.html},
}




@Article{Mal2016_UnderstandingDeepConvolutional_Mal,
  author    = {Mallat, Stéphane},
  journal   = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title     = {Understanding deep convolutional networks},
  year      = {2016},
  month     = apr,
  number    = {2065},
  pages     = {20150203},
  volume    = {374},
  abstract  = {Deep convolutional networks provide state-of-the-art classifications and regressions results over many high-dimensional problems. We review their architecture, which scatters data with a cascade of linear filter weights and nonlinearities. A mathematical framework is introduced to analyse their properties. Computations of invariants involve multiscale contractions with wavelets, the linearization of hierarchical symmetries and sparse separations. Applications are discussed.},
  doi       = {10.1098/rsta.2015.0203},
  groups    = {hist},
  keywords  = {deep convolutional neural networks, wavelets, learning},
  publisher = {Royal Society},
  url       = {https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0203},
}










@Article{Rai2017_PhysicsInformedDeep1_PerRPK,
  author     = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal    = {arXiv:1711.10561 [cs, math, stat]},
  title      = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
  year       = {2017},
  month      = nov,
  note       = {arXiv: 1711.10561},
  groups     = {hist},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Statistics - Machine Learning},
  shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
  url        = {http://arxiv.org/abs/1711.10561},
  urldate    = {2021-11-30},
}

@Article{Rai2017_PhysicsInformedDeep2_PerRPK,
  author     = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal    = {arXiv:1711.10566 [cs, math, stat]},
  title      = {Physics {Informed} {Deep} {Learning} ({Part} {II}): {Data}-driven {Discovery} of {Nonlinear} {Partial} {Differential} {Equations}},
  year       = {2017},
  month      = nov,
  note       = {arXiv: 1711.10566},
  groups     = {hist},
  keywords   = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Analysis of PDEs, Mathematics - Numerical Analysis, Statistics - Machine Learning},
  shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {II})},
  url        = {http://arxiv.org/abs/1711.10566},
  urldate    = {2021-11-30},
}



@Article{Owh2015_BayesianNumericalHomogenization_Owh,
  author  = {Owhadi, Houman},
  journal = {Multiscale Modeling \& Simulation},
  title   = {Bayesian Numerical Homogenization},
  year    = {2015},
  number  = {3},
  pages   = {812-828},
  volume  = {13},
  doi     = {10.1137/140974596},
  eprint  = {https://doi.org/10.1137/140974596},
  groups  = {hist},
  url     = {https://doi.org/10.1137/140974596},
}

@Article{Rai2017_InferringSolutionsDifferential_PerRPK,
  author   = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal  = {Journal of Computational Physics},
  title    = {Inferring solutions of differential equations using noisy multi-fidelity data},
  year     = {2017},
  issn     = {0021-9991},
  month    = apr,
  pages    = {736--746},
  volume   = {335},
  doi      = {10.1016/j.jcp.2017.01.060},
  groups   = {hist},
  keywords = {Machine learning, Integro-differential equations, Multi-fidelity modeling, Uncertainty quantification},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999117300761},
}

@Article{Rai2017_MachineLearningLinear_PerRPK,
  author   = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal  = {Journal of Computational Physics},
  title    = {Machine learning of linear differential equations using {Gaussian} processes},
  year     = {2017},
  issn     = {0021-9991},
  month    = nov,
  pages    = {683--693},
  volume   = {348},
  doi      = {10.1016/j.jcp.2017.07.050},
  groups   = {hist},
  keywords = {Probabilistic machine learning, Inverse problems, Fractional differential equations, Uncertainty quantification, Functional genomics},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999117305582},
}

@Article{Rai2018_HiddenPhysicsModels_KarRK,
  author     = {Raissi, Maziar and Karniadakis, George Em},
  journal    = {Journal of Computational Physics},
  title      = {Hidden physics models: {Machine} learning of nonlinear partial differential equations},
  year       = {2018},
  issn       = {0021-9991},
  month      = mar,
  pages      = {125--141},
  volume     = {357},
  doi        = {10.1016/j.jcp.2017.11.039},
  groups     = {hist},
  keywords   = {Probabilistic machine learning, System identification, Bayesian modeling, Uncertainty quantification, Fractional equations, Small data},
  language   = {en},
  shorttitle = {Hidden physics models},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999117309014},
}

@Article{Rai2018_NumericalGaussianProcesses_PerRPK,
  author  = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  journal = {SIAM Journal on Scientific Computing},
  title   = {Numerical Gaussian Processes for Time-Dependent and Nonlinear Partial Differential Equations},
  year    = {2018},
  number  = {1},
  pages   = {A172-A198},
  volume  = {40},
  doi     = {10.1137/17M1120762},
  eprint  = {https://doi.org/10.1137/17M1120762},
  groups  = {hist},
}

@Article{Rai2019_PhysicsInformedNeural_PerRPK,
  author     = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
  journal    = {Journal of Computational Physics},
  title      = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
  year       = {2019},
  issn       = {0021-9991},
  month      = feb,
  pages      = {686--707},
  volume     = {378},
  doi        = {10.1016/j.jcp.2018.10.045},
  groups     = {hist},
  keywords   = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
  language   = {en},
  shorttitle = {Physics-informed neural networks},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
}




%% PINN review bibliography

@Article{Kim2021_KnowledgeIntegrationDeep_KimKKLL,
  author     = {Kim, Sung Wook and Kim, Iljeok and Lee, Jonghwan and Lee, Seungchul},
  journal    = {Journal of Mechanical Science and Technology},
  title      = {Knowledge {Integration} into deep learning in dynamical systems: an overview and taxonomy},
  year       = {2021},
  issn       = {1976-3824},
  month      = apr,
  number     = {4},
  pages      = {1331--1342},
  volume     = {35},
  doi        = {10.1007/s12206-021-0342-5},
  groups     = {hist},
  language   = {en},
  shorttitle = {Knowledge {Integration} into deep learning in dynamical systems},
  url        = {https://doi.org/10.1007/s12206-021-0342-5},
}








%Approximation theorem


@Article{Hor1989_MultilayerFeedforwardNetworks_StiHSW,
  author   = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal  = {Neural Networks},
  title    = {Multilayer feedforward networks are universal approximators},
  year     = {1989},
  issn     = {0893-6080},
  month    = jan,
  number   = {5},
  pages    = {359--366},
  volume   = {2},
  doi      = {10.1016/0893-6080(89)90020-8},
  groups   = {hist},
  keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
}

@Article{Cyb1989_ApproximationSuperpositionsSigmoidal_Cyb,
  author   = {Cybenko, G.},
  journal  = {Mathematics of Control, Signals and Systems},
  title    = {Approximation by superpositions of a sigmoidal function},
  year     = {1989},
  issn     = {1435-568X},
  month    = dec,
  number   = {4},
  pages    = {303--314},
  volume   = {2},
  doi      = {10.1007/BF02551274},
  file     = {Springer Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2FBF02551274.pdf:application/pdf},
  groups   = {hist},
  language = {en},
  url      = {https://doi.org/10.1007/BF02551274},
}







@Article{Ber2018_UnifiedDeepArtificial_NysBN,
  author   = {Berg, Jens and Nyström, Kaj},
  journal  = {Neurocomputing},
  title    = {A unified deep artificial neural network approach to partial differential equations in complex geometries},
  year     = {2018},
  issn     = {0925-2312},
  month    = nov,
  pages    = {28--41},
  volume   = {317},
  doi      = {10.1016/j.neucom.2018.06.056},
  keywords = {Deep neural networks, Partial differential equations, Advection, Diffusion, Complex geometries},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S092523121830794X},
}








@Article{Rai2020_HiddenFluidMechanics_YazRYK,
  author     = {Raissi, Maziar and Yazdani, Alireza and Karniadakis, George Em},
  journal    = {Science},
  title      = {Hidden fluid mechanics: {Learning} velocity and pressure fields from flow visualizations},
  year       = {2020},
  month      = feb,
  number     = {6481},
  pages      = {1026--1030},
  volume     = {367},
  doi        = {10.1126/science.aaw4741},
  groups     = {hist},
  publisher  = {American Association for the Advancement of Science},
  shorttitle = {Hidden fluid mechanics},
  url        = {https://www.science.org/doi/10.1126/science.aaw4741},
}





@Article{Lu2021_LearningNonlinearOperators_JinLJP,
  author    = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  journal   = {Nature Machine Intelligence},
  title     = {Learning nonlinear operators via {DeepONet} based on the universal approximation theorem of operators},
  year      = {2021},
  issn      = {2522-5839},
  month     = mar,
  number    = {3},
  pages     = {218--229},
  volume    = {3},
  copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
  doi       = {10.1038/s42256-021-00302-5},
  groups    = {Main},
  keywords  = {Applied mathematics, Computational science},
  language  = {en},
  publisher = {Nature Publishing Group},
  url       = {https://www.nature.com/articles/s42256-021-00302-5},
}




@Article{Mao2021_DeepmmnetHypersonicsPredicting_LuMLM,
  author     = {Mao, Zhiping and Lu, Lu and Marxen, Olaf and Zaki, Tamer A. and Karniadakis, George Em},
  journal    = {Journal of Computational Physics},
  title      = {{DeepM}\&{Mnet} for hypersonics: {Predicting} the coupled flow and finite-rate chemistry behind a normal shock using neural-network approximation of operators},
  year       = {2021},
  issn       = {0021-9991},
  month      = dec,
  pages      = {110698},
  volume     = {447},
  doi        = {10.1016/j.jcp.2021.110698},
  groups     = {Main},
  keywords   = {Deep learning, Operator approximation, DeepONet, Hypersonics, Chemically reacting flow, Data assimilation},
  language   = {en},
  shorttitle = {{DeepM}\&{Mnet} for hypersonics},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999121005933},
}



%%%%%%%%%%%%%%%%%%%
%Reviews  - START
%%%%%%%%%%%%%%%%%

@Article{Ble2021_ThreeWaysSolve_ErnBE,
  author   = {Blechschmidt, Jan and Ernst, Oliver G.},
  journal  = {GAMM-Mitteilungen},
  title    = {Three ways to solve partial differential equations with neural networks — {A} review},
  year     = {2021},
  issn     = {1522-2608},
  number   = {2},
  pages    = {e202100006},
  volume   = {44},
  doi      = {10.1002/gamm.202100006},
  groups   = {hist},
  keywords = {backward differential equation, curse of dimensionality, Feynman–Kac, Hamilton–Jacobi–Bellman equations, neural networks, partial differential equation, PINN, stochastic process},
  language = {en},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/gamm.202100006},
}


%%%%%%%%%%%%%%%%%%%%
%NTX
%%%%%%%%%%%%%%%%%%%%%%



@Article{Mao2020_PhysicsInformedNeural_JagMJK,
  author   = {Mao, Zhiping and Jagtap, Ameya D. and Karniadakis, George Em},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Physics-informed neural networks for high-speed flows},
  year     = {2020},
  issn     = {0045-7825},
  month    = mar,
  pages    = {112789},
  volume   = {360},
  doi      = {10.1016/j.cma.2019.112789},
  groups   = {nxt},
  keywords = {Euler equations, Machine learning, Neural networks, Conservation laws, Riemann problem, Hidden fluid mechanics},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782519306814},
}




@Article{He2020_PhysicsInformedNeural_BarHBTT,
  author   = {He, QiZhi and Barajas-Solano, David and Tartakovsky, Guzel and Tartakovsky, Alexandre M.},
  journal  = {Advances in Water Resources},
  title    = {Physics-informed neural networks for multiphysics data assimilation with application to subsurface transport},
  year     = {2020},
  issn     = {0309-1708},
  month    = jul,
  pages    = {103610},
  volume   = {141},
  doi      = {10.1016/j.advwatres.2020.103610},
  groups   = {nxt},
  keywords = {Physics-informed deep neural networks, Data assimilation, Parameter estimation, Inverse problems, Subsurface flow and transport},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0309170819311649},
}


@Article{Zhu2021_MachineLearningMetal_LiuZLY,
  author     = {Zhu, Qiming and Liu, Zeliang and Yan, Jinhui},
  journal    = {Computational Mechanics},
  title      = {Machine learning for metal additive manufacturing: predicting temperature and melt pool fluid dynamics using physics-informed neural networks},
  year       = {2021},
  issn       = {1432-0924},
  month      = feb,
  number     = {2},
  pages      = {619--635},
  volume     = {67},
  doi        = {10.1007/s00466-020-01952-9},
  groups     = {nxt},
  language   = {en},
  shorttitle = {Machine learning for metal additive manufacturing},
  url        = {https://doi.org/10.1007/s00466-020-01952-9},
}


@Article{Yan2019_AdversarialUncertaintyQuantification_PerYP,
  author   = {Yang, Yibo and Perdikaris, Paris},
  journal  = {Journal of Computational Physics},
  title    = {Adversarial uncertainty quantification in physics-informed neural networks},
  year     = {2019},
  issn     = {0021-9991},
  month    = oct,
  pages    = {136--152},
  volume   = {394},
  doi      = {10.1016/j.jcp.2019.05.027},
  groups   = {nxt},
  keywords = {Variational inference, Generative adversarial networks, Probabilistic deep learning, Probabilistic scientific computing, Data-driven modeling},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999119303584},
}


@Article{Gao2021_PhygeonetPhysicsInformed_SunGSW,
  author     = {Gao, Han and Sun, Luning and Wang, Jian-Xun},
  journal    = {Journal of Computational Physics},
  title      = {{PhyGeoNet}: {Physics}-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state {PDEs} on irregular domain},
  year       = {2021},
  issn       = {0021-9991},
  month      = mar,
  pages      = {110079},
  volume     = {428},
  doi        = {10.1016/j.jcp.2020.110079},
  groups     = {nxt},
  keywords   = {Physics-informed neural networks, Label-free, Surrogate modeling, Physics-constrained deep learning, Partial differential equations, Navier-Stokes},
  language   = {en},
  shorttitle = {{PhyGeoNet}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999120308536},
}





@Article{Nab2021_EfficientTrainingPhysics_GlaNGM,
  author   = {Nabian, Mohammad Amin and Gladstone, Rini Jasmine and Meidani, Hadi},
  journal  = {Computer-Aided Civil and Infrastructure Engineering},
  title    = {Efficient training of physics-informed neural networks via importance sampling},
  year     = {2021},
  issn     = {1467-8667},
  number   = {8},
  pages    = {962--977},
  volume   = {36},
  doi      = {10.1111/mice.12685},
  groups   = {nxt},
  language = {en},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12685},
}












%%%%%%%%%%%%%%%%%%%
%Many eqs  - START
%%%%%%%%%%%%%%%%%


@Article{Ram2021_SpinnSparsePhysics_RamRR,
  author     = {Ramabathiran, Amuthan A. and Ramachandran, Prabhu},
  journal    = {Journal of Computational Physics},
  title      = {{SPINN}: {Sparse}, {Physics}-based, and partially {Interpretable} {Neural} {Networks} for {PDEs}},
  year       = {2021},
  issn       = {0021-9991},
  month      = nov,
  pages      = {110600},
  volume     = {445},
  doi        = {10.1016/j.jcp.2021.110600},
  groups     = {PaperWithCode},
  keywords   = {Physics-based neural networks, Sparse neural networks, Interpretable machine learning, Partial differential equations, Meshless methods, Numerical methods for PDEs},
  language   = {en},
  shorttitle = {{SPINN}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999121004952},
}

@Article{Sch2021_ExtremeTheoryFunctional_FurSFL,
  author     = {Schiassi, Enrico and Furfaro, Roberto and Leake, Carl and De Florio, Mario and Johnston, Hunter and Mortari, Daniele},
  journal    = {Neurocomputing},
  title      = {Extreme theory of functional connections: {A} fast physics-informed neural network method for solving ordinary and partial differential equations},
  year       = {2021},
  issn       = {0925-2312},
  month      = oct,
  pages      = {334--356},
  volume     = {457},
  doi        = {10.1016/j.neucom.2021.06.015},
  groups     = {nxt},
  keywords   = {Physics-informed neural networks, Extreme learning machine, Functional interpolation, Numerical methods, Universal approximator, Least-squares},
  language   = {en},
  shorttitle = {Extreme theory of functional connections},
  url        = {https://www.sciencedirect.com/science/article/pii/S0925231221009140},
}








%%%%%%%%%%%%%%%%%%%
%Many eqs  - END
%%%%%%%%%%%%%%%%%











%%%%%%%%%%%%%%%%%%%
%Navier Stokes  - START
%%%%%%%%%%%%%%%%%


@Article{Che2021_DeepLearningMethod_ZhaCZ,
  author    = {Cheng, Chen and Zhang, Guang-Tao},
  journal   = {Water},
  title     = {Deep {Learning} {Method} {Based} on {Physics} {Informed} {Neural} {Network} with {Resnet} {Block} for {Solving} {Fluid} {Flow} {Problems}},
  year      = {2021},
  month     = jan,
  number    = {4},
  pages     = {423},
  volume    = {13},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  doi       = {10.3390/w13040423},
  groups    = {nxt},
  keywords  = {N-S equations, PINN, Resnet, fluid flow, numerical simulation, experimental method},
  language  = {en},
  publisher = {Multidisciplinary Digital Publishing Institute},
  url       = {https://www.mdpi.com/2073-4441/13/4/423},
}



%%%%%%%%%%%%%%%%%%%
%Eikonal - START
%%%%%%%%%%%%%%%%%


@InProceedings{Gra2021_LearningAtrialFiber_PezGPC,
  author    = {Grandits, Thomas and Pezzuto, Simone and Costabal, Francisco Sahli and Perdikaris, Paris and Pock, Thomas and Plank, Gernot and Krause, Rolf},
  booktitle = {Functional {Imaging} and {Modeling} of the {Heart}},
  title     = {Learning {Atrial} {Fiber} {Orientations} and {Conductivity} {Tensors} from {Intracardiac} {Maps} {Using} {Physics}-{Informed} {Neural} {Networks}},
  year      = {2021},
  address   = {Cham},
  editor    = {Ennis, Daniel B. and Perotti, Luigi E. and Wang, Vicky Y.},
  pages     = {650--658},
  publisher = {Springer International Publishing},
  series    = {Lecture {Notes} in {Computer} {Science}},
  doi       = {10.1007/978-3-030-78710-3_62},
  groups    = {nxt},
  isbn      = {9783030787103},
  language  = {en},
}

@Article{Sah2020_PhysicsInformedNeural_YanSCYP,
  author   = {Sahli Costabal, Francisco and Yang, Yibo and Perdikaris, Paris and Hurtado, Daniel E. and Kuhl, Ellen},
  journal  = {Frontiers in Physics},
  title    = {Physics-{Informed} {Neural} {Networks} for {Cardiac} {Activation} {Mapping}},
  year     = {2020},
  issn     = {2296-424X},
  pages    = {42},
  volume   = {8},
  doi      = {10.3389/fphy.2020.00042},
  groups   = {PaperWithCode},
  url      = {https://www.frontiersin.org/article/10.3389/fphy.2020.00042},
  urldate  = {2021-11-18},
}


%%%%%%%%%%%%%%%%%%%
%Fractional - START
%%%%%%%%%%%%%%%%%

@Article{Meh2019_DiscoveringUniversalVariable_PanMPSK,
  author  = {Mehta, Pavan Pranjivan and Pang, Guofei and Song, Fangying and Karniadakis, George Em},
  journal = {Fractional Calculus and Applied Analysis},
  title   = {Discovering a universal variable-order fractional model for turbulent Couette flow using a physics-informed neural network},
  year    = {2019},
  number  = {6},
  pages   = {1675--1688},
  volume  = {22},
  doi     = {10.1515/fca-2019-0086},
  groups  = {nxt},
  url     = {https://doi.org/10.1515/fca-2019-0086},
}


@Article{Kha2021_IdentifiabilityPredictabilityInteger_CaiKCZ,
  author   = {Kharazmi, Ehsan and Cai, Min and Zheng, Xiaoning and Zhang, Zhen and Lin, Guang and Karniadakis, George Em},
  journal  = {Nature Computational Science},
  title    = {Identifiability and predictability of integer- and fractional-order epidemiological models using physics-informed neural networks},
  year     = {2021},
  issn     = {2662-8457},
  number   = {11},
  pages    = {744--753},
  volume   = {1},
  abstract = {We analyze a plurality of epidemiological models through the lens of physics-informed neural networks (PINNs) that enable us to identify time-dependent parameters and data-driven fractional differential operators. In particular, we consider several variations of the classical susceptible-infectious-removed (SIR) model by introducing more compartments and fractional-order and time-delay models. We report the results for the spread of COVID-19 in New York City, Rhode Island and Michigan states and Italy, by simultaneously inferring the unknown parameters and the unobserved dynamics. For integer-order and time-delay models, we fit the available data by identifying time-dependent parameters, which are represented by neural networks. In contrast, for fractional differential models, we fit the data by determining different time-dependent derivative orders for each compartment, which we represent by neural networks. We investigate the structural and practical identifiability of these unknown functions for different datasets, and quantify the uncertainty associated with neural networks and with control measures in forecasting the pandemic.},
  doi      = {10.1038/s43588-021-00158-0},
  groups   = {nxt},
  refid    = {Kharazmi2021},
  url      = {https://doi.org/10.1038/s43588-021-00158-0},
}



%%%%%%%%%%%%%%%%%%%
%Application - si START
%%%%%%%%%%%%%%%%%

@Article{Arn2021_StateModelingControl_KinAK,
  author   = {Arnold, Florian and King, Rudibert},
  journal  = {Engineering Applications of Artificial Intelligence},
  title    = {State–space modeling for control based on physics-informed neural networks},
  year     = {2021},
  issn     = {0952-1976},
  month    = may,
  pages    = {104195},
  volume   = {101},
  doi      = {10.1016/j.engappai.2021.104195},
  groups   = {Applications},
  keywords = {Machine learning, State–space, Neural networks, Model predictive control, State estimation},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0952197621000427},
}

@Article{Pat2022_ThermodynamicallyConsistentPhysics_ManPMT,
  author   = {Patel, Ravi G. and Manickam, Indu and Trask, Nathaniel A. and Wood, Mitchell A. and Lee, Myoungkyu and Tomas, Ignacio and Cyr, Eric C.},
  journal  = {Journal of Computational Physics},
  title    = {Thermodynamically consistent physics-informed neural networks for hyperbolic systems},
  year     = {2022},
  issn     = {0021-9991},
  month    = jan,
  pages    = {110754},
  volume   = {449},
  doi      = {10.1016/j.jcp.2021.110754},
  groups   = {Applications},
  keywords = {Physics-informed neural networks, Inverse problems, Machine learning, Equation of state, Conservation laws, Shock hydrodynamics},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999121006495},
}





@Article{Min2020_SurrogateModelComputational_TroMNTKNRZ,
  author   = {Minh Nguyen-Thanh, Vien and Trong Khiem Nguyen, Lu and Rabczuk, Timon and Zhuang, Xiaoying},
  journal  = {International Journal for Numerical Methods in Engineering},
  title    = {A surrogate model for computational homogenization of elastostatics at finite strain using high-dimensional model representation-based neural network},
  year     = {2020},
  issn     = {1097-0207},
  number   = {21},
  pages    = {4811--4842},
  volume   = {121},
  doi      = {10.1002/nme.6493},
  file     = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/nme.6493:application/pdf},
  groups   = {Applications},
  keywords = {computational homogenization, data-driven, FFT-based methods, nonlinear elasticity},
  language = {en},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.6493},
}



@Article{Cav2021_PhysicsInformedDeep_MosCMS,
  author   = {Cavanagh, Henry and Mosbach, Andreas and Scalliet, Gabriel and Lind, Rob and Endres, Robert G.},
  journal  = {Nature Communications},
  title    = {Physics-informed deep learning characterizes morphodynamics of Asian soybean rust disease},
  year     = {2021},
  issn     = {2041-1723},
  number   = {1},
  pages    = {6424},
  volume   = {12},
  abstract = {Medicines and agricultural biocides are often discovered using large phenotypic screens across hundreds of compounds, where visible effects of whole organisms are compared to gauge efficacy and possible modes of action. However, such analysis is often limited to human-defined and static features. Here, we introduce a novel framework that can characterize shape changes (morphodynamics) for cell-drug interactions directly from images, and use it to interpret perturbed development of Phakopsora pachyrhizi, the Asian soybean rust crop pathogen. We describe population development over a 2D space of shapes (morphospace) using two models with condition-dependent parameters: a top-down Fokker-Planck model of diffusive development over Waddington-type landscapes, and a bottom-up model of tip growth. We discover a variety of landscapes, describing phenotype transitions during growth, and identify possible perturbations in the tip growth machinery that cause this variation. This demonstrates a widely-applicable integration of unsupervised learning and biophysical modeling.},
  doi      = {10.1038/s41467-021-26577-1},
  groups   = {PaperWithCode},
  refid    = {Cavanagh2021},
  url      = {https://doi.org/10.1038/s41467-021-26577-1},
}



@InProceedings{Sti2021_PhysicsInformedNeural_MisSMC,
  author    = {Stiasny, Jochen and Misyris, George S. and Chatzivasileiadis, Spyros},
  booktitle = {2021 {IEEE} {Madrid} {PowerTech}},
  title     = {Physics-{Informed} {Neural} {Networks} for {Non}-linear {System} {Identification} for {Power} {System} {Dynamics}},
  year      = {2021},
  month     = jun,
  pages     = {1--6},
  doi       = {10.1109/PowerTech46648.2021.9495063},
  groups    = {Applications},
  keywords  = {Training, Uncertainty, Power system dynamics, Neural networks, Tools, Phasor measurement units, System identification, Physics-informed neural networks, system identification, state estimation, swing equation},
}



@Article{Sti2021_ReconstructionNanoscaleParticles_SchSS,
  author    = {Stielow, Thomas and Scheel, Stefan},
  journal   = {Physical Review E},
  title     = {Reconstruction of nanoscale particles from single-shot wide-angle free-electron-laser diffraction patterns with physics-informed neural networks},
  year      = {2021},
  month     = may,
  number    = {5},
  pages     = {053312},
  volume    = {103},
  doi       = {10.1103/PhysRevE.103.053312},
  file      = {Full Text PDF:https\://journals.aps.org/pre/pdf/10.1103/PhysRevE.103.053312:application/pdf},
  groups    = {PaperWithCode},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevE.103.053312},
}





@InProceedings{Abr2021_StudyFeedforwardNeural_FloAF,
  author    = {Abreu, Eduardo and Florindo, Joao B.},
  booktitle = {Computational {Science} – {ICCS} 2021},
  title     = {A {Study} on a {Feedforward} {Neural} {Network} to {Solve} {Partial} {Differential} {Equations} in {Hyperbolic}-{Transport} {Problems}},
  year      = {2021},
  address   = {Cham},
  editor    = {Paszynski, Maciej and Kranzlmüller, Dieter and Krzhizhanovskaya, Valeria V. and Dongarra, Jack J. and Sloot, Peter M. A.},
  pages     = {398--411},
  publisher = {Springer International Publishing},
  series    = {Lecture {Notes} in {Computer} {Science}},
  doi       = {10.1007/978-3-030-77964-1_31},
  groups    = {Applications},
  isbn      = {9783030779641},
  keywords  = {Neural networks , Partial differential equation , Transport models , Numerical approximation methods for pdes , Approximation of entropy solutions},
  language  = {en},
}





@Article{Alm2022_PredictionPorousMedia_AbuAA,
  author   = {Almajid, Muhammad M. and Abu-Al-Saud, Moataz O.},
  journal  = {Journal of Petroleum Science and Engineering},
  title    = {Prediction of porous media fluid flow using physics informed neural networks},
  year     = {2022},
  issn     = {0920-4105},
  month    = jan,
  pages    = {109205},
  volume   = {208},
  doi      = {10.1016/j.petrol.2021.109205},
  groups   = {Applications},
  keywords = {Physics-informed ML, PINN, Buckley-Leverett, Fractional flow},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0920410521008597},
}




@Article{Kim2021_DpmNovelTraining_LeeKLL,
  author     = {Kim, Jungeun and Lee, Kookjin and Lee, Dongeun and Jhin, Sheo Yon and Park, Noseong},
  journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title      = {{DPM}: {A} {Novel} {Training} {Method} for {Physics}-{Informed} {Neural} {Networks} in {Extrapolation}},
  year       = {2021},
  issn       = {2374-3468},
  month      = may,
  number     = {9},
  pages      = {8146--8154},
  volume     = {35},
  copyright  = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  groups     = {Applications},
  keywords   = {(Deep) Neural Network Algorithms},
  language   = {en},
  shorttitle = {{DPM}},
  url        = {https://ojs.aaai.org/index.php/AAAI/article/view/16992},
}






@Article{Smi2021_HyposviHypocentreInversion_RosSRAM,
  author     = {Smith, Jonthan D and Ross, Zachary E and Azizzadenesheli, Kamyar and Muir, Jack B},
  journal    = {Geophysical Journal International},
  title      = {{HypoSVI}: {Hypocentre} inversion with {Stein} variational inference and physics informed neural networks},
  year       = {2021},
  issn       = {0956-540X},
  month      = jan,
  number     = {1},
  pages      = {698--710},
  volume     = {228},
  doi        = {10.1093/gji/ggab309},
  groups     = {PaperWithCode},
  shorttitle = {{HypoSVI}},
  url        = {https://doi.org/10.1093/gji/ggab309},
}




@Article{Smi2021_EikonetSolvingEikonal_AziSAR,
  author     = {Smith, Jonathan D. and Azizzadenesheli, Kamyar and Ross, Zachary E.},
  journal    = {IEEE Transactions on Geoscience and Remote Sensing},
  title      = {{EikoNet}: {Solving} the {Eikonal} {Equation} {With} {Deep} {Neural} {Networks}},
  year       = {2021},
  issn       = {1558-0644},
  month      = dec,
  number     = {12},
  pages      = {10685--10696},
  volume     = {59},
  doi        = {10.1109/TGRS.2020.3039165},
  groups     = {PaperWithCode},
  keywords   = {Mathematical model, Training, Receivers, Neural networks, Computational modeling, Position measurement, Deep learning, Geophysics, partial differential equations (PDEs), ray tracing, travel time},
  shorttitle = {{EikoNet}},
}
























%%%%%%%%%%%%%%%%%%%
%Software  - START
%%%%%%%%%%%%%%%%%



@Article{lu2021deepxde,
  author  = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  journal = {SIAM Review},
  title   = {{DeepXDE}: A deep learning library for solving differential equations},
  year    = {2021},
  number  = {1},
  pages   = {208-228},
  volume  = {63},
  doi     = {10.1137/19M1274067},
  groups  = {Software},
}


@InProceedings{Hen2021_NvidiaSimnetAi_NarHNN,
  author     = {Hennigh, Oliver and Narasimhan, Susheela and Nabian, Mohammad Amin and Subramaniam, Akshay and Tangsali, Kaustubh and Fang, Zhiwei and Rietmann, Max and Byeon, Wonmin and Choudhry, Sanjay},
  booktitle  = {Computational {Science} – {ICCS} 2021},
  title      = {{NVIDIA} {SimNet}™: {An} {AI}-{Accelerated} {Multi}-{Physics} {Simulation} {Framework}},
  year       = {2021},
  address    = {Cham},
  editor     = {Paszynski, Maciej and Kranzlmüller, Dieter and Krzhizhanovskaya, Valeria V. and Dongarra, Jack J. and Sloot, Peter M.A.},
  pages      = {447--461},
  publisher  = {Springer International Publishing},
  series     = {Lecture {Notes} in {Computer} {Science}},
  doi        = {10.1007/978-3-030-77977-1_36},
  groups     = {Software},
  isbn       = {9783030779771},
  language   = {en},
  shorttitle = {{NVIDIA} {SimNet}™},
}


@Article{Hag2021_SciannKerastensorflowWrapper_JuaHJ,
  author     = {Haghighat, Ehsan and Juanes, Ruben},
  journal    = {Computer Methods in Applied Mechanics and Engineering},
  title      = {{SciANN}: {A} {Keras}/{Tensorflow} wrapper for scientific computations and physics-informed deep learning using artificial neural networks},
  year       = {2021},
  issn       = {00457825},
  month      = jan,
  note       = {arXiv: 2005.08803},
  pages      = {113552},
  volume     = {373},
  doi        = {10.1016/j.cma.2020.113552},
  groups     = {Software},
  keywords   = {Computer Science - Other Computer Science, Computer Science - Machine Learning, Computer Science - Mathematical Software, 74S30 (primary), 74S05, 74B05, 74L05, 74L10 (secondary), J.2},
  shorttitle = {{SciANN}},
  url        = {http://arxiv.org/abs/2005.08803},
}



@Article{Ara2021_ElvetNeuralNetwork_CriACS,
  author   = {Araz, Jack Y. and Criado, Juan Carlos and Spannowsky, Michael},
  journal  = {arXiv:2103.14575 [hep-lat, physics:hep-ph, physics:hep-th, stat]},
  title    = {Elvet -- a neural network-based differential equation and variational problem solver},
  year     = {2021},
  month    = mar,
  note     = {arXiv: 2103.14575},
  annote   = {Comment: 24 pages, 2 figures. typo fixed and added link to Google Colab for examples},
  groups   = {Software},
  keywords = {Computer Science - Machine Learning, High Energy Physics - Lattice, High Energy Physics - Phenomenology, High Energy Physics - Theory, Statistics - Machine Learning},
  url      = {http://arxiv.org/abs/2103.14575},
}



@Article{Kor2019_PydensPythonFramework_KhuKKT,
  author     = {Koryagin, Alexander and Khudorozkov, Roman and Tsimfer, Sergey},
  journal    = {arXiv:1909.11544 [cs, stat]},
  title      = {{PyDEns}: a {Python} {Framework} for {Solving} {Differential} {Equations} with {Neural} {Networks}},
  year       = {2019},
  month      = sep,
  note       = {arXiv: 1909.11544},
  groups     = {Software},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  shorttitle = {{PyDEns}},
  url        = {http://arxiv.org/abs/1909.11544},
}



@Article{mcclenny2021tensordiffeq,
  author  = {McClenny, Levi D and Haile, Mulugeta A and Braga-Neto, Ulisses M},
  journal = {arXiv preprint arXiv:2103.16034},
  title   = {TensorDiffEq: Scalable Multi-GPU Forward and Inverse Solvers for Physics Informed Neural Networks},
  year    = {2021},
  groups  = {Software},
}


@article{chen2020neurodiffeq,
  title={NeuroDiffEq: A Python package for solving differential equations with neural networks},
  author={Chen, Feiyu and Sondak, David and Protopapas, Pavlos and Mattheakis, Marios and Liu, Shuheng and Agarwal, Devansh and Di Giovanni, Marco},
  journal={Journal of Open Source Software},
  volume={5},
  number={46},
  pages={1931},
  year={2020}
}



@Article{Rac2021_UniversalDifferentialEquations_MaRMM,
  author   = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
  journal  = {arXiv:2001.04385 [cs, math, q-bio, stat]},
  title    = {Universal {Differential} {Equations} for {Scientific} {Machine} {Learning}},
  year     = {2021},
  month    = nov,
  note     = {arXiv: 2001.04385},
  annote   = {Comment: 5 figures, 2 tables, 11 supplemental figures, 29 pages, 25 supplemental pages},
  groups   = {Software},
  keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning},
  url      = {http://arxiv.org/abs/2001.04385},
}




@Article{Zub2021_NeuralpdeAutomatingPhysics_McCZMM,
  author     = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},
  journal    = {arXiv:2107.09443 [cs]},
  title      = {{NeuralPDE}: {Automating} {Physics}-{Informed} {Neural} {Networks} ({PINNs}) with {Error} {Approximations}},
  year       = {2021},
  month      = jul,
  note       = {arXiv: 2107.09443},
  annote     = {Comment: 74 pages, 20+ figures, 20+ tables},
  groups     = {Software},
  keywords   = {Computer Science - Mathematical Software, Computer Science - Symbolic Computation},
  shorttitle = {{NeuralPDE}},
  url        = {http://arxiv.org/abs/2107.09443},
}





@Article{Pen2021_IdrlnetPhysicsInformed_ZhaPZZ,
  author     = {Peng, Wei and Zhang, Jun and Zhou, Weien and Zhao, Xiaoyu and Yao, Wen and Chen, Xiaoqian},
  journal    = {arXiv:2107.04320 [cs, math]},
  title      = {{IDRLnet}: {A} {Physics}-{Informed} {Neural} {Network} {Library}},
  year       = {2021},
  month      = jul,
  note       = {arXiv: 2107.04320},
  groups     = {Software},
  keywords   = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
  shorttitle = {{IDRLnet}},
  url        = {http://arxiv.org/abs/2107.04320},
}


@inproceedings{gardner2018gpytorch,
  title={GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
  author={Gardner, Jacob R and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}



@Article{Xu2020_AdcmeLearningSpatially_DarXD,
  author     = {Xu, Kailai and Darve, Eric},
  journal    = {arXiv:2011.11955 [cs, math]},
  title      = {{ADCME}: {Learning} {Spatially}-varying {Physical} {Fields} using {Deep} {Neural} {Networks}},
  year       = {2020},
  month      = nov,
  note       = {arXiv: 2011.11955},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2011.11955.pdf:application/pdf},
  groups     = {Software},
  keywords   = {Mathematics - Numerical Analysis},
  shorttitle = {{ADCME}},
  url        = {http://arxiv.org/abs/2011.11955},
}



@Article{Ped2019_SolvingPartialDifferential_MarPMP,
  author   = {Pedro, Juan B. and Maroñas, Juan and Paredes, Roberto},
  journal  = {arXiv:1912.04737 [physics]},
  title    = {Solving {Partial} {Differential} {Equations} with {Neural} {Networks}},
  year     = {2019},
  month    = dec,
  note     = {arXiv: 1912.04737},
  groups   = {Software},
  keywords = {Physics - Computational Physics},
  url      = {http://arxiv.org/abs/1912.04737},
}


@Article{Xu2021_SolvingInverseProblems_DarXD,
  author   = {Xu, Kailai and Darve, Eric},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {Solving inverse problems in stochastic models using deep neural networks and adversarial training},
  year     = {2021},
  issn     = {0045-7825},
  month    = oct,
  pages    = {113976},
  volume   = {384},
  doi      = {10.1016/j.cma.2021.113976},
  groups   = {Software},
  keywords = {Adversarial training, Neural networks, Automatic differentiation},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782521003078},
}





%%%%%%%%%%%%%%%%%%%
%Software Applications  - START
%%%%%%%%%%%%%%%%%

@Article{Hag2021_PhysicsInformedDeep_RaiHRM,
  author   = {Haghighat, Ehsan and Raissi, Maziar and Moure, Adrian and Gomez, Hector and Juanes, Ruben},
  journal  = {Computer Methods in Applied Mechanics and Engineering},
  title    = {A physics-informed deep learning framework for inversion and surrogate modeling in solid mechanics},
  year     = {2021},
  issn     = {0045-7825},
  month    = jun,
  pages    = {113741},
  volume   = {379},
  doi      = {10.1016/j.cma.2021.113741},
  groups   = {Applications},
  keywords = {Artificial neural network, Physics-informed deep learning, Inversion, Transfer learning, Linear elasticity, Elastoplasticity},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0045782521000773},
}


@Article{Jam2021_MachineLearningAccelerating_HagJHI,
  author     = {Jamali, Behzad and Haghighat, Ehsan and Ignjatovic, Aleksandar and Leitão, João P. and Deletic, Ana},
  journal    = {Hydrological Processes},
  title      = {Machine learning for accelerating {2D} flood models: {Potential} and challenges},
  year       = {2021},
  issn       = {1099-1085},
  number     = {4},
  pages      = {e14064},
  volume     = {35},
  doi        = {10.1002/hyp.14064},
  groups     = {Applications},
  keywords   = {artificial neural networks, hybrid numerical schemes, machine learning, rapid flood modelling},
  language   = {en},
  shorttitle = {Machine learning for accelerating {2D} flood models},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hyp.14064},
}




@Article{Hof2021_MeshFreeSurrogate_GeiHGOK,
  author     = {Hoffer, Johannes G. and Geiger, Bernhard C. and Ofner, Patrick and Kern, Roman},
  journal    = {Applied Sciences},
  title      = {Mesh-{Free} {Surrogate} {Models} for {Structural} {Mechanic} {FEM} {Simulation}: {A} {Comparative} {Study} of {Approaches}},
  year       = {2021},
  month      = jan,
  number     = {20},
  pages      = {9411},
  volume     = {11},
  copyright  = {http://creativecommons.org/licenses/by/3.0/},
  doi        = {10.3390/app11209411},
  groups     = {Applications},
  keywords   = {FEM, surrogate modeling, mesh-free, machine learning, deep learning},
  language   = {en},
  publisher  = {Multidisciplinary Digital Publishing Institute},
  shorttitle = {Mesh-{Free} {Surrogate} {Models} for {Structural} {Mechanic} {FEM} {Simulation}},
  url        = {https://www.mdpi.com/2076-3417/11/20/9411},
}




@Article{Lu2020_ExtractionMechanicalProperties_DaoLDK,
  author    = {Lu, Lu and Dao, Ming and Kumar, Punit and Ramamurty, Upadrasta and Karniadakis, George Em and Suresh, Subra},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Extraction of mechanical properties of materials through deep learning from instrumented indentation},
  year      = {2020},
  issn      = {0027-8424},
  number    = {13},
  pages     = {7052--7062},
  volume    = {117},
  doi       = {10.1073/pnas.1922210117},
  eprint    = {https://www.pnas.org/content/117/13/7052.full.pdf},
  groups    = {Applications},
  publisher = {National Academy of Sciences},
  url       = {https://www.pnas.org/content/117/13/7052},
}



@Article{Lin2021_SeamlessMultiscaleOperator_MaxLMLK,
  author    = {Lin, Chensen and Maxey, Martin and Li, Zhen and Karniadakis, George Em},
  journal   = {Journal of Fluid Mechanics},
  title     = {A seamless multiscale operator neural network for inferring bubble dynamics},
  year      = {2021},
  issn      = {0022-1120, 1469-7645},
  month     = dec,
  volume    = {929},
  doi       = {10.1017/jfm.2021.866},
  groups    = {Applications},
  keywords  = {machine learning, computational methods},
  language  = {en},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/seamless-multiscale-operator-neural-network-for-inferring-bubble-dynamics/D516AB0EF954D0FF56AD864DB2618E94},
}



@Article{Lin2021_OperatorLearningPredicting_LiLLL,
  author    = {Lin, Chensen and Li, Zhen and Lu, Lu and Cai, Shengze and Maxey, Martin and Karniadakis, George Em},
  journal   = {The Journal of Chemical Physics},
  title     = {Operator learning for predicting multiscale bubble growth dynamics},
  year      = {2021},
  issn      = {0021-9606},
  month     = mar,
  number    = {10},
  pages     = {104118},
  volume    = {154},
  doi       = {10.1063/5.0041203},
  groups    = {Applications},
  publisher = {American Institute of Physics},
  url       = {https://aip.scitation.org/doi/10.1063/5.0041203},
}



@InProceedings{Alk2021_ModelingForwardWave_LiuALA,
  author    = {Alkhadhr, Shaikhah and Liu, Xilun and Almekkawy, Mohamed},
  booktitle = {2021 {IEEE} {International} {Ultrasonics} {Symposium} ({IUS})},
  title     = {Modeling of the {Forward} {Wave} {Propagation} {Using} {Physics}-{Informed} {Neural} {Networks}},
  year      = {2021},
  month     = sep,
  note      = {ISSN: 1948-5727},
  pages     = {1--4},
  doi       = {10.1109/IUS52206.2021.9593574},
  groups    = {Applications},
  issn      = {1948-5727},
  keywords  = {Training, Ultrasonic imaging, Propagation, Neural networks, Mathematical models, Numerical models, Frequency division multiplexing, Physics-Informed Neural Networks, Wave Equation, Forward Problem, Numerical Modelling},
}







@Article{Li2021_PhysicsGuidedNeural_BazLBZ,
  author     = {Li, Wei and Bazant, Martin Z. and Zhu, Juner},
  journal    = {Computer Methods in Applied Mechanics and Engineering},
  title      = {A physics-guided neural network framework for elastic plates: {Comparison} of governing equations-based and energy-based approaches},
  year       = {2021},
  issn       = {0045-7825},
  month      = sep,
  pages      = {113933},
  volume     = {383},
  doi        = {10.1016/j.cma.2021.113933},
  groups     = {PaperWithCode},
  keywords   = {Physics-informed neural network, Deep Ritz, Structural mechanics, Elastic plates},
  language   = {en},
  shorttitle = {A physics-guided neural network framework for elastic plates},
  url        = {https://www.sciencedirect.com/science/article/pii/S004578252100270X},
}





@Article{Kov2022_ConditionalPhysicsInformed_ExlKEK,
  author   = {Kovacs, Alexander and Exl, Lukas and Kornell, Alexander and Fischbacher, Johann and Hovorka, Markus and Gusenbauer, Markus and Breth, Leoni and Oezelt, Harald and Yano, Masao and Sakuma, Noritsugu and Kinoshita, Akihito and Shoji, Tetsuya and Kato, Akira and Schrefl, Thomas},
  journal  = {Communications in Nonlinear Science and Numerical Simulation},
  title    = {Conditional physics informed neural networks},
  year     = {2022},
  issn     = {1007-5704},
  month    = jan,
  pages    = {106041},
  volume   = {104},
  doi      = {10.1016/j.cnsns.2021.106041},
  groups   = {Applications},
  keywords = {Micromagnetics, Neural network, Ritz method},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S1007570421003531},
}




@Article{Gar2019_ReconcilingDeepLearning_ShaGS,
  author     = {Garnelo, Marta and Shanahan, Murray},
  journal    = {Current Opinion in Behavioral Sciences},
  title      = {Reconciling deep learning with symbolic artificial intelligence: representing objects and relations},
  year       = {2019},
  issn       = {2352-1546},
  month      = oct,
  pages      = {17--23},
  volume     = {29},
  doi        = {10.1016/j.cobeha.2018.12.010},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S2352154618301943/pdfft?md5=11aa9583d5761d5503d6585594014fc6&pid=1-s2.0-S2352154618301943-main.pdf&isDTMRedir=Y:application/pdf},
  groups     = {Discussion},
  language   = {en},
  series     = {Artificial {Intelligence}},
  shorttitle = {Reconciling deep learning with symbolic artificial intelligence},
  url        = {https://www.sciencedirect.com/science/article/pii/S2352154618301943},
}


%%%%%%%%%%%%%%%%%%%
%PCNN  - START
%%%%%%%%%%%%%%%%%


@Article{Liu2021_DualDimerMethod_WanLW,
  author   = {Liu, Dehao and Wang, Yan},
  journal  = {Neural Networks},
  title    = {A {Dual}-{Dimer} method for training physics-constrained neural networks with minimax architecture},
  year     = {2021},
  issn     = {0893-6080},
  month    = apr,
  pages    = {112--125},
  volume   = {136},
  abstract = {Data sparsity is a common issue to train machine learning tools such as neural networks for engineering and scientific applications, where experiments and simulations are expensive. Recently physics-constrained neural networks (PCNNs) were developed to reduce the required amount of training data. However, the weights of different losses from data and physical constraints are adjusted empirically in PCNNs. In this paper, a new physics-constrained neural network with the minimax architecture (PCNN-MM) is proposed so that the weights of different losses can be adjusted systematically. The training of the PCNN-MM is searching the high-order saddle points of the objective function. A novel saddle point search algorithm called Dual-Dimer method is developed. It is demonstrated that the Dual-Dimer method is computationally more efficient than the gradient descent ascent method for nonconvex–nonconcave functions and provides additional eigenvalue information to verify search results. A heat transfer example also shows that the convergence of PCNN-MMs is faster than that of traditional PCNNs.},
  doi      = {10.1016/j.neunet.2020.12.028},
  groups   = {Code_nPINN PCNN},
  keywords = {Machine learning, Physics-constrained neural networks, Partial differential equation, Minimax problem, Saddle point search},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608020304536},
}



%[auth:lower][year][veryshorttitle:lower]
%[auth:lower][year][veryshorttitle:lower]_[auth3_2][authorsAlpha]
%[auth:lower][year][veryshorttitle:lower]_[shorttitle:titlecase]_[auth3_2][authorsAlpha]


@InProceedings{Kri2021_CharacterizingPossibleFailure_GhoKGZ,
  author    = {Krishnapriyan, Aditi and Gholami, Amir and Zhe, Shandian and Kirby, Robert and Mahoney, Michael W},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Characterizing possible failure modes in physics-informed neural networks},
  year      = {2021},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {26548--26560},
  publisher = {Curran Associates, Inc.},
  volume    = {34},
  groups    = {Rev1},
  url       = {https://proceedings.neurips.cc/paper/2021/file/df438e5206f31600e6ae4af72f2725f1-Paper.pdf},
}
@Article{Wan2022_RespectingCausalityIs_SanWSP,
  author   = {Wang, Sifan and Sankaran, Shyam and Perdikaris, Paris},
  journal  = {arXiv:2203.07404 [nlin, physics:physics, stat]},
  title    = {Respecting causality is all you need for training physics-informed neural networks},
  year     = {2022},
  month    = mar,
  note     = {arXiv: 2203.07404},
  annote   = {Comment: 35 pages, 27 figures, 4 tables},
  keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Nonlinear Sciences - Chaotic Dynamics, Physics - Fluid Dynamics, Statistics - Machine Learning},
  url      = {http://arxiv.org/abs/2203.07404},
}

@Article{Hai2022_ImprovedTrainingPhysics_IliHI,
  author   = {Haitsiukevich, Katsiaryna and Ilin, Alexander},
  journal  = {arXiv:2204.05108 [cs, stat]},
  title    = {Improved {Training} of {Physics}-{Informed} {Neural} {Networks} with {Model} {Ensembles}},
  year     = {2022},
  month    = apr,
  note     = {arXiv: 2204.05108},
  groups   = {Rev1},
  keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
  url      = {http://arxiv.org/abs/2204.05108},
}

@Article{Wen2022_UFnoanEnhanced_LiWLA,
  author   = {Wen, Gege and Li, Zongyi and Azizzadenesheli, Kamyar and Anandkumar, Anima and Benson, Sally M.},
  journal  = {Advances in Water Resources},
  title    = {U-{FNO}—{An} enhanced {Fourier} neural operator-based deep-learning model for multiphase flow},
  year     = {2022},
  issn     = {0309-1708},
  month    = may,
  pages    = {104180},
  volume   = {163},
  doi      = {10.1016/j.advwatres.2022.104180},
  groups   = {Rev1},
  keywords = {Multiphase flow, Fourier neural operator, Convolutional neural network, Carbon capture and storage, Deep learning},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0309170822000562},
}
@Article{Ore2020_NBeatsNeural_CarOCCB,
  author     = {Oreshkin, Boris N. and Carpov, Dmitri and Chapados, Nicolas and Bengio, Yoshua},
  journal    = {arXiv:1905.10437 [cs, stat]},
  title      = {N-{BEATS}: {Neural} basis expansion analysis for interpretable time series forecasting},
  year       = {2020},
  month      = feb,
  note       = {arXiv: 1905.10437},
  groups     = {Rev1},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  shorttitle = {N-{BEATS}},
  url        = {http://arxiv.org/abs/1905.10437},
}
@Article{Kas2021_PhysicsInformedMachine_MusKMA,
  author     = {Kashinath, K. and Mustafa, M. and Albert, A. and Wu, J-L. and Jiang, C. and Esmaeilzadeh, S. and Azizzadenesheli, K. and Wang, R. and Chattopadhyay, A. and Singh, A. and Manepalli, A. and Chirila, D. and Yu, R. and Walters, R. and White, B. and Xiao, H. and Tchelepi, H. A. and Marcus, P. and Anandkumar, A. and Hassanzadeh, P. and Prabhat, null},
  journal    = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  title      = {Physics-informed machine learning: case studies for weather and climate modelling},
  year       = {2021},
  month      = apr,
  number     = {2194},
  pages      = {20200093},
  volume     = {379},
  doi        = {10.1098/rsta.2020.0093},
  groups     = {Rev1},
  keywords   = {weather and climate modeling, physics-informed machine learning, turbulent flows, neural networks, physical constraints},
  publisher  = {Royal Society},
  shorttitle = {Physics-informed machine learning},
  url        = {https://royalsocietypublishing.org/doi/full/10.1098/rsta.2020.0093},
}

@Article{Raf2022_DsfaPinnDeep_RafRRC,
  author     = {Rafiq, Muhammad and Rafiq, Ghazala and Choi, Gyu Sang},
  journal    = {IEEE Access},
  title      = {{DSFA}-{PINN}: {Deep} {Spectral} {Feature} {Aggregation} {Physics} {Informed} {Neural} {Network}},
  year       = {2022},
  issn       = {2169-3536},
  pages      = {22247--22259},
  volume     = {10},
  doi        = {10.1109/ACCESS.2022.3153056},
  groups     = {Rev1},
  shorttitle = {{DSFA}-{PINN}},
}



@Article{Hua2021_SolvingPartialDifferential_LiuHLS,
  author   = {Huang, Xiang and Liu, Hongsheng and Shi, Beiji and Wang, Zidong and Yang, Kang and Li, Yang and Weng, Bingya and Wang, Min and Chu, Haotian and Zhou, Jing and Yu, Fan and Hua, Bei and Chen, Lei and Dong, Bin},
  journal  = {arXiv:2111.01394 [physics]},
  title    = {Solving {Partial} {Differential} {Equations} with {Point} {Source} {Based} on {Physics}-{Informed} {Neural} {Networks}},
  year     = {2021},
  month    = nov,
  note     = {arXiv: 2111.01394},
  groups   = {Rev1},
  keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Physics - Computational Physics},
  url      = {http://arxiv.org/abs/2111.01394},
}


@Article{Won2022_LearningSinusoidalSpaces_OoiWOGO,
  author   = {Wong, Jian Cheng and Ooi, Chinchun and Gupta, Abhishek and Ong, Yew-Soon},
  journal  = {arXiv:2109.09338 [physics]},
  title    = {Learning in {Sinusoidal} {Spaces} with {Physics}-{Informed} {Neural} {Networks}},
  year     = {2022},
  month    = mar,
  note     = {arXiv: 2109.09338},
  annote   = {Comment: 16 pages, 13 figures},
  groups   = {Rev1},
  keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science, Physics - Computational Physics},
  url      = {http://arxiv.org/abs/2109.09338},
}



@Article{Bau2019_DeepLearningAs_KohBK,
  author    = {Bauer, Benedikt and Kohler, Michael},
  journal   = {The Annals of Statistics},
  title     = {On deep learning as a remedy for the curse of dimensionality in nonparametric regression},
  year      = {2019},
  issn      = {0090-5364, 2168-8966},
  month     = aug,
  number    = {4},
  pages     = {2261--2285},
  volume    = {47},
  doi       = {10.1214/18-AOS1747},
  groups    = {Rev1},
  keywords  = {62G08, 62G20, curse of dimensionality, neural networks, Nonparametric regression, rate of convergence},
  publisher = {Institute of Mathematical Statistics},
  url       = {https://projecteuclid.org/journals/annals-of-statistics/volume-47/issue-4/On-deep-learning-as-a-remedy-for-the-curse-of/10.1214/18-AOS1747.full},
}

@Article{Zub2021_NeuralpdeAutomatingPhysics_McCZMMa,
  author     = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},
  journal    = {arXiv:2107.09443 [cs]},
  title      = {{NeuralPDE}: {Automating} {Physics}-{Informed} {Neural} {Networks} ({PINNs}) with {Error} {Approximations}},
  year       = {2021},
  month      = jul,
  note       = {arXiv: 2107.09443},
  annote     = {Comment: 74 pages, 20+ figures, 20+ tables},
  groups     = {Rev1},
  keywords   = {Computer Science - Mathematical Software, Computer Science - Symbolic Computation},
  shorttitle = {{NeuralPDE}},
  url        = {http://arxiv.org/abs/2107.09443},
}


                        
@Article{Zhi2020_FrequencyPrincipleFourier_XuZX9,
  author   = {Zhi-Qin and John Xu and and 9573 and and Zhi-Qin John Xu and Yaoyu and Zhang and and 9574 and and Yaoyu Zhang and Tao and Luo and and 9575 and and Tao Luo and Yanyang and Xiao and and 9576 and and Yanyang Xiao and Zheng and Ma and and 9577 and and Zheng Ma},
  journal  = {Communications in Computational Physics},
  title    = {Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks},
  year     = {2020},
  issn     = {1991-7120},
  number   = {5},
  pages    = {1746--1767},
  volume   = {28},
  doi      = {https://doi.org/10.4208/cicp.OA-2020-0085},
  groups   = {Rev2},
  url      = {http://global-sci.org/intro/article_detail/cicp/18395.html},
}


@TechReport{Hil2022_CertifiedMachineLearning_UngHU,
  author     = {Hillebrecht, Birgit and Unger, Benjamin},
  title      = {Certified machine learning: {A} posteriori error estimation for physics-informed neural networks},
  year       = {2022},
  month      = mar,
  note       = {arXiv:2203.17055 [cs, math] type: article},
  doi        = {10.48550/arXiv.2203.17055},
  groups     = {Rev2},
  keywords   = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
  school     = {arXiv},
  shorttitle = {Certified machine learning},
  url        = {http://arxiv.org/abs/2203.17055},
}



@Article{Chi2022_CanPinnFast_WonCWO,
  author     = {Chiu, Pao-Hsiung and Wong, Jian Cheng and Ooi, Chinchun and Dao, My Ha and Ong, Yew-Soon},
  journal    = {Computer Methods in Applied Mechanics and Engineering},
  title      = {{CAN}-{PINN}: {A} fast physics-informed neural network based on coupled-automatic–numerical differentiation method},
  year       = {2022},
  issn       = {0045-7825},
  month      = may,
  pages      = {114909},
  volume     = {395},
  doi        = {10.1016/j.cma.2022.114909},
  groups     = {Rev2},
  keywords   = {Physics-informed neural network, Training loss formulation, Taylor series expansions, Coupled-automatic–numerical differentiation, Navier–Stokes equations, Inverse problem},
  language   = {en},
  shorttitle = {{CAN}-{PINN}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0045782522001906},
}














%% JabRef style:  [auth3][year]_[shorttitle:titlecase]_[auth3_2][authorsAlpha]


@Article{Mis2022_EstimatesGeneralizationError_MolMM,
  author   = {Mishra, Siddhartha and Molinaro, Roberto},
  journal  = {IMA Journal of Numerical Analysis},
  title    = {Estimates on the generalization error of physics-informed neural networks for approximating {PDEs}},
  year     = {2022},
  issn     = {0272-4979},
  month    = jan,
  pages    = {drab093},
  doi      = {10.1093/imanum/drab093},
  groups   = {Mishra Theory},
  url      = {https://doi.org/10.1093/imanum/drab093},
}


@Article{Mis2021_PhysicsInformedNeural_MolMM,
  author   = {Mishra, Siddhartha and Molinaro, Roberto},
  journal  = {Journal of Quantitative Spectroscopy and Radiative Transfer},
  title    = {Physics informed neural networks for simulating radiative transfer},
  year     = {2021},
  issn     = {0022-4073},
  month    = aug,
  pages    = {107705},
  volume   = {270},
  doi      = {10.1016/j.jqsrt.2021.107705},
  groups   = {Mishra Theory},
  keywords = {Radiative transfer, Inverse problems, Deep learning, Physics Informed NeuralNetworks},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0022407321001989},
}


@Article{De2021_ErrorAnalysisPhysics_MisDRM,
  author   = {De Ryck, Tim and Mishra, Siddhartha},
  journal  = {arXiv:2106.14473 [cs, math]},
  title    = {Error analysis for physics informed neural networks ({PINNs}) approximating {Kolmogorov} {PDEs}},
  year     = {2021},
  month    = jul,
  note     = {arXiv: 2106.14473},
  groups   = {Mishra Theory},
  keywords = {Mathematics - Numerical Analysis, Computer Science - Machine Learning},
  url      = {http://arxiv.org/abs/2106.14473},
}

@Article{Shi2020_ConvergencePhysicsInformed_DarSDK,
  author   = {Shin, Yeonjong and Darbon, Jerome and Karniadakis, George Em},
  journal  = {Communications in Computational Physics},
  title    = {On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type {PDEs}},
  year     = {2020},
  issn     = {1815-2406, 1991-7120},
  month    = jun,
  note     = {arXiv: 2004.01806},
  number   = {5},
  pages    = {2042--2074},
  volume   = {28},
  doi      = {10.4208/cicp.OA-2020-0193},
  groups   = {Mishra Theory},
  keywords = {Mathematics - Numerical Analysis, Computer Science - Machine Learning},
  url      = {http://arxiv.org/abs/2004.01806},
}



@Article{De2021_ApproximationFunctionsTanh_LanDRLM,
  author   = {De Ryck, Tim and Lanthaler, Samuel and Mishra, Siddhartha},
  journal  = {Neural Networks},
  title    = {On the approximation of functions by tanh neural networks},
  year     = {2021},
  issn     = {0893-6080},
  month    = nov,
  pages    = {732--750},
  volume   = {143},
  doi      = {10.1016/j.neunet.2021.08.015},
  groups   = {Mishra Theory, Mishra},
  keywords = {Neural networks, Tanh, Function approximation, Deep learning},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608021003208},
}




@Article{De2022_ErrorEstimatesPhysics_JagDRJM,
  author   = {De Ryck, Tim and Jagtap, Ameya D. and Mishra, Siddhartha},
  journal  = {arXiv:2203.09346 [cs, math]},
  title    = {Error estimates for physics informed neural networks approximating the {Navier}-{Stokes} equations},
  year     = {2022},
  month    = mar,
  note     = {arXiv: 2203.09346},
  groups   = {Mishra Theory},
  keywords = {Mathematics - Numerical Analysis, Computer Science - Machine Learning},
  url      = {http://arxiv.org/abs/2203.09346},
}

@Article{Shi2020_ErrorEstimatesResidual_ZhaSZK,
  author   = {Shin, Yeonjong and Zhang, Zhongqiang and Karniadakis, George Em},
  journal  = {arXiv:2010.08019 [cs, math]},
  title    = {Error estimates of residual minimization using neural networks for linear {PDEs}},
  year     = {2020},
  month    = nov,
  note     = {arXiv: 2010.08019},
  groups   = {Mishra Theory},
  keywords = {Mathematics - Numerical Analysis, 65M12, 41A46, 65N30, 35J25, 35S15},
  url      = {http://arxiv.org/abs/2010.08019},
}



@Article{Kut2022_MathematicsArtificialIntelligence_Kut,
  author   = {Kutyniok, Gitta},
  journal  = {arXiv:2203.08890 [cs, math, stat]},
  title    = {The {Mathematics} of {Artificial} {Intelligence}},
  year     = {2022},
  month    = mar,
  note     = {arXiv: 2203.08890},
  abstract = {We currently witness the spectacular success of artificial intelligence in both science and public life. However, the development of a rigorous mathematical foundation is still at an early stage. In this survey article, which is based on an invited lecture at the International Congress of Mathematicians 2022, we will in particular focus on the current "workhorse" of artificial intelligence, namely deep neural networks. We will present the main theoretical directions along with several exemplary results and discuss key open problems.},
  annote   = {Comment: 16 pages, 7 figures},
  groups   = {Mishra Theory},
  keywords = {Computer Science - Machine Learning, Mathematics - History and Overview, Statistics - Machine Learning, Primary 68T07, Secondary 41A25, 42C15, 35C20, 65D18},
  url      = {http://arxiv.org/abs/2203.08890},
}


%% Extra add


@Article{Fuk2020_LimitationsPhysicsInformed_TchFT,
  author    = {Fuks, Olga and Tchelepi, Hamdi A.},
  journal   = {Journal of Machine Learning for Modeling and Computing},
  title     = {{LIMITATIONS} {OF} {PHYSICS} {INFORMED} {MACHINE} {LEARNING} {FOR} {NONLINEAR} {TWO}-{PHASE} {TRANSPORT} {IN} {POROUS} {MEDIA}},
  year      = {2020},
  issn      = {2689-3967, 2689-3975},
  number    = {1},
  volume    = {1},
  doi       = {10.1615/.2020033905},
  groups    = {Rev2 2022},
  language  = {English},
  publisher = {Begel House Inc.},
  url       = {https://www.dl.begellhouse.com/journals/558048804a15188a,583c4e56625ba94e,415f83b5707fde65.html},
}



@Book{shalevshwartz2014understanding,
  author    = {Shalev-Shwartz, Shai and Ben-David, Shai},
  publisher = {Cambridge University Press},
  title     = {Understanding Machine Learning: From Theory to Algorithms},
  year      = {2014},
  doi       = {10.1017/CBO9781107298019},
  groups    = {Mishra Theory},
  place     = {Cambridge},
}



@Article{Yua2022_PinnAuxiliaryPhysics_NiYNDH,
  author     = {Yuan, Lei and Ni, Yi-Qing and Deng, Xiang-Yun and Hao, Shuo},
  journal    = {Journal of Computational Physics},
  title      = {A-{PINN}: {Auxiliary} physics informed neural networks for forward and inverse problems of nonlinear integro-differential equations},
  year       = {2022},
  issn       = {0021-9991},
  month      = aug,
  pages      = {111260},
  volume     = {462},
  doi        = {10.1016/j.jcp.2022.111260},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0021999122003229/pdfft?md5=f8d266642123a4065ae5306f21523ed5&pid=1-s2.0-S0021999122003229-main.pdf&isDTMRedir=Y:application/pdf},
  groups     = {Main},
  keywords   = {Physics informed neural network (PINN), Auxiliary physics informed neural network (A-PINN), Integro-differential equations (IDEs), Deep learning, Multi-output neural network},
  language   = {en},
  shorttitle = {A-{PINN}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999122003229},
}



@Article{Cai2021_PhysicsInformedNeural_MaoCMW,
  author     = {Cai, Shengze and Mao, Zhiping and Wang, Zhicheng and Yin, Minglang and Karniadakis, George Em},
  journal    = {Acta Mechanica Sinica},
  title      = {Physics-informed neural networks ({PINNs}) for fluid mechanics: a review},
  year       = {2021},
  issn       = {1614-3116},
  month      = dec,
  number     = {12},
  pages      = {1727--1738},
  volume     = {37},
  doi        = {10.1007/s10409-021-01148-1},
  file       = {Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2Fs10409-021-01148-1.pdf:application/pdf},
  groups     = {Main},
  keywords   = {Physics-informed learning, PINNs, Inverse problems, Supersonic flows, Biomedical flows},
  language   = {en},
  shorttitle = {Physics-informed neural networks ({PINNs}) for fluid mechanics},
  url        = {https://doi.org/10.1007/s10409-021-01148-1},
}




@InCollection{Cal2020_UniversalApproximators_Cal,
  author    = {Calin, Ovidiu},
  publisher = {Springer International Publishing},
  title     = {Universal {Approximators}},
  year      = {2020},
  address   = {Cham},
  editor    = {Calin, Ovidiu},
  isbn      = {9783030367213},
  pages     = {251--284},
  series    = {Springer {Series} in the {Data} {Sciences}},
  doi       = {10.1007/978-3-030-36721-3_9},
  groups    = {Rev2 2022},
  language  = {en},
  url       = {https://doi.org/10.1007/978-3-030-36721-3_9},
}

@Article{Elb2021_DeepNeuralNetwork_PerEPGB,
  author   = {Elbrächter, Dennis and Perekrestenko, Dmytro and Grohs, Philipp and Bölcskei, Helmut},
  journal  = {IEEE Transactions on Information Theory},
  title    = {Deep {Neural} {Network} {Approximation} {Theory}},
  year     = {2021},
  issn     = {1557-9654},
  month    = may,
  number   = {5},
  pages    = {2581--2623},
  volume   = {67},
  doi      = {10.1109/TIT.2021.3062161},
  keywords = {Neural networks, Complexity theory, Function approximation, Dictionaries, Approximation error, Two dimensional displays, Training data, Neural networks, machine learning, function approximation, rate-distortion theory, nonlinear approximation, metric entropy},
}



@Article{Bel2019_ReconcilingModernMachine_HsuBHMM,
  author    = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal   = {Proceedings of the National Academy of Sciences},
  title     = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
  year      = {2019},
  month     = aug,
  number    = {32},
  pages     = {15849--15854},
  volume    = {116},
  doi       = {10.1073/pnas.1903070116},
  groups    = {Rev2 2022},
  publisher = {Proceedings of the National Academy of Sciences},
  url       = {https://www.pnas.org/doi/10.1073/pnas.1903070116},
}



@Article{Pin1999_ApproximationTheoryMlp_Pin,
  author    = {Pinkus, Allan},
  journal   = {Acta Numerica},
  title     = {Approximation theory of the {MLP} model in neural networks},
  year      = {1999},
  issn      = {1474-0508, 0962-4929},
  month     = jan,
  pages     = {143--195},
  volume    = {8},
  doi       = {10.1017/S0962492900002919},
  groups    = {Rev2},
  language  = {en},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/journals/acta-numerica/article/abs/approximation-theory-of-the-mlp-model-in-neural-networks/18072C558C8410C4F92A82BCC8FC8CF9},
}

@InCollection{Arn2015_StabilityConsistencyConvergence_Arn,
  author    = {Arnold, Douglas N.},
  publisher = {Springer},
  title     = {Stability, {Consistency}, and {Convergence} of {Numerical} {Discretizations}},
  year      = {2015},
  address   = {Berlin, Heidelberg},
  editor    = {Engquist, Björn},
  isbn      = {9783540705291},
  pages     = {1358--1364},
  doi       = {10.1007/978-3-540-70529-1_407},
  groups    = {Rev2},
  language  = {en},
  url       = {https://doi.org/10.1007/978-3-540-70529-1_407},
}


@Book{Rya2006_TheoreticalIntroductionNumerical_TsyRT,
  author    = {Ryaben'kii, Victor S. and Tsynkov, Semyon V.},
  publisher = {CRC Press},
  title     = {A {Theoretical} {Introduction} to {Numerical} {Analysis}},
  year      = {2006},
  isbn      = {9781584886075},
  month     = nov,
  groups    = {Rev2},
  keywords  = {Mathematics / Number Systems, Science / Physics / Mathematical \& Computational, Technology \& Engineering / Mechanical, Computers / General},
  language  = {en},
}

@InProceedings{Tho1992_NumericalMethods101convergence_Tho,
  author    = {Thompson, David B.},
  title     = {Numerical {Methods} 101 -- {Convergence} of {Numerical} {Models}},
  year      = {1992},
  pages     = {398--403},
  publisher = {ASCE},
  abstract  = {Numerical Methods 101—Convergence of Numerical Models.},
  groups    = {Rev2},
  language  = {eng},
  url       = {https://cedb.asce.org/CEDBsearch/record.jsp?dockey=0078142},
}






@Article{Dis1994_NeuralNetworkBased_PhaDP,
  author   = {Dissanayake, M. W. M. G. and Phan-Thien, N.},
  journal  = {Communications in Numerical Methods in Engineering},
  title    = {Neural-network-based approximations for solving partial differential equations},
  year     = {1994},
  issn     = {1099-0887},
  number   = {3},
  pages    = {195--201},
  volume   = {10},
  doi      = {10.1002/cnm.1640100303},
  groups   = {History},
  language = {en},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.1640100303},
}




@Article{Lag2000_NeuralNetworkMethods_LikLLP,
  author   = {Lagaris, I.E. and Likas, A.C. and Papageorgiou, D.G.},
  journal  = {IEEE Transactions on Neural Networks},
  title    = {Neural-network methods for boundary value problems with irregular boundaries},
  year     = {2000},
  issn     = {1941-0093},
  month    = sep,
  number   = {5},
  pages    = {1041--1049},
  volume   = {11},
  doi      = {10.1109/72.870037},
  groups   = {History},
  keywords = {Boundary value problems, Boundary conditions, Multilayer perceptrons, Differential equations, Shape, Geometry, Neural networks, Concurrent computing, Radial basis function networks, Testing},
}




@Article{Tor2020_TheoryTrainingDeep_ViaTRVSA,
  author   = {Torabi Rad, M. and Viardin, A. and Schmitz, G. J. and Apel, M.},
  journal  = {Computational Materials Science},
  title    = {Theory-training deep neural networks for an alloy solidification benchmark problem},
  year     = {2020},
  issn     = {0927-0256},
  month    = jul,
  pages    = {109687},
  volume   = {180},
  doi      = {10.1016/j.commatsci.2020.109687},
  groups   = {fUTURE},
  keywords = {Solidification, Machine learning, Deep neural networks, TensorFlow, Theory-trained neural networks (TTNs)},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0927025620301786},
}

@Article{Oez2021_PoissonCnnConvolutional_HamOeHL,
  author     = {Özbay, Ali Girayhan and Hamzehloo, Arash and Laizet, Sylvain and Tzirakis, Panagiotis and Rizos, Georgios and Schuller, Björn},
  journal    = {Data-Centric Engineering},
  title      = {Poisson {CNN}: {Convolutional} neural networks for the solution of the {Poisson} equation on a {Cartesian} mesh},
  year       = {2021},
  issn       = {2632-6736},
  volume     = {2},
  doi        = {10.1017/dce.2021.7},
  groups     = {Rev2},
  keywords   = {Convolutional neural network, partial differential equations, Poisson equation},
  language   = {en},
  publisher  = {Cambridge University Press},
  shorttitle = {Poisson {CNN}},
  url        = {https://www.cambridge.org/core/journals/data-centric-engineering/article/poisson-cnn-convolutional-neural-networks-for-the-solution-of-the-poisson-equation-on-a-cartesian-mesh/8CDFD5C9D5172E51B924E9AA1BA253A1},
}








@Article{Kum2011_MultilayerPerceptronsRadial_YadKY,
  author     = {Kumar, Manoj and Yadav, Neha},
  journal    = {Computers \& Mathematics with Applications},
  title      = {Multilayer perceptrons and radial basis function neural network methods for the solution of differential equations: {A} survey},
  year       = {2011},
  issn       = {0898-1221},
  month      = nov,
  number     = {10},
  pages      = {3796--3811},
  volume     = {62},
  doi        = {10.1016/j.camwa.2011.09.028},
  groups     = {History},
  keywords   = {Differential equations, Neural network, Multilayer perceptron, Radial basis functions, Backpropagation algorithm},
  language   = {en},
  shorttitle = {Multilayer perceptrons and radial basis function neural network methods for the solution of differential equations},
  url        = {https://www.sciencedirect.com/science/article/pii/S0898122111007966},
}


@TechReport{Pas2017_AutomaticDifferentiationPytorch_GroPGC,
  author   = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  title    = {Automatic differentiation in {PyTorch}},
  year     = {2017},
  month    = oct,
  abstract = {A summary of automatic differentiation techniques employed in PyTorch library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...},
  language = {en},
  url      = {https://openreview.net/forum?id=BJJsrmfCZ},
}




%%%%
% Rev 1
%%%

@Article{Sir2018_DgmDeepLearning_SpiSS,
  author     = {Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal    = {Journal of Computational Physics},
  title      = {{DGM}: {A} deep learning algorithm for solving partial differential equations},
  year       = {2018},
  issn       = {0021-9991},
  month      = dec,
  pages      = {1339--1364},
  volume     = {375},
  doi        = {10.1016/j.jcp.2018.08.029},
  groups     = {Rev2, Rev1},
  keywords   = {Partial differential equations, Machine learning, Deep learning, High-dimensional partial differential equations},
  language   = {en},
  shorttitle = {{DGM}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
}


@Article{Wei2018_DeepRitzMethod_YuWY,
  author     = {E, Weinan and Yu, Bing},
  journal    = {Communications in Mathematics and Statistics},
  title      = {The {Deep} {Ritz} {Method}: {A} {Deep} {Learning}-{Based} {Numerical} {Algorithm} for {Solving} {Variational} {Problems}},
  year       = {2018},
  issn       = {2194-671X},
  month      = mar,
  number     = {1},
  pages      = {1--12},
  volume     = {6},
  doi        = {10.1007/s40304-018-0127-z},
  groups     = {Rev1},
  keywords   = {Deep Ritz Method, Variational problems, PDE, Eigenvalue problems, 35Q68},
  language   = {en},
  shorttitle = {The {Deep} {Ritz} {Method}},
  url        = {https://doi.org/10.1007/s40304-018-0127-z},
}


@Article{Mar2021_OldNewCan_Mar,
  author     = {Markidis, Stefano},
  journal    = {Frontiers in Big Data},
  title      = {The {Old} and the {New}: {Can} {Physics}-{Informed} {Deep}-{Learning} {Replace} {Traditional} {Linear} {Solvers}?},
  year       = {2021},
  issn       = {2624-909X},
  volume     = {4},
  groups     = {Rev1},
  shorttitle = {The {Old} and the {New}},
  url        = {https://www.frontiersin.org/article/10.3389/fdata.2021.669097},
}

@Article{Sch2021_CausalRepresentationLearning_LocSLB,
  author   = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  journal  = {Proceedings of the IEEE},
  title    = {Toward {Causal} {Representation} {Learning}},
  year     = {2021},
  issn     = {1558-2256},
  month    = may,
  number   = {5},
  pages    = {612--634},
  volume   = {109},
  doi      = {10.1109/JPROC.2021.3058954},
  groups   = {Rev1},
  keywords = {Mathematical model, Machine learning, Data models, Differential equations, Inference algorithms, Training data, Adaptation models, Artificial intelligence, Representation learning, Artificial intelligence, causality, deep learning, representation learning},
}


@Article{Kar2020_ExtendedPhysicsInformed_Kar,
  author     = {Karniadakis, Ameya D. Jagtap \& George Em},
  journal    = {Communications in Computational Physics},
  title      = {Extended {Physics}-{Informed} {Neural} {Networks} ({XPINNs}): {A} {Generalized} {Space}-{Time} {Domain} {Decomposition} {Based} {Deep} {Learning} {Framework} for {Nonlinear} {Partial} {Differential} {Equations}},
  year       = {2020},
  issn       = {1815-2406, 1991-7120},
  month      = jun,
  number     = {5},
  pages      = {2002--2041},
  volume     = {28},
  doi        = {10.4208/cicp.OA-2020-0164},
  groups     = {Rev2},
  shorttitle = {Extended {Physics}-{Informed} {Neural} {Networks} ({XPINNs})},
  url        = {http://global-sci.org/intro/article_detail/cicp/18403.html},
}


@Article{Jag2020_AdaptiveActivationFunctions_KawJKK,
  author   = {Jagtap, Ameya D. and Kawaguchi, Kenji and Karniadakis, George Em},
  journal  = {Journal of Computational Physics},
  title    = {Adaptive activation functions accelerate convergence in deep and physics-informed neural networks},
  year     = {2020},
  issn     = {0021-9991},
  month    = mar,
  pages    = {109136},
  volume   = {404},
  doi      = {10.1016/j.jcp.2019.109136},
  groups   = {Rev2},
  keywords = {Machine learning, Bad minima, Inverse problems, Physics-informed neural networks, Partial differential equations, Deep learning benchmarks},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0021999119308411},
}

@Article{Sit2020_ImplicitNeuralRepresentations_MarSMB,
  author   = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
  journal  = {arXiv:2006.09661 [cs, eess]},
  title    = {Implicit {Neural} {Representations} with {Periodic} {Activation} {Functions}},
  year     = {2020},
  month    = jun,
  note     = {arXiv: 2006.09661},
  annote   = {Comment: Project website: https://vsitzmann.github.io/siren/ Project video: https://youtu.be/Q2fLWGBeaiI},
  groups   = {Rev2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
  url      = {http://arxiv.org/abs/2006.09661},
}



@Article{Rud2022_InterpretableMachineLearning_CheRCC,
  author     = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
  journal    = {Statistics Surveys},
  title      = {Interpretable machine learning: {Fundamental} principles and 10 grand challenges},
  year       = {2022},
  issn       = {1935-7516},
  month      = jan,
  number     = {none},
  pages      = {1--85},
  volume     = {16},
  doi        = {10.1214/21-SS133},
  groups     = {Rev2},
  keywords   = {62-02, 68T01, explainable machine learning, Interpretable machine learning},
  publisher  = {Amer. Statist. Assoc., the Bernoulli Soc., the Inst. Math. Statist., and the Statist. Soc. Canada},
  shorttitle = {Interpretable machine learning},
  url        = {https://projecteuclid.org/journals/statistics-surveys/volume-16/issue-none/Interpretable-machine-learning-Fundamental-principles-and-10-grand-challenges/10.1214/21-SS133.full},
}

@TechReport{Sch2021_LearningEstimateRegions_HaySH,
  author   = {Scharzenberger, Cody and Hays, Joe},
  title    = {Learning {To} {Estimate} {Regions} {Of} {Attraction} {Of} {Autonomous} {Dynamical} {Systems} {Using} {Physics}-{Informed} {Neural} {Networks}},
  year     = {2021},
  month    = nov,
  note     = {arXiv:2111.09930 [cs] type: article},
  annote   = {Comment: 31 pages, 17 figures},
  doi      = {10.48550/arXiv.2111.09930},
  groups   = {Rev1},
  keywords = {Computer Science - Machine Learning},
  school   = {arXiv},
  url      = {http://arxiv.org/abs/2111.09930},
}




@TechReport{Nan2021_ProgressTowardsSolving_HenNHN,
  author   = {Nandi, Tarak and Hennigh, Oliver and Nabian, Mohammad and Liu, Yong and Woo, Mino and Jordan, Terry and Shahnam, Mehrdad and Syamlal, Madhava and Guenther, Chris and VanEssendelft, Dirk},
  title    = {Progress {Towards} {Solving} {High} {Reynolds} {Number} {Reacting} {Flows} in {SimNet}},
  year     = {2021},
  month    = nov,
  abstract = {NVIDIA GPU Technology Conference, November 8-11, 2021.},
  groups   = {Rev2},
  language = {English},
  url      = {https://www.osti.gov/biblio/1846970-progress-towards-solving-high-reynolds-number-reacting-flows-simnet},
}

@TechReport{Nan2022_DevelopingDigitalTwins_HenNHN,
  author   = {Nandi, Tarak and Hennigh, Oliver and Nabian, Mohammad and Liu, Yong and Woo, Mino and Jordan, Terry and Shahnam, Mehrdad and Guenther, Chris and VanEssendelft, Dirk},
  title    = {Developing {Digital} {Twins} for {Energy} {Applications} {Using} {Modulus}},
  year     = {2022},
  month    = mar,
  abstract = {NVIDIA GPU Technology Conference (Spring 2022), Virtual, March 21-24, 2022},
  groups   = {Rev2},
  language = {English},
  url      = {https://www.osti.gov/biblio/1866819},
}



@TechReport{Ser2018_HorovodFastEasy_DelSDB,
  author     = {Sergeev, Alexander and Del Balso, Mike},
  title      = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
  year       = {2018},
  month      = feb,
  note       = {arXiv:1802.05799 [cs, stat] type: article},
  doi        = {10.48550/arXiv.1802.05799},
  groups     = {Rev2},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning},
  school     = {arXiv},
  shorttitle = {Horovod},
  url        = {http://arxiv.org/abs/1802.05799},
}


@Article{Dar2020_SurveyDeepLearning_KumDKAK,
  author     = {Dargan, Shaveta and Kumar, Munish and Ayyagari, Maruthi Rohit and Kumar, Gulshan},
  journal    = {Archives of Computational Methods in Engineering},
  title      = {A {Survey} of {Deep} {Learning} and {Its} {Applications}: {A} {New} {Paradigm} to {Machine} {Learning}},
  year       = {2020},
  issn       = {1886-1784},
  month      = sep,
  number     = {4},
  pages      = {1071--1092},
  volume     = {27},
  doi        = {10.1007/s11831-019-09344-w},
  groups     = {Rev2},
  language   = {en},
  shorttitle = {A {Survey} of {Deep} {Learning} and {Its} {Applications}},
  url        = {https://doi.org/10.1007/s11831-019-09344-w},
}










